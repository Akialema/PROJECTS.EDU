{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Полезная штука, если данных много и в аутпуте всё не помещается.\n",
    "## В текущей директории создаётся тхт файл с полным содержимым.\n",
    "#\n",
    "#\n",
    "#series_for_output = pd.Series(booking_df['negative_review'].value_counts().reset_index()['negative_review'].sort_values().str.strip().str.lower().unique())\n",
    "#\n",
    "## Сюда нужно поместить то, что хотим полностью увидеть.\n",
    "#output_data = series_for_output[series_for_output.apply(lambda x: len(x)) < 14]\n",
    "\n",
    "## Если аутпут - сериес:\n",
    "#with open('output_neg_rev.txt', 'w') as f:\n",
    "#    output_data.to_csv('output_neg_rev.txt', sep='\\t', index=False)\n",
    "\n",
    "## Если аутпут - списко:\n",
    "#with open('output.txt', 'w') as f:\n",
    "#    for item in output_data:\n",
    "#        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# А эта строчка позволит посмотреть, какие в памяти имеются глобальные переменные:\n",
    "# [var for var in globals() if not var.startswith('_')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id='contents'>Оглавляшка:</h2>\n",
    "\n",
    "1. [Функция-аналог .info() - my_info()](#myinfo)\n",
    "2. Графические функции:\n",
    "    * [Функция рисования 3-х графиков: гистограммы, коробки и Q-Q в одном флаконе.](#histboxqq)(для массива чисел)\n",
    "    * [Beeswarm + hist для двух переменных](#beeswarmhist)\n",
    "    * [Функция рисования графика корреляции.](#mycorr)\n",
    "    * [Функция декорирования оси Х столбчатой диаграммы.](#decoratexlabels)(крутая штука)\n",
    "    * [Функция рисования порогов вероятности для метрик recall, precision и f1_score](#recprecfthreshplot)\n",
    "    * [Функция рисования нахождения оптимального К для алгоритма K-MEANS с помощью inertia и silhouette](#kmeansinertiasilhouette)\n",
    "    * [Функция построения кривой обучения](#educurvebuildfunc) learning_curve()\n",
    "    * [Функция построения precision-recall кривой с меткой наилучшего порога](#prcurve) (PR-кривая)\n",
    "3. [Функция, создающая словарь для .fillna() заполнения NaN.](#fillnadict)\n",
    "4. [Функция отлова выбросов по методу Тьюки.](#outlierstukie)\n",
    "5. [Функция проверки на нормальность Шапиро-Уилка.](#shapirowilkfunc)\n",
    "6. [Функция проверки равенства дисперсий от Левена.](#levenefunc)\n",
    "7. Функция для работы с [\"строками\", \"внутри\", \"списка тэгов\", \"1 Adult\", \"2 Adults\"]\n",
    "    * [ОСНОВНАЯ](#maintagsworkfunc)\n",
    "    * [Второстепенная](#retagsworkfunc)\n",
    "8. Функция для работы с текстом. Когда столбец состоит только из элементов str\n",
    "    * [Основная функция с алгоритмом Вейдера](#vadersentanalmain)\n",
    "    * [Второстепенная функция с алгоритмом Вейдера](#vadersentanalsec)\n",
    "    * [Функция с алгоритмом Роберты на основе отзывов в твиттере](#robertapolarity)\n",
    "9. Функции для подбора гиперпараметров.\n",
    "    * [Optuna](#optunafunc)\n",
    "10. Математика\n",
    "    * [Исследование функции для одной переменной](#func_research) сырая\n",
    "    * [Исследование функции для двух, трёх переменных](#func_research_xyz) сырая\n",
    "    * [Интерактивный график градиентного спуска для функции одной переменной.](#gradfxinteractive)\n",
    "11. География\n",
    "    * [Функция, которая вычисляет расстояние между двумя точками lattitude и longitude.](#latlongdist)\n",
    "12. [Функция для тренировки нескольких моделей, используя несколько алгоритмов ML за 1 заход](#multiplemodelstraining)\n",
    "13. [Функция для генерации комбинаций элементов входящего списка](#combinationsgenerator) (классный комбинатор элементов списка)\n",
    "14. [Функция для тестирования силы столбца, относительно целевого. f_classif + chi2](#fclassifchitester)\n",
    "15. [Группировщик с применением ко всему датафрейму](#specialgrouper)\n",
    "16. [Функция для подсчёта любых метрик на выбор, как для и просто тест выборки, так и трейн + тест выборок](#printmetrics) вывод любых метрик\n",
    "17. Тестовые функции:\n",
    "    * [Функция для удаления сильно скоррелированных столбцов. (тестовая)](#highcorreraser)\n",
    "    * [Функция для автоматического поиска сильных столбцов относительно целевого столбца (тестовая)](#strongcolssearcher)\n",
    "    * ТЕСТОВАЯ [Функция для определения положительных и отрицательных слов в предложении](#goodbadwordssearcher) (супер сырая)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример работы tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee7afbb1d60f4f9c9a0d334e92dd0140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cell progress: : 0%|          | 0/100 [Elapsed: 00:00 m/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "list_for_tqdm = [i for i in range(100)]\n",
    "\n",
    "total_progress = 100\n",
    "with tqdm(total=total_progress, desc=f\"Cell progress: \", bar_format='{desc}: {percentage:.0f}%|{bar}| {n:.0f}/{total_fmt} [Elapsed: {elapsed} m/s]') as pbar:\n",
    "    cumulative_progress = 0 # Чтобы прогресс бар работал, как надо и завершался всегда на 100.\n",
    "##########^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^##############\n",
    "# сюда нужно вставить свой цикл.\n",
    "    for i, word in enumerate(list_for_tqdm):\n",
    "        time.sleep(0.1)\n",
    "        \n",
    "##########vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv##############        \n",
    "        # Рассчитываем размер шага на основе оставшегося прогресса и оставшихся итераций.\n",
    "        remaining_iterations = len(list_for_tqdm) - i # вот тут пригодился индекс i\n",
    "        remaining_progress = total_progress - cumulative_progress\n",
    "        step_size = remaining_progress / remaining_iterations\n",
    "        \n",
    "        # Обновляем общий прогресс.\n",
    "        cumulative_progress += step_size\n",
    "        pbar.update(step_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id='myinfo'>Функция печатающая аналог <span style='color:orange'>.info()</span> в расширенном варианте.</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My info.\n",
    "def my_info(df, catmaxnum=None):\n",
    "    \"\"\"Info function to show basic df indicators\n",
    "\n",
    "    Args:\n",
    "        df (pd.Dataframe): any df.\n",
    "        catmaxnum (int): the minimum threshold(rus: порог) after which a column will be considered a category. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        _type_: HTML object\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from IPython.display import HTML\n",
    "    \n",
    "    df_copy = df.copy() # it needs here.\n",
    "    \n",
    "    # Loop and condition for moment, when there are unhashable types in our column. (list, dict, set)\n",
    "    for i in df_copy.columns:\n",
    "        if any(isinstance(cell, (list, set, dict)) for cell in df_copy[i]):\n",
    "            df_copy[i] = df_copy[i].astype(str)\n",
    "                \n",
    "    func_df = pd.DataFrame({\n",
    "        \n",
    "        'column': df_copy.columns,\n",
    "        'num of unique vals': df_copy.nunique(),\n",
    "        'type': [str(df_copy.dtypes[i]) for i in df_copy.columns],\n",
    "        'mode': [round(df_copy[i].mode()[0], 2) if pd.api.types.is_numeric_dtype(df_copy[i].dtype) else df_copy[i].mode()[0] for i in df_copy.columns],\n",
    "        'number of entries': len(df_copy),\n",
    "        'NaN vals': df_copy.isna().sum(),\n",
    "        'number of dublics': df_copy.duplicated().sum(),\n",
    "        'describe': [f\"{df_copy[i].describe().reindex(['min', 'max', 'mean', 'std']).round(2).to_string()}\" if pd.api.types.is_numeric_dtype(df_copy[i].dtype) else 'see type column' for i in df_copy.columns]\n",
    "        \n",
    "    }).sort_values(by=['num of unique vals', 'column'], ascending=[True, True]).reset_index(drop=True)\n",
    "    \n",
    "    if catmaxnum:\n",
    "        func_df.insert(3, 'classification', ['category' if i < catmaxnum else 'numeric' for i in func_df['num of unique vals']])\n",
    "    \n",
    "    if len(str(df_copy.memory_usage(deep=True).sum())) > 6:\n",
    "        print(f\"memory usage: {str(round(df_copy.memory_usage(deep=True).sum() / 1e6, 1)) + ' MB'}\")\n",
    "    else:\n",
    "        print(f\"memory usage: {str(round(df_copy.memory_usage(deep=True).sum() / 1e3, 1)) + ' KB'}\")\n",
    "    \n",
    "    # Add CSS style to left-align the \"describe\" column\n",
    "    html_output = func_df.to_html().replace('\\\\n', '<br>')\n",
    "    html_output = html_output.replace('<td>min', '<td style=\"text-align: left;\">min...:')\n",
    "    html_output = html_output.replace('<br>max', '<br style=\"text-align: left;\">max..:')\n",
    "    html_output = html_output.replace('<br>mean', '<br style=\"text-align: left;\">mean:')\n",
    "    html_output = html_output.replace('<br>std', '<br style=\"text-align: left;\">std....:')\n",
    "    \n",
    "    return HTML(html_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to contents](#contents)\n",
    "\n",
    "<h2 id='outlierstukie'>Функция отлова выбросов по методу Тьюки</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Tukie outliers finder function.\n",
    "def outliers_iqr_mode_log(data, feature=None, log_scale=False, left=1.5, right=1.5):\n",
    "    \n",
    "    func_data = data.copy()\n",
    "    \n",
    "    if isinstance(func_data, pd.DataFrame):    \n",
    "        if log_scale: # if we want to logarithm a feature.\n",
    "            if any(func_data[feature] == 0) and all(func_data[feature] >= 0): # if we have 0 values and all vals > 0\n",
    "                x = np.log(func_data[feature] + 1)\n",
    "            elif any(func_data[feature] == 0) and not all(func_data[feature] >= 0): # if we have 0 and negative vals\n",
    "                x = np.log(abs(func_data[feature]) + 1)\n",
    "            else:\n",
    "                x = np.log(func_data[feature])\n",
    "        else:\n",
    "            x = func_data[feature]\n",
    "    else:\n",
    "        x = func_data # if data == array or series\n",
    "        \n",
    "    quantile_25, quantile_75 = x.quantile(0.25), x.quantile(0.75)\n",
    "    iqr = quantile_75 - quantile_25\n",
    "    \n",
    "    lower_bound = quantile_25 - iqr * left\n",
    "    upper_bound = quantile_75 + iqr * right\n",
    "    \n",
    "    outliers = func_data[(x < lower_bound) | (x > upper_bound)]\n",
    "    cleaned = func_data[(x >= lower_bound) & (x <= upper_bound)]\n",
    "    \n",
    "    return outliers, cleaned.reset_index(drop=True) # .copy() is needed here if we wanna use cleaned daataframe. Or system will swear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to contents](#contents)\n",
    "\n",
    "<h3 id='histboxqq'>Функция рисования 3-х графиков: гистограмма, коробка и Q-Q.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_box_qq(arg, full_iqr=False):\n",
    "    \"\"\"Function of drawing histogram, box and cuckoo in one bottle.\n",
    "\n",
    "    full_iqr... It's shorter.. I wrote a function for the lognormal distribution, i.e. there are no outliers on the left.\n",
    "    But then I thought about it and decided to add it, because then I still log the feature.\n",
    "    And in general, if the array is usually distributed.\n",
    "    In a word: full_iqr is Boolean. By default, False. I.e., only the right borders are drawn.\n",
    "    \n",
    "    The input is a Series or array.\n",
    "\n",
    "    Args:\n",
    "        arg (Series/np.array): any numeric series or array.\n",
    "        full_iqr (bool, optional): choose, u wanna show only right outliers borders or both. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        plt\n",
    "    \"\"\"\n",
    "    \n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import statsmodels.api as sm # рисовалка qq-plot.\n",
    "    import seaborn as sns\n",
    "    \n",
    "    func_arg = arg.copy()\n",
    "    # replace inf.\n",
    "    func_arg = func_arg.replace([np.inf, -np.inf], np.nan)\n",
    "    # If there is a nan, I drop it. Because it does not draw if there is a nan.\n",
    "    func_arg = pd.Series(func_arg).dropna()\n",
    "    \n",
    "    fig, ax = plt.subplots(ncols=3, nrows=1, figsize=[18, 5])\n",
    "    \n",
    "    # The histogram. And the settings for the histogram lines. Legend = label in each line.\n",
    "    # Red is the median, green is the tukey, and blue is 3 sigma.\n",
    "    sns.histplot(func_arg, kde=True, bins=50, ax=ax[0])\n",
    "    \n",
    "    ax[0].get_lines()[0].set_color('black')\n",
    "    ax[0].axvline(func_arg.median(), color='red', linestyle='--', linewidth='1.8', label='median')\n",
    "    ax[0].axvline(func_arg.quantile(0.75) + ((func_arg.quantile(0.75) - func_arg.quantile(0.25)) * 1.5), color='g', ls='--', lw=2, label='1.5 IQR Tukie')\n",
    "    ax[0].axvline(func_arg.mean() + 3 * func_arg.std(), color='b', ls='--', lw=2, label='3 IQR z-score')\n",
    "    ax[0].legend()\n",
    "    ax[0].set_xlabel(f'Feauture {func_arg.name}')\n",
    "    \n",
    "    # Box. And the settings for it.\n",
    "    sns.boxplot(func_arg, ax=ax[1], orient='h', medianprops={'color': 'red', 'linestyle': '--'})\n",
    "\n",
    "    ax[1].axvline(func_arg.quantile(0.75) + ((func_arg.quantile(0.75) - func_arg.quantile(0.25)) * 1.5), color='g', ls='--', lw=2, label='1.5 IQR Tukie')\n",
    "    ax[1].axvline(func_arg.mean() + 3 * func_arg.std(), color='b', ls='--', lw=2, label='3 IQR z-score')\n",
    "    ax[1].legend()    \n",
    "    ax[1].set_xlabel(f'Feauture {func_arg.name}')\n",
    "    \n",
    "    if full_iqr: # Left emission catch lines are added.\n",
    "        ax[0].axvline(func_arg.quantile(0.25) - ((func_arg.quantile(0.75) - func_arg.quantile(0.25)) * 1.5), color='g', ls='--', lw=2)\n",
    "        ax[0].axvline(func_arg.mean() - 3 * func_arg.std(), color='b', ls='--', lw=2)\n",
    "        ax[1].axvline(func_arg.quantile(0.25) - ((func_arg.quantile(0.75) - func_arg.quantile(0.25)) * 1.5), color='g', ls='--', lw=2)\n",
    "        ax[1].axvline(func_arg.mean() - 3 * func_arg.std(), color='b', ls='--', lw=2)\n",
    "    \n",
    "    # This line builds a Q-Q graph.\n",
    "    qq = sm.ProbPlot(func_arg, fit=True).qqplot(marker='*', markerfacecolor='b', markeredgecolor='b', alpha=0.3, ax=ax[2])\n",
    "    # Line, also with the ability to influence color. fmt parameter.\n",
    "    sm.qqline(qq.axes[2], line='45', fmt='r', linestyle='--')\n",
    "    \n",
    "    # The general title.\n",
    "    plt.suptitle(f'Distribution of the feature: {func_arg.name}').set_fontsize(20)\n",
    "    \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to contents](#contents)\n",
    "\n",
    "<h3 id='beeswarmhist'>Beeswarm + hist для двух переменных.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swarm_hist(df, targ_col, depend_col, figsize=(15, 5)):\n",
    "    \n",
    "    # сырая, есть что добавить в гиперпараметры\n",
    "    # функция, грубо говоря, про распределение таргета в зависимости от... каких-либо категорий в depend_col\n",
    "    \n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    fig, ax = plt.subplots(ncols=2, figsize=figsize)\n",
    "    \n",
    "    # Построение графиков beeswarm + hist\n",
    "    for i, j in enumerate(df[depend_col].unique()[::-1]):\n",
    "        y = df[df[depend_col] == j]\n",
    "        x = np.random.normal(i + 1, 0.1, size=len(y)) # 0.1 отвечает за расстояние между\n",
    "        ax[0].plot(x, y[targ_col], '*', label=f'{depend_col} {j}', alpha=0.6)\n",
    "        \n",
    "        # +i * 12 - чтобы башенки гистограмм не накладывались друг на друга (12 - отвечает за расстояние между)\n",
    "        sns.histplot(y[targ_col] + i * 12, ax=ax[1], kde=False, \n",
    "                     label=f'{depend_col} {j}', binwidth=0.2, color=f\"C{i}\", alpha=0.5)\n",
    "    \n",
    "    ax[0].set_xticks(df[depend_col].unique()[::-1])\n",
    "    ax[0].set_xlabel(f'{depend_col.capitalize()}')\n",
    "    ax[0].set_ylabel(f'{targ_col.capitalize()}')\n",
    "    ax[0].set_title(f'Beeswarm plot of {targ_col.capitalize()} by {depend_col.capitalize()}')\n",
    "    ax[0].legend()\n",
    "    \n",
    "    ax[1].set_title(f'Histogram of {targ_col.capitalize()} by {depend_col.capitalize()}')\n",
    "    ax[1].set_xticks([])\n",
    "    ax[1].set_xlabel(f'{targ_col.capitalize()}')\n",
    "    ax[1].set_ylabel('Count')\n",
    "    ax[1].legend()\n",
    "    \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to contents](#contents)\n",
    "\n",
    "<h3 id='mycorr'>Функция рисования тепловой карты корреляции.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_corr(df, targ_col=False, method='pearson', corrneg=0, corrpos=0, figsize=(10, 5), xtickrot=90, annot_kws={'size': 'medium'}, show_high_corr_cols_names=False, ax=None):\n",
    "    \"\"\"\n",
    "    The function of drawing a correlation graph, but not an ordinary one, but one in which the lower part is the entire one,\n",
    "    and the upper part contains only those correlation values that are greater and less than some number we have specified.\n",
    "\n",
    "    Args:\n",
    "        df (pd.Dataframe): any df.\n",
    "        targ_col (str, optional): if targ_col is set, func just move this col into the end of dataframe, to better visualisation. It will be at the last row of the plot. Defaults: False\n",
    "        method (str, optional): method u like. Default: pearson.\n",
    "        corrneg (float, optional): negative threshold for display. (for negative correlation value)\n",
    "        corrpos (float, optional): positive threshold. (for a positive value) both default to 0. At 0, the full standard correlation matrix will be displayed.\n",
    "        figsize (tuple, optional): u know what it is.)\n",
    "        xtickrot(int, optional): rotate x axis labels. Default: 90\n",
    "        annot_kws(dict, optional): size of annot font. Default: {\"size\": 'medium'}\n",
    "        show_high_corr_cols_names(bool, optional): if we wanna watch names of high corr cols. Default: False. Should use without plot and other plot args. Just thresholds and this one.\n",
    "        ax (ax, optional): use only if u wanna add this plot to your multiple plots. Default: None\n",
    "\n",
    "    Returns:\n",
    "        _type_: plt or df\n",
    "    \"\"\"\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    if targ_col:\n",
    "        # sort indexes by correlation with targ_col from highest to lowest\n",
    "        most_corr_cols_index = df.corr(numeric_only=True, method=method)[targ_col].drop(targ_col).sort_values(ascending=False).index\n",
    "        # putting targ_col to fist row (y = 1) of the chart with sorted features (watch ur chart and u will get it)\n",
    "        corr_df = pd.concat([df.drop(targ_col, axis=1)[most_corr_cols_index], df[targ_col]], axis=1).corr(numeric_only=True, method=method).copy()\n",
    "    else:\n",
    "        corr_df = df.corr(numeric_only=True, method=method).copy()\n",
    "    \n",
    "    # We find indices where the correlation value is greater and less than a certain number.\n",
    "    high_corr_pairs = ((corr_df >= corrpos) & (corr_df <= 1)) | (corr_df <= corrneg)\n",
    "    \n",
    "    # We receive only those pairs of features where the condition is met. The rest are NaN.\n",
    "    high_corr_features = corr_df[high_corr_pairs]\n",
    "    \n",
    "    # if we wanna watch high corr pairs in the form of a df. Without picture. I decided 2 split it up.\n",
    "    if show_high_corr_cols_names:\n",
    "        # stack hight corr feature.\n",
    "        shccn = high_corr_features.stack().to_frame().reset_index().rename(columns={0: 'corr'})\n",
    "        # sort'em.\n",
    "        shccn[['level_0', 'level_1']] = np.sort(shccn[['level_0', 'level_1']], axis=1)\n",
    "        # keep only 1 pair of high corr cols names.\n",
    "        shccn = shccn.groupby(['level_0', 'level_1']).first().reset_index()\n",
    "        # removing the diagonal.\n",
    "        shccn = shccn.query('level_0 != level_1')\n",
    "        # return with resetting index.\n",
    "        return shccn.sort_values(by='corr', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # if we wanna see the pic.\n",
    "    else:\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "        ax.set_title(f'Correlation of incoming dataframe features.', pad=10) # узкое место pad=10, чтоб заголовок был на расстоянии от графика\n",
    "        \n",
    "        # 2 masks, one triu (tri upper), the second tril (tri lower).\n",
    "        mask_for_logging_df = np.triu(np.ones_like(corr_df, dtype=bool))\n",
    "        mask_for_logging_df_2 = np.tril(np.ones_like(high_corr_features, dtype=bool))\n",
    "        \n",
    "        # 2 graphics. I didn’t figure out how to make such a picture using 1 chart. That's why cbar=False helps.\n",
    "        sns.heatmap(corr_df, annot=True, annot_kws=annot_kws, cmap='coolwarm', mask=mask_for_logging_df, vmax=1, vmin=-1, linewidths=0.1, fmt='.2f', ax=ax) # <--- lower part\n",
    "        \n",
    "        # if condition == 1 high_corr_features contains only nan, so we dont need upper part. \n",
    "        # or.. high_corr_features has no nan, i.e. corrpos and corrneg == 0, so we draw all corr matrix.\n",
    "        if (len(high_corr_features.isna().sum().value_counts()) != 1) | ((corrpos == 0) & (corrneg == 0)):\n",
    "            sns.heatmap(high_corr_features, annot=True, annot_kws=annot_kws, cmap='coolwarm', mask=mask_for_logging_df_2, cbar=False, vmax=1, vmin=-1, linewidths=0.1, fmt='.2f', ax=ax) # <--- upper part\n",
    "        \n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=xtickrot)\n",
    "    \n",
    "        ax.plot([0, len(df.columns)], [0, len(df.columns)], color='black', linewidth=2); # It's just a line. I don't know why. But i like it.\n",
    "        \n",
    "        return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to contents](#contents)\n",
    "\n",
    "<h3 id='fillnadict'>Функция, создающая словарь для заполнения метода .fillna</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillna_dict_for_custom_df(df, func, targ_col=None, roundnum=10):\n",
    "    \"\"\"Function for generating a dictionary for the fillna function argument.\n",
    "    \n",
    "    The function takes as input a dataframe with np.nan and the function that we want to aggregate.\n",
    "    That is, we have a feature in which there is a nan. We want to fill the nan, for example, with the median.\n",
    "    We run through the names of the columns that have nan and assign each nan a median value for the column.\n",
    "    Why the condition? When we ask for mode, we get a Series object that needs to be accessed by index to get the value.\n",
    "    And when we ask for the average or median, it immediately returns the value.\n",
    "\n",
    "    Args:\n",
    "        df (pd.Dataframe): any df\n",
    "        func (str): any func in str format. \n",
    "        targ_col (str, optional): name of target column by which we wanna groupby and have needed agregation. Defaults: None.\n",
    "        roundnum (int, optional): num of np.round(X, ?), how we want to round our outcoming number. Defaults: 10.\n",
    "        \n",
    "        Ex: df.fillna(value=fillna_dict_for_custom_df(df, 'mean'))\n",
    "\n",
    "    Returns:\n",
    "        _type_: dict\n",
    "    \"\"\"\n",
    "    \n",
    "    if targ_col:\n",
    "        return {i: df.groupby(targ_col)[i].transform(lambda x: x.mode().iloc[0]) if np.issubdtype(df[i], 'object') or func == 'mode' else np.round(df.groupby(targ_col)[i].transform(func), roundnum) for i in df.loc[:, df.isna().mean() > 0].columns}\n",
    "    return {i: df[i].mode().iloc[0] if np.issubdtype(df[i], 'object') or func == 'mode' else np.round(df[i].agg(func), roundnum) for i in df.loc[:, df.isna().mean() > 0].columns}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to contents](#contents)\n",
    "\n",
    "<h3 id='decoratexlabels'>Функция декорирования оси Х столбчатой диаграммы.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ОРИГИНАЛ!\n",
    "#from itertools import groupby # для рисования мультииндекс меток по оси х (и не только, но здесь - только).\n",
    "#\n",
    "## Чумовейшие три функции, которые рисуют мультииндекс разметку по оси Х!!!\n",
    "## Работа происходит по третьей функции. \n",
    "## На вход подаётся ax и датафрейм вида: \n",
    "## 1. главный индекс; \n",
    "## 2. второстепенный индекс или группа индексов, по иерархии от старшего к младшему; \n",
    "## 3. последним идёт индекс, который является оттенком (hue).\n",
    "## Вид группировки: .groupby(['1_st_order_idx', '2_nd_order_idx', ..., 'hue_idx'])['nums_column'].mean().unstack()\n",
    "## P.S. для работы нужен groupby из itertools\n",
    "#\n",
    "#def add_line(ax, xpos, ypos):\n",
    "#    line = plt.Line2D([xpos, xpos], [ypos + .1, ypos],\n",
    "#                      transform=ax.transAxes, color='gray')\n",
    "#    line.set_clip_on(False)\n",
    "#    ax.add_line(line)\n",
    "#\n",
    "#def label_len(my_index,level):\n",
    "#    labels = my_index.get_level_values(level)\n",
    "#    return [(k, sum(np.fromiter((1 for i in g), dtype=int))) for k, g in groupby(labels)]\n",
    "#\n",
    "#def label_group_bar_table(ax, df):\n",
    "#    ypos = -.1\n",
    "#    scale = 1./df.index.size\n",
    "#    for level in range(df.index.nlevels)[::-1]:\n",
    "#        pos = 0\n",
    "#        for label, rpos in label_len(df.index, level):\n",
    "#            lxpos = (pos + .5 * rpos)*scale\n",
    "#            ax.text(lxpos, ypos, label, ha='center', transform=ax.transAxes)\n",
    "#            add_line(ax, pos*scale, ypos)\n",
    "#            pos += rpos\n",
    "#        add_line(ax, pos*scale ,ypos)\n",
    "#        ypos -= .1\n",
    "        \n",
    "def label_group_bar_table(ax, df, orient='v'):\n",
    "    \"\"\"label_group_bar_table \n",
    "\n",
    "    Чумовейшие три функции, которые рисуют мультииндекс разметку по оси Х!!!\n",
    "    На вход подаётся ax и датафрейм вида: \n",
    "     1. главный индекс; \n",
    "     2. второстепенный индекс или группа индексов, по иерархии от старшего к младшему; \n",
    "     3. последним идёт индекс, который является оттенком (hue).\n",
    "     Вид группировки: .groupby(['1_st_order_idx', '2_nd_order_idx', ..., 'hue_idx'])['nums_column'].mean().unstack()\n",
    "     P.S. для работы нужен groupby из itertools\n",
    "     P.S.2 Сие ваяние не моё, потому хз, что там происходит и я не распишу каждую строчку, но результат меня устраивает.)\n",
    "\n",
    "    Args:\n",
    "        ax (_type_): ax with ur plot.\n",
    "        df (pd.DataFrame): ur df which u wanna plot.\n",
    "        orient (str, optional): if we wanna draw horisontal plot, just set not 'v' parameter. Defaults: 'v'.\n",
    "    \"\"\"\n",
    "    from itertools import groupby # для рисования мультииндекс меток по оси х (и не только, но здесь - только).\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    def add_line(ax, xpos, ypos):\n",
    "        line = plt.Line2D([xpos, xpos], [ypos + .1, ypos],\n",
    "                          transform=ax.transAxes, color='gray')\n",
    "        line.set_clip_on(False)\n",
    "        ax.add_line(line)\n",
    "        \n",
    "    def add_line_h(ax, xpos, ypos):\n",
    "        line = plt.Line2D([xpos + .1, xpos], [ypos, ypos],\n",
    "                          transform=ax.transAxes, color='gray')\n",
    "        line.set_clip_on(False)\n",
    "        ax.add_line(line)\n",
    "    \n",
    "    def label_len(my_index, level):\n",
    "        labels = my_index.get_level_values(level)\n",
    "        return [(k, sum(np.fromiter((1 for i in g), dtype=int))) for k, g in groupby(labels)]\n",
    "    \n",
    "    \n",
    "    if orient == 'v':    \n",
    "        ypos = -.1\n",
    "        scale = 1./df.index.size\n",
    "        for level in range(df.index.nlevels)[::-1]:\n",
    "            pos = 0\n",
    "            for label, rpos in label_len(df.index, level):\n",
    "                lxpos = (pos + .5 * rpos)*scale\n",
    "                ax.text(lxpos, ypos, label, ha='center', transform=ax.transAxes)\n",
    "                add_line(ax, pos*scale, ypos)\n",
    "                pos += rpos\n",
    "            add_line(ax, pos*scale, ypos)\n",
    "            ypos -= .1\n",
    "    else:\n",
    "        xpos = -.1\n",
    "        scale = 1./df.index.size\n",
    "        for level in range(df.index.nlevels)[::-1]:\n",
    "            pos = 0\n",
    "            for label, rpos in label_len(df.index, level):\n",
    "                lypos = (pos + .3 * rpos)*scale\n",
    "                ax.text(xpos, lypos, label, ha='left', transform=ax.transAxes)\n",
    "                add_line_h(ax, xpos, pos*scale)\n",
    "                pos += rpos\n",
    "            add_line_h(ax, xpos, pos*scale)\n",
    "            xpos -= .1\n",
    "\n",
    "####################################################################################################################\n",
    "# Пример использования:\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(30, 8))\n",
    "\n",
    "plt.suptitle('Динамика соотношения среднего годового дохода в зависимости от формата занятости, года и опыта').set_fontsize(25)\n",
    "\n",
    "# Подготовка датафрейма.\n",
    "df_for_this_cell = df_for_this_cell.groupby([choose_job, 'remote_ratio', 'work_year', 'experience_level'])['salary_in_usd'].mean().unstack()\n",
    "\n",
    "# Сама строка.\n",
    "df_for_this_cell.plot(kind='bar', ax=ax, stacked=True, legend=True)\n",
    "\n",
    "ax.set_xticklabels('') # обязательно удаление названий по оси х\n",
    "ax.set_xlabel('')\n",
    "\n",
    "# И оборачивание подготовленного, сгруппированного датафрейма в функцию\n",
    "# На самом деле можно и без этого. Функция чисто декоративная.\n",
    "label_group_bar_table(ax, df_for_this_cell)\n",
    "####################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to contents](#contents)\n",
    "\n",
    "<h3 id='recprecfthreshplot'>Функция рисования порогов вероятности для метрик recall, precision и f1_score</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rpf_threshold_dependence(y, y_proba, thresholds=np.arange(0.1, 1, 0.05), figsize=(10, 4), xtickrot=0):\n",
    "    \"\"\"rpf_threshold_dependence\n",
    "\n",
    "    Рисовалка графиков зависимости метрик от порога вероятности.\n",
    "\n",
    "    Args:\n",
    "        y (pd.Series): правильные ответы.\n",
    "        y_proba (pd.Series): предсказанные ответы.\n",
    "        thresholds (numpy.ndarray, optional): пороги вероятностей. Defaults to np.arange(0.1, 1, 0.05).\n",
    "        figsize (tuple): figsize. Defaults to (10, 4).\n",
    "        xtickrot (int): rotation of x labels. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        plt: rpf_threshold_dependence.show()\n",
    "    \"\"\"\n",
    "    from sklearn import metrics\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Создадим списки, в которых будем хранить значения метрик.\n",
    "    recall_scores = []\n",
    "    precision_scores = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    # В цикле перебираем значения порогов вероятности\n",
    "    for threshold in thresholds:\n",
    "        # Делаем предсказание\n",
    "        y_pred = y_proba.apply(lambda x: 1 if x > threshold else 0)\n",
    "        \n",
    "        #Считаем метрики и добавляем их в списки\n",
    "        recall_scores.append(metrics.recall_score(y, y_pred))\n",
    "        precision_scores.append(metrics.precision_score(y, y_pred, zero_division=0)) # Здесь поставил zero_div=0, т.к. иногда вылезает предупреждение.\n",
    "        f1_scores.append(metrics.f1_score(y, y_pred))\n",
    "    \n",
    "    # Визуализируем метрики при различных threshold.\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Строим линейный график зависимости recall от threshold.\n",
    "    ax.plot(thresholds, recall_scores, label='Recall')\n",
    "    # Строим линейный график зависимости precision от threshold.\n",
    "    ax.plot(thresholds, precision_scores, label='Precision')\n",
    "    # Строим линейный график зависимости F1 от threshold.\n",
    "    ax.plot(thresholds, f1_scores, label='F1-score')\n",
    "     \n",
    "    # Даём графику название и подписываем оси.\n",
    "    ax.set_title('Recall/Precision/F1 dependence on the threshold')\n",
    "    ax.set_xlabel('Probability threshold')\n",
    "    ax.set_ylabel('Score')\n",
    "    # Устанавливаем отметки по оси x.\n",
    "    ax.set_xticks(thresholds)\n",
    "    # Добавил поворот меток оси х. Иногда полезно.\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=xtickrot) \n",
    "    # Отображаем легенду.\n",
    "    ax.legend();\n",
    "    \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to contents](#contents)\n",
    "\n",
    "<h3 id='kmeansinertiasilhouette'>Функция рисования нахождения оптимального К для алгоритма K-MEANS с помощью inertia и silhouette</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inertia_silhouette_plot(X, alg, max_range=10, coef='silhouette', figsize=(12, 5)):\n",
    "    \"\"\"inertia_silhouette_plot \n",
    "\n",
    "    Функция рисования графиков инерции и силуэта для определения опимального количества кластеров для алгоритма k-means.\n",
    "    \n",
    "    Args:\n",
    "        X (pd.DataFrame): incoming X. (X should be DataFrame object)\n",
    "        alg (sklearn.cluster._kmeans.KMeans): incoming alg.\n",
    "        max_range (int, optional): max number of iterations. Defaults to 10.\n",
    "        coef (str): choose what we wanna see: inertia graph, silhouette graph, ot both. Defaults to 'silhouette'.\n",
    "        figsize (tuple, optional): figsize of picture. Defaults to (12, 5).\n",
    "\n",
    "    Returns:\n",
    "        plt: plt.show()\n",
    "    \"\"\"\n",
    "    # import nessessary lib for copy incoming alg.\n",
    "    import copy\n",
    "    from sklearn.metrics import silhouette_score\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    if coef == 'inertia':\n",
    "        # set figax.\n",
    "        fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "        # copy of incoming alg. It needs here.\n",
    "        func_alg = copy.deepcopy(alg)\n",
    "        # Series for plot.\n",
    "        inertia_series = pd.Series([func_alg.set_params(n_clusters=i).fit(X).inertia_ for i in range(1, max_range)], index=range(1, max_range))\n",
    "        # plot.\n",
    "        sns.lineplot(inertia_series, ax=ax, marker='X')\n",
    "        # plot settings.\n",
    "        plt.suptitle('Inertia of incoming X')\n",
    "        ax.set_xlabel(\"cluster\", fontsize=12), ax.set_ylabel(\"inertia\", fontsize=12)\n",
    "        ax.set_xticks(range(1, max_range, 1)), ax.set_yticks(inertia_series)\n",
    "        \n",
    "        return plt\n",
    "        \n",
    "    elif coef == 'silhouette':\n",
    "        # set figax.\n",
    "        fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "        # copy of incoming alg. It needs here.\n",
    "        func_alg = copy.deepcopy(alg)\n",
    "        # Series for plot.\n",
    "        silhouette_series = pd.Series([silhouette_score(X, func_alg.set_params(n_clusters=i).fit(X).labels_) for i in range(2, max_range)], index=range(2, max_range))\n",
    "        # plot.\n",
    "        sns.lineplot(silhouette_series, ax=ax, marker='o')\n",
    "        # plot settings.\n",
    "        plt.suptitle('Silhouette of incoming X')\n",
    "        ax.set_xlabel(\"cluster\", fontsize=12), ax.set_ylabel(\"silhouette\", fontsize=12)\n",
    "        ax.set_xticks(range(1, max_range, 1)), ax.set_yticks(silhouette_series)\n",
    "        \n",
    "        return plt\n",
    "    \n",
    "    else:\n",
    "        # set figax.\n",
    "        fig, ax = plt.subplots(1, 2, figsize=figsize)\n",
    "        # copy of incoming alg. It needs here.\n",
    "        func_alg = copy.deepcopy(alg)\n",
    "        # 2 series. 1 = inertia series, 2 = silhouette series.\n",
    "        inertia_series = pd.Series([func_alg.set_params(n_clusters=i).fit(X).inertia_ for i in range(1, max_range)], index=range(1, max_range))\n",
    "        silhouette_series = pd.Series([silhouette_score(X, func_alg.set_params(n_clusters=i).fit(X).labels_) for i in range(2, max_range)], index=range(2, max_range))\n",
    "        # plotting.\n",
    "        sns.lineplot(inertia_series, ax=ax[0], marker='X')\n",
    "        sns.lineplot(silhouette_series, ax=ax[1], marker='o')\n",
    "        \n",
    "        plt.suptitle('Inertia and silhouette of incoming X')\n",
    "        \n",
    "        fig.tight_layout(w_pad=5, h_pad=3) # расстояние между подграфиками\n",
    "        plt.subplots_adjust(top=0.9) # чтобы заголовок не налезал на графики\n",
    "        \n",
    "        # set x and y labels.\n",
    "        ax[0].set_xlabel(\"cluster\", fontsize=12), ax[0].set_ylabel(\"inertia\", fontsize=12)\n",
    "        ax[1].set_xlabel(\"cluster\", fontsize=12), ax[1].set_ylabel(\"silhouette\", fontsize=12)\n",
    "        \n",
    "        # set x and y ticks bins.\n",
    "        ax[0].set_xticks(range(1, max_range, 1)), ax[0].set_yticks(inertia_series)\n",
    "        ax[1].set_xticks(range(1, max_range, 1)), ax[1].set_yticks(silhouette_series)\n",
    "        \n",
    "        del func_alg, inertia_series, silhouette_series\n",
    "        \n",
    "        return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to contents](#contents)\n",
    "\n",
    "<h3 id='educurvebuildfunc'>Функция построения кривой обучения</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(model, X, y, cv, scoring='f1', ax=None, title=''):\n",
    "    \n",
    "    from sklearn import model_selection #методы разделения и валидации\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Вычисляем координаты для построения кривой обучения\n",
    "    train_sizes, train_scores, valid_scores = model_selection.learning_curve(estimator = model, # модель\n",
    "                                                                             X = X, # матрица наблюдений X\n",
    "                                                                             y = y, # вектор ответов y\n",
    "                                                                             cv = cv, # кросс-валидатор\n",
    "                                                                             scoring = scoring) # метрика\n",
    "    # Вычисляем среднее значение по фолдам для каждого набора данных\n",
    "    train_scores = np.mean(train_scores, axis=1)\n",
    "    valid_scores = np.mean(valid_scores, axis=1)\n",
    "    \n",
    "    # Если координатной плоскости не было передано, создаём новую\n",
    "    if not ax:\n",
    "        fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    \n",
    "    ax.plot(train_sizes, train_scores, label='Train') # Строим кривую обучения по метрикам на тренировочных фолдах\n",
    "    ax.plot(train_sizes, valid_scores, label='Valid') # Строим кривую обучения по метрикам на валидационных фолдах\n",
    "    \n",
    "    ax.set_title(f'Learning curve: {title}')\n",
    "    ax.set_xlabel('Train data size')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.xaxis.set_ticks(train_sizes) # Устанавливаем отметки по оси абсцисс\n",
    "    ax.set_ylim(0, 1) # Устанавливаем диапазон оси ординат\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to contents](#contents)\n",
    "\n",
    "<h3 id='prcurve'>Функция построения precision-recall кривой с меткой наилучшего порога (PR-кривая)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr_curve(y, y_proba_pred, ax=None, title='', toggle_pic_or_threshnum='pic'):\n",
    "    \"\"\"pr_curve \n",
    "    \n",
    "    Графическая функция для отображения кривой precision-recall с возможностью выводить либо график, либо наилучший порог.\n",
    "\n",
    "    Args:\n",
    "        y (pd.Series): y_train or any u like.\n",
    "        y_proba_pred (numpy.ndarray): proba_pred.\n",
    "        ax (?, optional): this is just ax. Defaults to None.\n",
    "        title (str, optional): title. Defaults to ''.\n",
    "        toggle_pic_or_threshnum (str, optional): _description_. Defaults to 'pic'.\n",
    "\n",
    "    Returns:\n",
    "        plt or float: u will recieve either a pic or a best threshold num.\n",
    "    \"\"\"\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn import metrics\n",
    "    \n",
    "    # Мне нравится пандас, потому сделаю через датафрейм\n",
    "    prc = pd.DataFrame(metrics.precision_recall_curve(y, y_proba_pred), index=['precision', 'recall', 'thresholds']).T\n",
    "    \n",
    "    # Вычисляем F1-меру при различных threshold.\n",
    "    f1_scores = (2 * prc['precision'] * prc['recall']) / (prc['precision'] + prc['recall'])\n",
    "    # Определяем индекс максимума.\n",
    "    idx = np.argmax(f1_scores)\n",
    "    \n",
    "    # Условие для: либо рисуем график... \n",
    "    if toggle_pic_or_threshnum == 'pic':\n",
    "        # если не указан ax\n",
    "        if not ax:\n",
    "            fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        # Линия и точка на линии\n",
    "        ax.plot(prc['recall'], prc['precision'], label=f\"{title} auc: {metrics.auc(prc['recall'], prc['precision']):.2f}\")\n",
    "        ax.scatter(prc.loc[idx, 'recall'], prc.loc[idx, 'precision'], c='black', marker='o', s=80, label='Best F1 score')\n",
    "        \n",
    "        # Добавляем аннотацию с меткой точки на оси X (порог и ф1_скор)\n",
    "        recall_best = prc.loc[idx, 'recall'] # координаты точки\n",
    "        precision_best = prc.loc[idx, 'precision']\n",
    "        ax.annotate(f\"Threshold: {prc.loc[idx, 'thresholds']:.2f}\\nF1_score: {f1_scores[idx]:.2f}\", \n",
    "                    xy=(recall_best+0.01, precision_best), \n",
    "                    xytext=(recall_best + 0.05, precision_best),  # Смещаем текст немного вправо\n",
    "                    arrowprops=dict(facecolor='black', shrink=0.05, width=0.7, headwidth=7, headlength=7))\n",
    "        # Метки и заголовок.\n",
    "        ax.set_xlabel('Recall')\n",
    "        ax.set_ylabel('Precision')\n",
    "        ax.set_title(f'Precision-recall curve: {title}')\n",
    "        \n",
    "        plt.legend();\n",
    "    # либо выводим наилучший порог метрики.\n",
    "    else:\n",
    "        return prc.loc[idx, 'thresholds']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to contents](#contents)\n",
    "\n",
    "<h3 id='shapirowilkfunc'>Функция проверки на нормальность Шапиро-Уилка.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "alpha = 0.5\n",
    "\n",
    "# Функция проверки на нормальность Шапиро-Уилка.\n",
    "def shapirowilk(df_or_arr, alpha=0.05):\n",
    "    \"\"\"Продвинутая функция проверки нормальности Шапиро-Уилка.\n",
    "    \n",
    "    Тут всё просто. Если датафрейм, то формируем через цикл списки названий нормальных и не нормальных столбцов.\n",
    "    Иначе сразу печатаем реультат теста. Т.к. у нас Series или массив.\n",
    "    Далее смотрим на длины списков и формируем список выводов функции. И выводим его.\n",
    "    Почему продвинутая? Потому что я раньше считал количество норм и не норм столбцов через: dict(Counter([shapirowilk(df_for_this_cell[i]) for i in df_for_this_cell.columns]))\n",
    "    а сама функция выглядела, как функция Левена ниже.))\n",
    "\n",
    "    Args:\n",
    "        df_or_arr (pd.DataFrame): Dataframe or Series or array\n",
    "        alpha (float): alpha significance level\n",
    "\n",
    "    Returns:\n",
    "        df or str\n",
    "    \"\"\"\n",
    "    func_df = pd.DataFrame()\n",
    "\n",
    "    if isinstance(df_or_arr, pd.DataFrame):\n",
    "        for i, j in enumerate(df_or_arr.columns):\n",
    "            if stats.shapiro(df_or_arr[j])[1] <= alpha:\n",
    "                func_df.loc[i, 'Distribution'] = df_or_arr[j].name\n",
    "                func_df.loc[i, 'Status'] = 'не нормальное'\n",
    "            else:\n",
    "                func_df.loc[i, 'Distribution'] = df_or_arr[j].name\n",
    "                func_df.loc[i, 'Status'] = 'нормальное'\n",
    "    else:\n",
    "        return f'Распределение не нормальное. При alpha = {alpha}' if stats.shapiro(df_or_arr)[1] <= alpha else f'Распределение нормальное. При alpha = {alpha}'\n",
    "    \n",
    "    return func_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to contents](#contents)\n",
    "\n",
    "<h3 id='levenefunc'>Функция проверки равнества дисперсий от Левена.</h3>\n",
    "(недоработанная)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция проверки на равенство дисперсий от Левена.\n",
    "def levene(*args):\n",
    "    return 'Дисперсии не равны. Нужно использовать непараметрический тест. При alpha = 0.05' if stats.levene(*args)[1] <= alpha else 'Дисперсии равны. При alpha = 0.05'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to contents](#contents)\n",
    "\n",
    "<h3 id='maintagsworkfunc'>Функция для работы с [\"строками\", \"внутри\", \"списка тэгов\", \"1 Adult\", \"2 Adults\"]</h3>\n",
    "ОСНОВНАЯ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_specific_string(tags_column, words_to_transform, list_of_exceptions=[], hard_split_list=[], replacement_dict=False):\n",
    "    from tqdm.notebook import tqdm_notebook\n",
    "    \"\"\"\n",
    "    Функция преобразования str элементов списка.\n",
    "    Типа когда у нас есть список, в котором есть строки, которые нужно побить по какому-то слову или словосочетанию.\n",
    "    И вернуть обратно такой же список, но с побитыми строками. \n",
    "    P.S. Почему не сделать изначально весь столбец .lower() и потом использовать функцию?\n",
    "    Приятней же искать в выводе файла 'output_transformed.txt' следующие слова на обрезку, которые начинаются с заглавной буквы.\n",
    "    Ну, лично мне. Можно изначально сделать всё lower и оно всё равно будет работать.\n",
    "\n",
    "    Args:\n",
    "        tags_column (series): столбец со списками, которые с элементами str.\n",
    "        split_word (str): ключевое слово или словосочетание.\n",
    "        list_of_exceptions (list): список исключений, которые мы не хотим видеть в итоговом списке.\n",
    "        hard_split_list (list): список слов, по которым строка будет делиться, не склеивая остатки (with, and, etc).\n",
    "        replacement_dict (dict): словарь замены одинакового, написанного по-разному.\n",
    "\n",
    "    Returns:\n",
    "        list: возвращаем побитый список.))\n",
    "        \n",
    "    Пример использования: \n",
    "    нужно вставить код ниже в ячейку после функции и поменять your_df[tags_col] на свой дф с столбцом тэгов и везде заменить your_df (выделить your_df и на клаве: ctrl+D или cmd+D если мак). \n",
    "    ----------------------------------------------------------------------------------------------------------\n",
    "        words_to_transform = [# любые слова или словосочетания, по которым хотим побить каждую строку, порядок важен] # типа сначала сложносоставные слова \"1 2 Adult\", а потом \"2 Adult\". У себя я разбил на 2 тего: \"1 2 Adult\" - 1 адулт и 2 адулт. Чувак же указал.\n",
    "        list_of_exceptions = [# слова исключения, которым в списках не место (with, and)] # Тут порядок не важен.\n",
    "        hard_split_list = [# слова, по которым строка будет делиться, без склеивания остатков (with, and, etc)]\n",
    "        replacement_dict = {# словарь, если хотим заменить какие-то слова или словосочетания} # И тут тоже.\n",
    "        \n",
    "        # Помещаем преобразование в отдельный столбец.\n",
    "        your_df['transformed'] = transform_specific_string(your_df[tags_col], words_to_transform=words_to_transform, list_of_exceptions=list_of_exceptions, replacement_dict=replacement_dict)\n",
    "        \n",
    "        # vvvvvvvvvvvvvvvvvvv  Ниже работа с выводом  vvvvvvvvvvvvvvvvvvv\n",
    "        # Если хотим посмотреть, что у нас получилось используем следующее:\n",
    "        # Полезная штука, если данных много и в аутпуте всё не помещается.\n",
    "        # В текущей директории создаётся файл в котором находится список уникальных элементов столбца. Ну или не совсем уникальных, если не делали .lower() Нужно вставить после .explode()\n",
    "        output_data = list(set(your_df['transformed'].explode()))\n",
    "        \n",
    "        with open('output_transformed.txt', 'w') as f:\n",
    "            for item in output_data:\n",
    "                f.write(\"%s\\n\" % item)\n",
    "        \n",
    "        # Можно проверить, как оно бьётся по конкретному слову, словосочетанию:\n",
    "        print(your_df.loc[[' 2 adult '.lower() in str(i) for i in your_df['tags'].apply(lambda x: [i.lower() for i in x])]])\n",
    "        \n",
    "        # Можно вывести 2 столбца со списками и посмотреть, как идут дела.)\n",
    "        your_df.loc[:, ['tags', 'transformed']]\n",
    "        \n",
    "        # Если есть ещё одна функция, которая делает то же самое, то можно проверить столбцы на не равенство после работы двух функций. \n",
    "        # Всё должно быть False. И если есть True, убираем вальюкаунтс и спереди добавляем df.loc[]. И смотрим, что где плохо бьётся.\n",
    "        (df_main['transformed'].apply(lambda x: [i.lower() for i in x]) != df_main['transformed_2'].apply(lambda x: [i.lower() for i in x])).value_counts()\n",
    "    ----------------------------------------------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    # Внутренняя функция, которая лупит конкретную строку.\n",
    "    def transform(row, split_word, list_of_exceptions, hard_split_list, replacement_dict):\n",
    "        # Возвращаемый список.\n",
    "        new_row = []\n",
    "        # Цикл по элементам строки.\n",
    "        for item in row:\n",
    "            # Создаём временную переменную, которая является элементом списка + пробелы с двух сторон, потому что реплейс мы делаем по отдельно стоящему слову, т.е. слову, которое окружено с двух сторон пробелами.\n",
    "            temp_item = ' ' + item.lower() + ' '\n",
    "            # Условие на вхождение слова разделителя в элемент списка. Я сначала сделал через re.search, но re работает дольше, потому переписал через вот так.\n",
    "            if split_word.lower() in item.lower():\n",
    "                # Если это слова, по которым нужно именно разделить на отдельные части, то:\n",
    "                if split_word.lower() in hard_split_list:\n",
    "                    # Заменяем слово разделитель на |||||\n",
    "                    temp_item = temp_item.replace(f' {split_word.lower()} ', '|||||')\n",
    "                    # Если реплейс сработал то:\n",
    "                    if '|||||' in temp_item:\n",
    "                        # Делим по реплесу.\n",
    "                        split_items = temp_item.split('|||||')\n",
    "                        # Добавляем само слово разделитель и остатки после разделения - отдельно.\n",
    "                        new_row.extend([split_word.strip(), *[i.strip() for i in split_items]])\n",
    "                    else:\n",
    "                        # Иначе записываем просто итем.\n",
    "                        new_row.append(item.strip())\n",
    "                # Если же это другие слова или словосочетания, то:\n",
    "                else:\n",
    "                    # Заменяем разделитель на |||||.\n",
    "                    temp_item = temp_item.replace(f' {split_word.lower()} ', '|||||')\n",
    "                    # Тут как раз проверка на \"сработал реплейс или нет\". \n",
    "                    # У меня был момент: входящая строка: 'Two Connecting Superior Double Rooms', слово-разделитель: 'Superior Double Room'. \n",
    "                    # Входит? Входит. Реплейсит? - нет. Ничего не возвращает и я добавляю просто слово разделитель, упуская саму строку. \n",
    "                    # Потому сначала вот это условие.\n",
    "                    if '|||||' in temp_item:\n",
    "                        # Если да, то бьём и\n",
    "                        split_items = temp_item.split('|||||')\n",
    "                        # добавляем само слово разделитель, и склеиваем остатки по пробелу.\n",
    "                        new_row.extend([split_word.strip(), ' '.join([i.strip() for i in split_items])])\n",
    "                    else:\n",
    "                        # Иначе записываем просто итем.\n",
    "                        new_row.append(item.strip())\n",
    "            else:\n",
    "                # Как и здесь.\n",
    "                new_row.append(item.strip())\n",
    "                \n",
    "        # Чистим список.\n",
    "        new_row = [item for item in new_row if item != '' and item not in list_of_exceptions]\n",
    "        # Переименовываем элементы списка только тогда, когда это нужно и переименование зависит от слова-разделителя.\n",
    "        if replacement_dict and split_word in replacement_dict:\n",
    "            new_row = [replacement_dict[split_word] if split_word in item else item for item in new_row]\n",
    "\n",
    "        return new_row\n",
    "    \n",
    "    ##########vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv##############\n",
    "    # Общий прогресс прогресс бара от 0 до 100% и Инициализация прогресс бара с настройками отображения.\n",
    "    total_progress = 100\n",
    "    with tqdm_notebook(total=total_progress, desc=f\"Cell progress: \", bar_format='{desc}: {percentage:.0f}%|{bar}| {n:.0f}/{total_fmt} [Elapsed: {elapsed} m/s, Remaining: {remaining} m/s]') as pbar:\n",
    "        cumulative_progress = 0 # Чтобы прогресс бар работал, как надо и завершался всегда на 100.\n",
    "    ##########^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^##############\n",
    "        \n",
    "        # Здесь сам цикл, в котором мы перебираем каждое слово и передаём его в каждую строку, которая является списком.))\n",
    "        # И обрабатываем это слово с помощью функции transform для каждой строки.\n",
    "        for i, word in enumerate(words_to_transform):\n",
    "            tags_column = tags_column.apply(transform, split_word=word, list_of_exceptions=list_of_exceptions, hard_split_list=hard_split_list, replacement_dict=replacement_dict)\n",
    "            \n",
    "    ##########vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv##############        \n",
    "            # Рассчитываем размер шага на основе оставшегося прогресса и оставшихся итераций.\n",
    "            remaining_iterations = len(words_to_transform) - i # вот тут пригодился индекс i\n",
    "            remaining_progress = total_progress - cumulative_progress\n",
    "            step_size = remaining_progress / remaining_iterations\n",
    "            \n",
    "            # Обновляем внутренний и общий прогресс.\n",
    "            cumulative_progress += step_size\n",
    "            pbar.update(step_size)\n",
    "    ##########^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^##############\n",
    "\n",
    "    return tags_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to contents](#contents)\n",
    "\n",
    "<h3 id='retagsworkfunc'>Функция для работы с [\"строками\", \"внутри\", \"списка тэгов\", \"1 Adult\", \"2 Adults\"]</h3>\n",
    "Второстепенная с использованием модуля re."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_specific_string_2(tags_column, words_to_transform, list_of_exceptions=[], replacement_dict=False): # Рабочий вариант №2\n",
    "    from tqdm.notebook import tqdm_notebook\n",
    "    import re\n",
    "    \"\"\"\n",
    "    Функция преобразования str элементов списка.\n",
    "    Типа когда у нас есть список, в котором есть строки, которые нужно побить по какому-то слову или словосочетанию.\n",
    "    И вернуть обратно такой же список, но с побитыми строками. \n",
    "    P.S. Почему не сделать изначально весь столбец .lower() и потом использовать функцию, но.. \n",
    "    Приятней же искать в выводе файла 'output_transformed.txt' следующие слова на обрезку, которые начинаются с заглавной буквы.\n",
    "    Ну, лично мне. \n",
    "\n",
    "    Args:\n",
    "        tags_column (series): столбец со списками, которые с элементами str.\n",
    "        split_word (str): ключевое слово или словосочетание.\n",
    "        list_of_exceptions (list): список исключений, которые мы не хотим добавлять при сплите: with, and.\n",
    "        replacement_dict (dict): словарь замены одинакового, написанного по-разному.\n",
    "\n",
    "    Returns:\n",
    "        list: возвращаем побитый список.))\n",
    "        \n",
    "    Пример использования: \n",
    "    нужно вставить код ниже в ячейку после функции и поменять your_df[tags_col] на свой дф с столбцом тэгов и везде заменить your_df (выделить your_df и на клаве: ctrl+D или cmd+D если мак). \n",
    "    ----------------------------------------------------------------------------------------------------------\n",
    "        words_to_transform = [# любые слова или словосочетания, по которым хотим побить каждую строку, порядок важен] # типа сначала сложносоставные слова \"1 2 Adult\", а потом \"2 Adult\". У себя я разбил на 2 тего: \"1 2 Adult\" - 1 адулт и 2 адулт. Чувак же указал.\n",
    "        list_of_exceptions = [# слова исключения, которым в списках не место (with, and)] # Тут порядок не важен.\n",
    "        replacement_dict = {# словарь, если хотим заменить какие-то слова или словосочетания} # И тут тоже.\n",
    "        \n",
    "        # Помещаем преобразование в отдельный столбец.\n",
    "        your_df['transformed'] = transform_specific_string(your_df[tags_col], words_to_transform=words_to_transform, list_of_exceptions=list_of_exceptions, replacement_dict=replacement_dict)\n",
    "        \n",
    "        # vvvvvvvvvvvvvvvvvvv  Ниже работа с выводом  vvvvvvvvvvvvvvvvvvv\n",
    "        # Если хотим посмотреть, что у нас получилось используем следующее:\n",
    "        # Полезная штука, если данных много и в аутпуте всё не помещается.\n",
    "        # В текущей директории создаётся файл в котором находится список уникальных элементов столбца. Ну или не совсем уникальных, если не делали .lower() Нужно вставить после .explode()\n",
    "        output_data = list(set(your_df['transformed'].explode()))\n",
    "        \n",
    "        with open('output_transformed.txt', 'w') as f:\n",
    "            for item in output_data:\n",
    "                f.write(\"%s\\n\" % item)\n",
    "        \n",
    "        # Можно проверить, как оно бьётся по конкретному слову, словосочетанию:\n",
    "        print(your_df.loc[[' 2 adult '.lower() in str(i) for i in your_df['tags'].apply(lambda x: [i.lower() for i in x])]])\n",
    "        \n",
    "        # Можно вывести 2 столбца со списками и посмотреть, как идут дела.)\n",
    "        your_df.loc[:, ['tags', 'transformed']]\n",
    "        \n",
    "        # В эту версию я добавлю отладчик, т.к. основная версия другая, без модуля re.\n",
    "        new_row = []\n",
    "        item = 'Two Connecting Superior Double Rooms'\n",
    "        split_word_list = ['Superior Double Room']\n",
    "        \n",
    "        for split_word in split_word_list:\n",
    "            temp_item = ' ' + item.lower() + ' '\n",
    "            \n",
    "            if split_word.lower() in item.lower():\n",
    "                if split_word.lower() == 'with' or split_word.lower() == 'and':\n",
    "                    temp_item = temp_item.replace(f' {split_word.lower()} ', '|||||')\n",
    "                    split_items = temp_item.split('|||||')\n",
    "                    new_row.extend([split_word.strip(), *[i.strip() for i in split_items]])\n",
    "                else:\n",
    "                    \n",
    "                    temp_item = temp_item.replace(f' {split_word.lower()} ', '|||||')\n",
    "                    \n",
    "                    if '|||||' in temp_item:\n",
    "                        print(temp_item) # Принтами можно проверить каждый шаг.\n",
    "                        split_items = temp_item.split('|||||')\n",
    "                        new_row.extend([split_word.strip(), ' '.join([i.strip() for i in split_items if len(i) > 1])])\n",
    "                    else:\n",
    "                        new_row.append(item.strip())\n",
    "            else:\n",
    "                new_row.append('xyu')\n",
    "        new_row\n",
    "        \n",
    "        # Проверяем на равенство после работы двух функций.\n",
    "        (df_main['transformed'].apply(lambda x: [i.lower() for i in x]) != df_main['transformed_2'].apply(lambda x: [i.lower() for i in x])).value_counts()\n",
    "    ----------------------------------------------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    # Внутренняя функция, которая лупит конкретную строку.\n",
    "    def transform(row, split_word, list_of_exceptions, replacement_dict):\n",
    "        # Возвращаемый список.\n",
    "        new_row = []\n",
    "        # Цикл по элементам строки.\n",
    "        for item in row:\n",
    "            # Используется модуль re для поиска слова, которое окружено с двух сторон пробелами и проверяет его на вхождение в элемент списка.\n",
    "            if re.search(r'\\b' + split_word.lower() + r'\\b', item.lower()):\n",
    "                # Если это слова, по которым нужно именно разделить на отдельные части, то:\n",
    "                if split_word.lower() == 'with' or split_word.lower() == 'and':\n",
    "                    # бьём по нему элемент списка и\n",
    "                    split_items = item.lower().split(split_word.lower())\n",
    "                    # добавляем само слово разделитель и остальное, что осталось в разбитом элементе.\n",
    "                    new_row.extend([split_word.strip(), *[i.strip() for i in split_items]])\n",
    "                # Если же это другие слова или словосочетания, то:\n",
    "                else:\n",
    "                    # бьём,\n",
    "                    split_items = item.lower().split(split_word.lower())\n",
    "                    # добавляем само слово и склеиваем остатки.\n",
    "                    new_row.extend([split_word.strip(), ' '.join([i.strip() for i in split_items])])\n",
    "            # Если же слово не совпадает:\n",
    "            else:\n",
    "                # то просто добавляем элемент списка неизменным.\n",
    "                new_row.append(item.strip())\n",
    "        \n",
    "        # Чистим строку.\n",
    "        new_row = [item for item in new_row if len(item) > 2 and item not in list_of_exceptions]\n",
    "        # Переименовываем элементы списка только тогда, когда это нужно и переименование зависит от слова-разделителя.\n",
    "        if replacement_dict and split_word in replacement_dict:\n",
    "            new_row = [replacement_dict[split_word] if split_word in item else item for item in new_row]\n",
    "\n",
    "        return new_row\n",
    "    \n",
    "    # Общий прогресс можно убрать. Просто у меня файл достаточно большой и это занимает пару минут. Так чисто, декоративное.\n",
    "    ##########vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv##############\n",
    "    # Общий прогресс прогресс бара от 0 до 100% и Инициализация прогресс бара с настройками отображения.\n",
    "    total_progress = 100\n",
    "    with tqdm_notebook(total=total_progress, desc=f\"Cell progress: \", bar_format='{desc}: {percentage:.0f}%|{bar}| {n:.0f}/{total_fmt} [Elapsed: {elapsed} m/s, Remaining: {remaining} m/s]') as pbar:\n",
    "        cumulative_progress = 0 # Чтобы прогресс бар работал, как надо и завершался всегда на 100%.\n",
    "    ##########^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^##############\n",
    "        \n",
    "        # Здесь сам цикл, в котором мы перебираем каждое слово и передаём его в каждую строку, которая является списком.))\n",
    "        # И обрабатываем это слово с помощью функции transform для каждой строки.\n",
    "        for i, word in enumerate(words_to_transform):\n",
    "            tags_column = tags_column.apply(transform, split_word=word, list_of_exceptions=list_of_exceptions, replacement_dict=replacement_dict)\n",
    "            \n",
    "    ##########vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv##############        \n",
    "            # Рассчитываем размер шага на основе оставшегося прогресса и оставшихся итераций.\n",
    "            remaining_iterations = len(words_to_transform) - i # вот тут пригодился индекс i\n",
    "            remaining_progress = total_progress - cumulative_progress\n",
    "            step_size = remaining_progress / remaining_iterations\n",
    "            \n",
    "            # Обновляем общий прогресс.\n",
    "            cumulative_progress += step_size\n",
    "            pbar.update(step_size)\n",
    "    ##########^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^##############\n",
    "\n",
    "    return tags_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to contents](#contents)\n",
    "\n",
    "<h3 id='multiplemodelstraining'>Функция для тренировки нескольких моделей, используя несколько алгоритмов ML за 1 заход</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_models_training(df, target_feature, list_of_ml_algs, params_dict=False, random_state=42, test_size=0.25):\n",
    "    print(f'{Fore.RED}КОРОЛЬ АРТУР НА НАС НАПАЛИ!!!{Fore.RESET}') # https://www.youtube.com/watch?v=-XuyTXj_5F8&t=6s хз, чёт вспомнилось...\n",
    "    \"\"\" Freeware by Akialema.\n",
    "    Сие ваяние было вдохновлено... В телеграмме периодически запускают эвент типа мол: \"попробуйте себя в роли датасаентиста\",\n",
    "    который длится 3 дня. И на третий день предлагалось сделать вот это вот, что происходит в проекте-3 и что происходит в этой функции: \n",
    "    х-трэйны, у-трэйны, тэсты, предсказания, кто там на что делится.. тыща параметров и т.д. И финальным заданием было попробовать \n",
    "    перебрать несколько алгоритмов, чтобы получить наименьшее некое число. И вот в тот момент я понял, что для этого дела нужен цикл.\n",
    "    С графическим представлением. Потому что не даром их так много. А потом я посмотрел прекрасную работу Андрея Волкова и такой: \n",
    "    вот же она, эта картинка.) И чтоб не плодить простыню х-трэйнов-у-трэйнов, её можно просто обернуть в цикл и в функцию.\n",
    "    Я сначала делал график прям в функции, чтоб такой ждёшь, ждёшь выполнения ячейки и хочется какой-то интерактивности прогресса \n",
    "    с последующим финальным представлением графика сравнения МАРЕ на сладкое. Но потом понял, что лучше делать картиночку отдельной ячейкой.\n",
    "    Короче кайфанул и доволен. Да и я хз, может есть уже реализация подобного, я ж новичок. Потому быстро накалякал своё. Вообще не понимаю,\n",
    "    что там происходит и правильно ли. Но работает. Я просто хорошо задаю вопросы чату гпт.))) \n",
    "    Забирайте, если понравилось.)\n",
    "    \n",
    "    P.S. функция не идеальна. Я не знаю названия всех алгоритмов. Какие чат гпт сказал, что топ, те и тут. Например функция явно не для \n",
    "    AutoML, там какая-то длинная реализация.. в работе Андрея.)) ыы Сам я пока не рискну.) \n",
    "    Явно я эту функцию ещё дополню. Или пойму, что это пустая трата времени. \n",
    "    P.S.2 ЕСЛИ ВЫ ЗНАЕТЕ, КАК СДЕЛАТЬ ПЛАВНЫЙ ПРОГРЕСС БАР ПО ЯЧЕЙКЕ - РАССКАЖИТЕ МНЕ. Пожалуйста. <^-------- Можно не читать.\n",
    "    \n",
    "    Функция проведения нескольких (или одной) тренировок моделей.\n",
    "    \n",
    "    ---> На вход подаём итоговый датафрейм, в котором только числа и нет нанок, \n",
    "         целевой признак,\n",
    "         список названий алгоритмов для тренировки,\n",
    "         и словарь с параметрами для алгоритмов.\n",
    "\n",
    "    Args:\n",
    "        df (pd.Dataframe): итоговый дф.\n",
    "        target_feature (str): целевой признак.\n",
    "        list_of_ml_algs (list): список Агрессоров, каждый из них в str формате.)) (агрессоров может быть как 1, так и сколько душе угодно, но важно, чтобы они соответствовали структуре \"инит класса -> тренировка -> предсказание\").\n",
    "        params_dict (dict, optional): словарь вида {alg: [{параметры алгоритма}, {параметры фита}]}, по которому создаётся датафрейм с параметрами. Первый внутренний словарь для параметров алгоритма, второй - для фита. Defaults to False.\n",
    "        random_state (int, optional): я хз, что это, я так до сих пор и не прочувствовал np.random.seed. Defaults to 42.\n",
    "        test_size (float, optional): размер тестовой выборки. Defaults to 0.25.\n",
    "\n",
    "    Returns:\n",
    "        pd.Dataframe: На выход получаем датафрейм с натренированными моделями, числом МАРЕ и чем-нибудь ещё.\n",
    "        \n",
    "    Пример работы. Нужно скопировать код ниже и вставить в ячейку после функции.\n",
    "    ------------------------------------------------------------------------------------------------------\n",
    "    # Здесь нужно выбрать, какими алгоритмами будем гонять числа.)\n",
    "    list_of_ml_algs = ['RandomForestRegressor', 'ExtraTreesRegressor', 'BaggingRegressor', 'GradientBoostingRegressor', 'CatBoostRegressor', 'AdaBoostRegressor', 'DecisionTreeRegressor']\n",
    "    \n",
    "    # Параметры для каждого алгоритма. Порядок не важен, как и указание параметров для всех алгоритмов. \n",
    "    # Можно указать параметры только для тех, для которых нужно, остальные отработают по своим дефолтным параметрам.\n",
    "    # Важна структура {alg: [{параметры алгоритма}, {параметры фита}]}, иначе можно хлебнуть горя.)) Индекс-столбец-столбец.\n",
    "    # А можно и не делать этот словарь вовсе. Алгоритмы отработают по своим настройкам по-умолчанию.\n",
    "    params_dict = {\n",
    "            'RandomForestRegressor': [{'n_estimators': 10, 'n_jobs': -1, 'random_state': 42}, {}],\n",
    "            'ExtraTreesRegressor': [{'n_estimators': 10, 'max_depth': 3}, {}],\n",
    "            'BaggingRegressor': [{'n_estimators': 10}, {}],\n",
    "            'GradientBoostingRegressor': [{'n_estimators': 10, 'learning_rate': .1}, {}],\n",
    "            'CatBoostRegressor': [{'iterations': 10000, 'learning_rate': .1, 'depth': 3, 'verbose': False}, {}],\n",
    "            'AdaBoostRegressor': [{'n_estimators': 10, 'learning_rate': 1.0}, {}],\n",
    "            'DecisionTreeRegressor': [{}, {}]\n",
    "    } # везде эстиматоры 10, чтоб оно отработало быстро и показало работу. \n",
    "    \n",
    "    # Здесь нужно вставить свой end_df, свой 'целевой_признак', решить, какими алгоритмами будем баловаться и с какими параметрами.)\n",
    "    any_var_name = multiple_models_training(end_df, 'reviewer_score', list_of_ml_algs=list_of_ml_algs, params_dict=params_dict)\n",
    "    \n",
    "    any_var_name\n",
    "    ------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    Нужно быть внимательным/ной, чтоб всё сработало, как надо. Я вроде всё расписал.\n",
    "    \"\"\"\n",
    "    \n",
    "    from sklearn import metrics\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from colorama import Fore, Back, Style # Чтоб аутпут был цветастый.\n",
    "    from tqdm.notebook import tqdm\n",
    "    import pandas as pd\n",
    "    import time\n",
    "    \n",
    "    # Если мы передаём параметры словарём извне.\n",
    "    if params_dict:\n",
    "        params_dict_func = {alg: [{}, {}] for alg in list_of_ml_algs} # Создаётся словарь следующей формы: {alg: [{}, {}]}, для каждого элемента списка с названиями алгоритмов.\n",
    "        params_dict_func.update(params_dict) # Обновление словаря словарём извне.\n",
    "        params_df = pd.DataFrame.from_dict(params_dict_func, orient='index', columns=['alg_params', 'fit_params']) # ДФ по словарю.\n",
    "    # Если мы не передаём параметры в функцию, то все параметры будут по умолчанию для каждого алгоритма.\n",
    "    else:\n",
    "        params_df = pd.DataFrame([[{},{}]], index=list_of_ml_algs, columns=['alg_params', 'fit_params'])\n",
    "    \n",
    "    X = df.drop(columns=[target_feature], axis=1) # дропаем целевой признак из икса.\n",
    "    y = df[target_feature] # и помещаем его в игрек.\n",
    "    \n",
    "    # Эти штуки.. Я надеюсь они в правильном порядке стоят. Вроде бы у всех так.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    list_of_algs_output = [] # список по которому будем формировать выходящий датафрейм.\n",
    "    \n",
    "    total_progress = 100  # Общий прогресс прогресс бара от 0 до 100%\n",
    "    \n",
    "    # Инициализация прогресс бара с настройками отображения.\n",
    "    with tqdm(total=total_progress, desc=f\"Cell progress: \", bar_format='{desc}: {percentage:.0f}%|{bar}| {n:.0f}/{total_fmt} [Elapsed: {elapsed} m/s]') as pbar:\n",
    "        \n",
    "        cumulative_progress = 0 # Чтобы прогресс бар работал, как надо и завершался всегда на 100.\n",
    "        \n",
    "        # Цикл по списку алгоритмов.\n",
    "        for i, alg in enumerate(list_of_ml_algs): # i пригодился в вычислении шага прогрессбара.\n",
    "            \n",
    "            start_alg_time = time.time() # Время начала работы обучения модели.\n",
    "            \n",
    "            # Инит алгоритма через globals, т.к. название алгоритма == str. (можно через eval(), но гпт чату не понравилась эта идея) \n",
    "            # Параметры == параметры столбца 'class_params' для строки с именем alg.\n",
    "            model = globals()[alg](**params_df.loc[alg, 'alg_params']) \n",
    "            \n",
    "            model.fit(X_train, y_train, **params_df.loc[alg, 'fit_params']) # Здесь сама тренировка. Параметры из столбца 'fit_params'.\n",
    "            \n",
    "            current_time = time.time() # Время окончания работы алгоритма.\n",
    "            \n",
    "            y_pred = model.predict(X_test) # Предсказание.\n",
    "            \n",
    "            # Делаем МАПЕ.\n",
    "            mape = metrics.mean_absolute_percentage_error(y_test, y_pred) * 100\n",
    "            # Я не знаю, что это и для чего это здесь. Просто добавил, чтобы убедиться, что в вывод можно ещё чего-нибудь прикрутить.\n",
    "            # Соответственно, нужно обновлять аппенд и формирование столбцов итогового дф.\n",
    "            accuracy = model.score(X_test, y_test) \n",
    "            \n",
    "            list_of_algs_output.append([alg, mape, accuracy, model]) # Добавляем всю эту петрушку в список, по которому будет формироваться таблица.\n",
    "            \n",
    "            ##########################################################################################################################\n",
    "            \n",
    "            # Вычисление времени для принта ниже. Типа вот, мол, модель обучилась, поздравляю.\n",
    "            execution_time_alg = current_time - start_alg_time\n",
    "            # Можно прикрутить, что-то другое для принта.\n",
    "            print(f\"Model training completed! With: {str(round(execution_time_alg, 2)) + ' sec.' if execution_time_alg < 60 else str(round(execution_time_alg/60, 2)) + ' min.'} Alg: {Fore.CYAN}{alg}{Fore.RESET}.\") # чтоб секунды были секундами, а минуты - минутами.\n",
    "            time.sleep(0.2) # Сон я добавил, потому что сначала прогресс бар отрабатывает, а потом принт. А надо наоборот.)\n",
    "            \n",
    "            # Рассчитываем размер шага на основе оставшегося прогресса и оставшихся итераций.\n",
    "            remaining_iterations = len(list_of_ml_algs) - i # вот тут пригодился индекс i\n",
    "            remaining_progress = total_progress - cumulative_progress\n",
    "            step_size = remaining_progress / remaining_iterations\n",
    "            \n",
    "            # Обновляем общий прогресс. Все округления происходят при инициализации прогрессбара.\n",
    "            cumulative_progress += step_size\n",
    "            pbar.update(step_size)\n",
    "        \n",
    "    func_df = pd.DataFrame(list_of_algs_output, columns=['alg_name', 'mape_percent', 'accuracy', 'model']).sort_values(by='mape_percent').reset_index(drop=True) # ДФ-чик \n",
    "    \n",
    "    print(f'{Fore.GREEN}Mission accomplished{Fore.RESET}.')\n",
    "    # Возвращаем дф, в котором мапе и сама модель. \n",
    "    return func_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to contents](#contents)\n",
    "\n",
    "<h3 id='vadersentanalsec'>Функция для формирования датафрейма с определением тональности текста по алгоритму Вейдера.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_sentiment_analyser(df, targ_col):\n",
    "    \"\"\"Функция для формирования датафрейма с определением тональности текста по алгоритму Вейдера.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): incoming df\n",
    "        targ_col (str): target column\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: df with neg, neu, pos, compound from Vader.\n",
    "        \n",
    "        looks like:\n",
    "        __|targ_col_neg|targ_col_neu|targ_col_pos|targ_col_comp|\n",
    "        0 |         0.1|         0.8|         0.2|         0.64|\n",
    "        etc.\n",
    "    \"\"\"\n",
    "    # Импорт необходимых библиотек. Библиотеки лучше выносить за рамки функции.\n",
    "    import pandas as pd\n",
    "    from tqdm.notebook import tqdm\n",
    "    import nltk\n",
    "    from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "    nltk.download('vader_lexicon')\n",
    "    \n",
    "    # Инит класса.\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    # Словарь для записи результатов.\n",
    "    res = {}\n",
    "    \n",
    "    total_progress = 100\n",
    "    with tqdm(total=total_progress, desc=f\"Cell progress: \", bar_format='{desc}: {percentage:.0f}%|{bar}| {n:.0f}/{total_fmt} [Elapsed: {elapsed} m/s, Remaining: {remaining} m/s]') as pbar:\n",
    "        cumulative_progress = 0 # Чтобы прогресс бар работал, как надо и завершался всегда на 100.\n",
    "    ##########^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^##############\n",
    "        \n",
    "        # Здесь сам цикл, в котором мы перебираем каждое слово и передаём его в каждую строку, которая является списком.))\n",
    "        # И обрабатываем это слово с помощью функции transform для каждой строки.\n",
    "        for i, text in enumerate(df[targ_col]):\n",
    "            res[i] = sia.polarity_scores(text)\n",
    "            \n",
    "    ##########vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv##############        \n",
    "            # Рассчитываем размер шага на основе оставшегося прогресса и оставшихся итераций.\n",
    "            remaining_iterations = len(df) - i # вот тут пригодился индекс i\n",
    "            remaining_progress = total_progress - cumulative_progress\n",
    "            step_size = remaining_progress / remaining_iterations\n",
    "            \n",
    "            # Обновляем внутренний и общий прогресс.\n",
    "            cumulative_progress += step_size\n",
    "            pbar.update(step_size)\n",
    "    ##########^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^##############\n",
    "    # Переименование столбцов.\n",
    "    new_columns = {col: f\"{targ_col}_{col}\" for col in res[0].keys()}\n",
    "    # Возврат дф-ки с решейпом и переименованием столбцов.\n",
    "    return pd.DataFrame(res).T.rename(columns=new_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to contents](#contents)\n",
    "\n",
    "<h3 id='vadersentanalmain'>Функция для формирования датафрейма с определением тональности текста по алгоритму Вейдера (основная).</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_sentiment_analysis(df, targ_col):\n",
    "    \"\"\"Функция для формирования датафрейма с определением тональности текста по алгоритму Вейдера.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): incoming df\n",
    "        targ_col (str): target column\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: df with neg, neu, pos, compound from Vader.\n",
    "        \n",
    "        looks like:\n",
    "        __|targ_col_neg|targ_col_neu|targ_col_pos|targ_col_comp|\n",
    "        0 |         0.1|         0.8|         0.2|         0.64|\n",
    "        etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Импорт необходимых библиотек. Библиотеки лучше выносить за рамки функции.\n",
    "    import pandas as pd\n",
    "    from tqdm.notebook import tqdm\n",
    "    import nltk\n",
    "    from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "    nltk.download('vader_lexicon')\n",
    "    \n",
    "    # Инит класса.\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    # Словарь для записи результатов.\n",
    "    res = {}\n",
    "    \n",
    "    # Цикл по строкам датафрейма и запись анализа настроения в словарь.\n",
    "    for i, text in enumerate(tqdm(df[targ_col], total=len(df))):\n",
    "        res[i] = sia.polarity_scores(text)\n",
    "    \n",
    "    # Переименование столбцов.\n",
    "    new_columns = {col: f\"{targ_col}_{col}\" for col in res[0].keys()}\n",
    "    # Возврат дф-ки с решейпом и переименованием столбцов.\n",
    "    return pd.DataFrame(res).T.rename(columns=new_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to contents](#contents)\n",
    "\n",
    "<h3 id='robertapolarity'>Функция с алгоритмом Роберты на основе отзывов в твиттере.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roberta_polarity_scores(df, targ_col):\n",
    "    \"\"\"Функция для формирования датафрейма с определением тональности текста по алгоритму Роберты.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): incoming df\n",
    "        targ_col (str): target column\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: df with neg, neu, pos, compound from Vader.\n",
    "        \n",
    "        looks like:\n",
    "        __|targ_col_neg|targ_col_neu|targ_col_pos|\n",
    "        0 |         0.1|         0.8|         0.2|\n",
    "        etc.\n",
    "    \"\"\"\n",
    "    # Подгрузка библиотек. Библиотеки лучше вынести за пределы функции.\n",
    "    from transformers import AutoTokenizer\n",
    "    from transformers import AutoModelForSequenceClassification\n",
    "    from scipy.special import softmax\n",
    "    \n",
    "    # Инит классов, токенизеров.\n",
    "    MODEL = f'cardiffnlp/twitter-roberta-base-sentiment'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "    \n",
    "    # Словарь, по которому формируется выходящий дф.\n",
    "    res = {}\n",
    "    # Цикл по строкам столбца.\n",
    "    for i, text in enumerate(tqdm(df[targ_col], total=len(df))):\n",
    "      # Трай - эксепт здесь для того, что некоторые строки слишком длинные и алгоритм роберты с ними не справляется и выдаёт ошибку.\n",
    "      try: \n",
    "        # Всё, что ниже можно написать в 1 строку, но го в 4.\n",
    "        encoded_text = tokenizer(text, return_tensors='pt') # Тут токенизация слов.\n",
    "        output = model(**encoded_text) # Тут я не помню, какой аутпут был, если интересно - ниже в одну строку и можно обрубить, и посмотреть на каждый этап.\n",
    "        scores = softmax(output[0][0].detach().numpy()) # Здесь происходит экспоненцирование (вот такое вот слово) чисел с помощью softmax.\n",
    "        res[i] = {'roberta_neg': scores[0], 'roberta_neu': scores[1], 'roberta_pos': scores[2]} # Ну и запись в словарь.\n",
    "      except RuntimeError:\n",
    "        print(f'Broke for id: {i}') # если ошибка, то выводим номер строки.\n",
    "\n",
    "    # Переименование столбцов.\n",
    "    new_columns = {col: f\"{targ_col}_{col}\" for col in res[0].keys()}\n",
    "    # Возврат дф-ки с решейпом и переименованием столбцов.\n",
    "    return pd.DataFrame(res).T.rename(columns=new_columns)\n",
    "\n",
    "# Эта строчка выведет настроение конкретной строки текста, т.е. вместо 'example' нужно вставить 'любой текст'.\n",
    "#print(softmax(AutoModelForSequenceClassification.from_pretrained(f'cardiffnlp/twitter-roberta-base-sentiment')(**AutoTokenizer.from_pretrained(f'cardiffnlp/twitter-roberta-base-sentiment')('example', return_tensors='pt'))[0][0].detach().numpy())) # вместо 'example' любой текст"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to contents](#contents)\n",
    "\n",
    "<h3 id='optunafunc'>Optuna (тестовая)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optuna_mult(trial, X_train, y_train, param_dict, estimator, metric, cross_val=None):\n",
    "    \n",
    "    from functools import partial # чтобы вызывать функции с доп параметрами\n",
    "    import optuna\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING) # INFO, WARNING, CRITICAL, ERROR\n",
    "    from sklearn.model_selection import KFold, cross_val_score\n",
    "    from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error, r2_score\n",
    "    \n",
    "    # блок сетки параметров\n",
    "    func_dict = {}\n",
    "    \n",
    "    for i, j in param_dict.items():\n",
    "        \n",
    "        if isinstance(j, dict):\n",
    "            if 'low' in j:\n",
    "                if isinstance(j['low'], float):\n",
    "                    func_dict[i] = trial.suggest_float(**j)\n",
    "                else:\n",
    "                    func_dict[i] = trial.suggest_int(**j)\n",
    "            else:\n",
    "                func_dict[i] = trial.suggest_categorical(**j)\n",
    "        else:\n",
    "            func_dict[i] = j\n",
    "    \n",
    "    # имя для метрики (можно и нужно расширить эту хрень)\n",
    "    func_score_metric = ''\n",
    "    if metric in ['mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_error']:\n",
    "        func_score_metric = 'neg_' + metric\n",
    "\n",
    "    # создаем модель\n",
    "    model = globals()[estimator](**func_dict)\n",
    "    \n",
    "    if cross_val is None:\n",
    "        # обучаем модель\n",
    "        model.fit(X_train, y_train)\n",
    "        score = globals()[metric](y_train, model.predict(X_train))\n",
    "    else:\n",
    "        # задаём параметры кросс-валидации (стратифицированная 5-фолдовая с перемешиванием)\n",
    "        kf = globals()[cross_val](n_splits=5, shuffle=True, random_state=func_dict['random_state'])\n",
    "        # проводим кросс-валидацию  \n",
    "        score = -cross_val_score(estimator=model, X=X_train, y=y_train, \n",
    "                                 scoring=func_score_metric, cv=kf, n_jobs=-1).mean()\n",
    "    return score\n",
    "\n",
    "############################### ниже применение\n",
    "\n",
    "param_dict_lasso = {'alpha': {'name': 'alpha', 'low': 0.05, 'high': 1, 'step': 0.05},\n",
    "                    'max_iter': {'name': 'max_iter', 'low': 1000, 'high': 10000, 'step': 1000},\n",
    "                    'tol': {'name': 'tol', 'low': 0.0001, 'high': 0.001, 'step': 0.00005},\n",
    "                    'random_state': 42}\n",
    "\n",
    "study_lasso = optuna.create_study(study_name=\"Lasso\", direction=\"minimize\")\n",
    "# ищем лучшую комбинацию гиперпараметров n_trials раз\n",
    "study_lasso.optimize(partial(optuna_mult, \n",
    "                             X_train=mid_df_poly, y_train=y, \n",
    "                             param_dict=param_dict_lasso,\n",
    "                             estimator='Lasso',\n",
    "                             metric='mean_squared_error',\n",
    "                             cross_val='KFold'), n_trials=40, show_progress_bar=True)\n",
    "\n",
    "# выводим результаты на обучающей выборке\n",
    "print(f\"Наилучшие значения гиперпараметров {study_lasso.best_params}\")\n",
    "print(f\"MAE на обучающем наборе: {study_lasso.best_value:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to contents](#contents)\n",
    "\n",
    "<h3 id='func_research'>Исследование функции для одной переменной.</h3> сырая"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_function_research(f, ylimits=[-10, 10], xlimits=[-10, 10], x_vals=np.linspace(-10, 10, 100), show_extr=False):\n",
    "    \"\"\"my_function_research \n",
    "\n",
    "    Функция, выводящая сводку для простых функций. Из серии \"исследование функции\"\n",
    "\n",
    "    Args:\n",
    "        f (formula): x**3 + 4*x**2 - 4*x - 16 like\n",
    "        ylimits (list, optional): list of limits to correct visualisation. Defaults to [-10, 10].\n",
    "        xlimits (list, optional): list of limits to correct visualisation. Defaults to [-10, 10].\n",
    "        x_vals (_type_, optional): range of x-nums to.. plot. Defaults to np.linspace(-10, 10, 100).\n",
    "        show_extr (bool, optional): show extremums or not. Try. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        none: this is print func\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import sympy\n",
    "    from IPython.display import Math, display\n",
    "    \n",
    "    def find_extremum_coordinates(df, searchnum):\n",
    "        \n",
    "        \"\"\" \n",
    "        Функция для печати экстремум точек на графике. Функция сырая, потому, если будет ошибка, связанная с вычислением этих точек, то просто\n",
    "        show_extr=False и тогда эта функция не будет отрабатывать.\n",
    "        \n",
    "        df - это таблица о двух столбцах = решение функции для каждого икса из x_vals.\n",
    "        В столбце 0 находятся иксы, а в столбце 1 - игреки.\n",
    "        searchnum - это... корни первой производной.\n",
    "        \n",
    "        \"\"\"\n",
    "    \n",
    "        func_df = pd.DataFrame()\n",
    "        second_der_vals = [sympy.diff(f, x, 2).subs(x, i).evalf() for i in sympy.solve(sympy.diff(f, x))]\n",
    "        \n",
    "        # Бежим по корням.\n",
    "        for i, j in enumerate(searchnum):\n",
    "            \n",
    "            func_series = df.iloc[:, 0].copy()\n",
    "            \n",
    "            # Для каждого корня мы ищем в цикле while диапазон чисел (до 10 штук), которые приблизительно равны каждому корню.\n",
    "            while len(func_series) > 10:\n",
    "                median = func_series.median()\n",
    "                \n",
    "                # Здесь находится костыль. Т.е. у линспейса от -10 до 10 медиана = 0 \n",
    "                # и если корень производной = 0, то берем дапазон чисел вокруг нуля в кол-ве 9 шт. (-4, +5)\n",
    "                if j == 0:\n",
    "                    func_series = func_series.iloc[len(func_series)//2 - 4:len(func_series)//2 + 5]\n",
    "                \n",
    "                elif j < median:\n",
    "                    func_series = func_series.loc[func_series <= median]\n",
    "                    \n",
    "                elif j > median:\n",
    "                    func_series = func_series.loc[func_series >= median]\n",
    "            \n",
    "            # Далее смотрим, если значение второй производной < 0 - значит ищем максимальное значение, которое является экстремумом,\n",
    "            # И то же самое, если взначение второй производной > 0: ищем минимальное.\n",
    "            # А вот для = 0 снова костыль. Я беру медиану. Поэтому в строке выше (-4, +5), чтоб было нечётное кол-во.\n",
    "            func_series_df = df.iloc[func_series.index]\n",
    "            len_of_pos = len(func_series_df.loc[func_series_df[1] > 0])\n",
    "            len_of_neg = len(func_series_df.loc[func_series_df[1] < 0])\n",
    "            \n",
    "            # Ниже замес следующий:\n",
    "            # Если вторая производная < 0, то берем максимальное значение по у, если нет, то наоборот, если 0, то медиану берем (костыль)\n",
    "            # Далее идёт.. в столбце могут быть не только одни + или они -, но и +/- и тогда сравниваем длины + и -, \n",
    "            # и берем нужный максимум или минимум по тому списку, который больше.\n",
    "            # Т.к. там может быть резкий скачок, если например внизу парабола, которая смотрит вниз, а через 0 идёт кубическая парабола.\n",
    "            # И там, где заканчивается парабола начинается куб.парабола. Ну типа они обе стремятся, скажем, к вертикальной асимптоте -1.\n",
    "            if second_der_vals[i] < 0: \n",
    "                if all(func_series_df[1] > 0) or all(func_series_df[1] < 0):\n",
    "                    func_df[i] = func_series_df.loc[func_series_df[1].idxmax()]\n",
    "                else:\n",
    "                    if len_of_pos > len_of_neg:\n",
    "                        func_df[i] = func_series_df.loc[func_series_df[1] > 0].loc[func_series_df.loc[func_series_df[1] > 0, 1].idxmax()]\n",
    "                    else:\n",
    "                        func_df[i] = func_series_df.loc[func_series_df[1] < 0].loc[func_series_df.loc[func_series_df[1] < 0, 1].idxmax()]\n",
    "            elif second_der_vals[i] > 0:\n",
    "                if all(func_series_df[1] > 0) or all(func_series_df[1] < 0):\n",
    "                    func_df[i] = func_series_df.loc[func_series_df[1].idxmin()]\n",
    "                else:\n",
    "                    if len_of_pos > len_of_neg:\n",
    "                        func_df[i] = func_series_df.loc[func_series_df[1] > 0].loc[func_series_df.loc[func_series_df[1] > 0, 1].idxmin()]\n",
    "                    else:\n",
    "                        func_df[i] = func_series_df.loc[func_series_df[1] < 0].loc[func_series_df.loc[func_series_df[1] < 0, 1].idxmin()]\n",
    "            elif second_der_vals[i] == 0:\n",
    "                func_df[i] = func_series_df.loc[func_series_df[1] == func_series_df[1].median()].iloc[0, :] # там получается дф, потому берем строку\n",
    "        return func_df\n",
    "    \n",
    "    x = sympy.Symbol('x')\n",
    "    \n",
    "    first_derivative = sympy.diff(f, x)\n",
    "    display(Math(f\"Первая \\ производная: {sympy.latex(first_derivative)}\"))\n",
    "    \n",
    "    roots = sympy.solve(first_derivative, x)\n",
    "    display(Math(f\"Корень/ни \\ первой \\ производной: {sympy.latex(roots)}\"))\n",
    "    \n",
    "    second_derivative = sympy.diff(first_derivative, x)\n",
    "    display(Math(f\"Вторая \\ производная: {sympy.latex(second_derivative)}\"))\n",
    "    \n",
    "    values_of_second_der = [second_derivative.subs(x, i).evalf() for i in roots]\n",
    "    display(Math(f\"Значение/я \\ второй \\ производной \\ для \\ корней \\ первой: {sympy.latex(values_of_second_der)}\"))\n",
    "    \n",
    "##################### Дальше график\n",
    "    x_vals = x_vals\n",
    "    vals = [f.subs(x, i) for i in x_vals]\n",
    "    plot_df = pd.DataFrame([x_vals, vals]).T.astype(float)\n",
    "    \n",
    "    plt.figure(figsize=(4,4))\n",
    "    \n",
    "    plt.axvline(0, color='black', linewidth=0.5, linestyle='--')\n",
    "    plt.axhline(0, color='black', linewidth=0.5, linestyle='--')\n",
    "    \n",
    "    # Условие можно продолжить, там ещё лог, син, кос, тан и т.д.\n",
    "    if isinstance(sympy.denom(f), sympy.Symbol) or isinstance(sympy.denom(f), sympy.Add) or isinstance(sympy.denom(f), sympy.Pow):\n",
    "        # Ищем, в каких числах знаменатель будет 0, условие для комплексных чисел.\n",
    "        crit_nums = [i.evalf() for i in sympy.solve(sympy.denom(f), x) if not isinstance(i, sympy.core.add.Add)]\n",
    "        # Критических чисел может быть много и для \"многих\" - условие.\n",
    "        if len(crit_nums) > 1:\n",
    "            # Сначала рисуем крайние справа и слева.\n",
    "            for i in crit_nums:\n",
    "                if i == min(crit_nums):\n",
    "                    plt.plot(plot_df.loc[plot_df[0] < i, 0], plot_df.loc[plot_df[0] < i, 1])\n",
    "                elif i == max(crit_nums):\n",
    "                    plt.plot(plot_df.loc[plot_df[0] > i, 0], plot_df.loc[plot_df[0] > i, 1])\n",
    "            # А потом рисуем промежутки.\n",
    "            # Например у нас списко крит_нумов = [1,2,3,4,5]\n",
    "            # В цикле перебираем: [i, j] [1,2], [2,3], [3,4], [4,5] ну и меньше-больше.\n",
    "            for i, j in zip(crit_nums, crit_nums[1:]):\n",
    "                \n",
    "                plt.plot(plot_df.loc[(plot_df[0] > i) & (plot_df[0] < j), 0], plot_df.loc[(plot_df[0] > i) & (plot_df[0] < j), 1])\n",
    "        # Иначе, если крит_нум - это 1 число \n",
    "        else:\n",
    "            plt.plot(plot_df.loc[plot_df[0] < crit_nums[0], 0], plot_df.loc[plot_df[0] < crit_nums[0], 1])\n",
    "            plt.plot(plot_df.loc[plot_df[0] > crit_nums[0], 0], plot_df.loc[plot_df[0] > crit_nums[0], 1])\n",
    "    # Иначе знаменатель нормальный и просто рисуем.)\n",
    "    else:\n",
    "        plt.plot(x_vals, vals)\n",
    "    # Если хотим видеть метки экстремумов, типа пик выпуклости или впуклости\n",
    "    if show_extr:\n",
    "        \n",
    "        roots_4_coord_df = [float(i.evalf()) for i in roots]\n",
    "        coord_df = find_extremum_coordinates(df=plot_df, searchnum=roots_4_coord_df)\n",
    "        \n",
    "        # Добавляем аннотацию с метками точек экстремумов на оси X\n",
    "        for i, j in enumerate(roots):\n",
    "            x_coord = coord_df.iloc[0, i] # координаты точки\n",
    "            y_coord = coord_df.iloc[1, i]\n",
    "            plt.annotate(f\"Extremum: ${sympy.latex(j)}$\",\n",
    "                         xy=(x_coord, y_coord), \n",
    "                         xytext=(x_coord+1, y_coord),  # Смещаем текст немного вправо\n",
    "                         backgroundcolor='w',\n",
    "                         arrowprops=dict(facecolor='black', shrink=0.03, width=0.7, headwidth=4, headlength=10))\n",
    "    \n",
    "    plt.xticks(list(range(xlimits[0], xlimits[1]+1)), fontsize=6)\n",
    "    plt.yticks(list(range(ylimits[0], ylimits[1]+1)), fontsize=6)\n",
    "    \n",
    "    plt.title('График для входящей функции:')\n",
    "    plt.ylim(ylimits)\n",
    "    plt.xlim(xlimits)\n",
    "    plt.show()\n",
    "    \n",
    "    #return coord_df\n",
    "    #return pd.DataFrame([[i.evalf() for i in roots], values_of_second_der], index=['Корни 1 производной', 'Значения 2 для корней первой']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to contents](#contents)\n",
    "\n",
    "<h3 id='func_research_xyz'>Исследование функции для двух, трёх переменных.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extremums_of_mult_vars_func(f, return_='df', range_of_nums_for_plot=np.linspace(-5, 5, 100)):\n",
    "    \n",
    "    import re\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import sympy as sp\n",
    "    \n",
    "    # Тут я пытался оформить отображение латех формул в ячейках датафрейма... не получилось, оставлю\n",
    "    def render_latex(row):\n",
    "        return [f\"$ {val} $\" for val in row]\n",
    "    \n",
    "    # эта функция делает из x**2 + y**2 -> x<sup>2</sup> + y<sup>2</sup> (это пример)\n",
    "    # короче костыль. я хз как печатать в плотли формулы.(\n",
    "    def latex_to_html_converter(string):\n",
    "    \n",
    "        list_of_crit_symbols = ['(', ')', '+', '-', '/']    \n",
    "        func_str = string\n",
    "        func_str = ''.join([' ' + i + ' ' if i in list_of_crit_symbols else i for i in func_str])\n",
    "        \n",
    "        func_str = func_str.replace('**', '<sup>').replace('*', '')\n",
    "        \n",
    "        if 'sqrt' in func_str:\n",
    "            func_str = func_str.replace('sqrt', '\\u221A')\n",
    "        \n",
    "        if '<sup>' in func_str:\n",
    "            \n",
    "            func_str = func_str.split(' ')\n",
    "            \n",
    "            for i, j in enumerate(func_str):\n",
    "                if '<sup>' in j:\n",
    "                    func_str[i] = func_str[i] + '</sup>'\n",
    "            func_str = ' '.join(func_str)\n",
    "        \n",
    "        return func_str.replace(' ', '')\n",
    "    \n",
    "    # отсортированный список переменных ['x', 'y', ...]\n",
    "    vars = sorted(list({i for i in re.sub(r'sqrt|exp|log|sin|cos', '', str(f)) if i.isalpha()}))\n",
    "    \n",
    "    display(Math(f\"Incoming \\ func: {sp.latex(f)}\"))\n",
    "    display(*[Math(f\"f_{i}': {sp.latex(sp.diff(f, i))}\") for i in vars])\n",
    "    display(Math(f\"f_{{xx}}'': {sp.latex(sp.diff(f, vars[0], 2))}\"))\n",
    "    \n",
    "    # решение для первой производной\n",
    "    critical_points = sp.solve([sp.diff(f, i) for i in vars], vars)\n",
    "    \n",
    "    # проверка, если там только одно решение и возвращается словарь\n",
    "    # т.к. у меня функция заточена под список кортежей\n",
    "    if isinstance(critical_points, dict):\n",
    "        critical_points = [tuple(critical_points.values())]\n",
    "    \n",
    "    # тут нужна проверка на комплексные числа\n",
    "    if not all(j.is_real for i in critical_points for j in i):\n",
    "        print('There is a complex numbers in critical points')\n",
    "        critical_points = [pnt for pnt in critical_points if all(s.is_real for s in pnt)]\n",
    "    \n",
    "    # словарь вторых производных\n",
    "    second_der_dict = {'f_'+i+j: sp.diff(sp.diff(f, i), j) for i in vars for j in vars} #{'f_'+''.join(i): diff(diff(f, i[0]), i[1]) for i in itertools.product(vars, repeat=2)}\n",
    "    \n",
    "    # ниже два условия: если переменных 2 и если 3, и формулы для них\n",
    "    # когда переменных две - в дополнение строится график входящей функции\n",
    "    if len(vars) == 2:\n",
    "        \n",
    "        Hessian = second_der_dict['f_xx'] * second_der_dict['f_yy'] - second_der_dict['f_xy']**2\n",
    "        \n",
    "        import numpy as np\n",
    "        import plotly.graph_objs as go\n",
    "        \n",
    "        f_func = sp.lambdify((vars), f, 'numpy')\n",
    "\n",
    "        # Создание сетки значений\n",
    "        X = range_of_nums_for_plot\n",
    "        Y = range_of_nums_for_plot\n",
    "        X, Y = np.meshgrid(X, Y)\n",
    "        Z = f_func(X, Y)\n",
    "        \n",
    "        # Создание 3D поверхности\n",
    "        surface = go.Surface(z=Z, x=X, y=Y, colorscale='Viridis')\n",
    "        \n",
    "        # Настройка макета. костыль в title. плотли не умеет отображать латех, но умеет хтмл, потому sup-ы. см функцию.\n",
    "        layout = go.Layout(title='Incoming func: ' + latex_to_html_converter(str(f)),#'Incoming func: ' + ' '.join([i + '</sup>' if 'sup' in i else i for i in str(f).replace('**', '<sup>').replace('*', '').split()]),#'&mu;g/m<sup>2</sup>',\n",
    "                           scene = dict(xaxis_title='X',\n",
    "                                        yaxis_title='Y',\n",
    "                                        zaxis_title='Z'),\n",
    "                           width=600,\n",
    "                           height=400)\n",
    "        \n",
    "        # Построение фигуры\n",
    "        fig = go.Figure(data=[surface], layout=layout)\n",
    "        \n",
    "        # Отображение графика\n",
    "        fig.show()\n",
    "    \n",
    "    if len(vars) == 3:\n",
    "        \n",
    "        sdd = second_der_dict\n",
    "        Hessian = sdd['f_xx']*sdd['f_yy']*sdd['f_zz'] + 2*sdd['f_xy']*sdd['f_xz']*sdd['f_yz'] - sdd['f_xx']*sdd['f_yz']**2 - sdd['f_yy']*sdd['f_xz']**2 - sdd['f_zz']*sdd['f_xy']**2\n",
    "    \n",
    "    if return_ == 'df':\n",
    "        # Ну и тут 3 столбца: крит точки, решение для второй производной по хх, т.к. это первый минор и решение уравнения гессиана\n",
    "        return pd.DataFrame([[[float(k.evalf()) for k in point], float(second_der_dict['f_xx'].subs({var: point[i] for i,var in enumerate(vars)}).evalf()), float(Hessian.subs({var: point[i] for i,var in enumerate(vars)}).evalf())] for point in critical_points], columns=['crit_nums', 'first_minor', 'hessian_det'])#.style.format(na_rep='', escape='latex').apply(render_latex)\n",
    "    else:\n",
    "        return pd.DataFrame(np.array(list(second_der_dict.values())).reshape(len(second_der_dict)//len(vars), len(second_der_dict)//len(vars)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to contents](#contents)\n",
    "\n",
    "<h3 id='gradfxinteractive'>Интерактивный график градиентного спуска для функции одной переменной.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grad_fx(f, df, n_iter=20, start_point=0, step=0.1, mn=100, x_range=np.arange(0, 5.0, 0.1), figsize=(6, 6)):\n",
    "    \n",
    "    # https://www.youtube.com/watch?v=OKeZEbJgQKc видео с этим кодом\n",
    "    \"\"\" Пока не оформлено\n",
    "\n",
    "    Пример работы:\n",
    "    \n",
    "    def f(x):\n",
    "    return x*x - 5*x + 5\n",
    "\n",
    "    def df(x):\n",
    "        return 2*x - 5\n",
    "    \n",
    "    plot_grad_fx(f, df)\n",
    "    \"\"\"\n",
    "    \n",
    "    import time\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    from IPython.display import clear_output\n",
    "    \n",
    "    f_plt = [f(x) for x in x_range]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.grid(True)\n",
    "    \n",
    "    ax.plot(x_range, f_plt)\n",
    "    point = ax.scatter(start_point, f(start_point), c='red')\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        # Здесь изменяется шаг сходимости. С большего к меньшему с константой mn, \n",
    "        # т.е. на последней итерации шаг будет 1/100 = 0.01\n",
    "        step = 1 / (np.min((i+1, mn)))\n",
    "        # np.sign возвращает знак, + или -\n",
    "        start_point -= step * np.sign(df(start_point))\n",
    "        \n",
    "        # Обновляем положение точки\n",
    "        point.set_offsets([start_point, f(start_point)])\n",
    "        \n",
    "        # Обновляем график для интерактивного отображения\n",
    "        clear_output(wait=True)\n",
    "        display(fig)\n",
    "        time.sleep(0.05)\n",
    "    \n",
    "    print(start_point)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to contents](#contents)\n",
    "\n",
    "<h3 id='strongcolssearcher'>Функция для автоматического поиска сильных столбцов относительно целевого столбца (тестовая).</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strong_cols_auto_searcher(df, list_of_cat_cols, list_of_num_cols, list_of_agg_func, targ_col, threshold_number=4000):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.feature_selection import f_classif\n",
    "    from tqdm.notebook import tqdm\n",
    "    \n",
    "    \"\"\"strong_cols_auto_searcher:\n",
    "\n",
    "    Функция, которая ищет наиболее значимые столбцы, для предсказания целевого столбца.\n",
    "    т.е. по очереди происходит группировка всех названий столбцов из списка list_of_cat_cols с столбцами из списка list_of_num_cols\n",
    "    с агрегацией функций из списка list_of_agg_func и сразу происходит отсев по пороговому числу threshold_number.\n",
    "    Ну и запись в выходящий датафрейм.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): incoming df.\n",
    "        list_of_cat_cols (list): [['col1'], ['col2'], ['col3', 'col4']] list with lists inside with str in this lists.\n",
    "        list_of_num_cols (list): ['col1', 'col2', 'col3'] just list with str elements.\n",
    "        list_of_agg_func (list): ['mean', 'count', 'nunique', 'sum'] list with any func name.\n",
    "        targ_col (str): 'col' - any col u like.\n",
    "        threshold_number (int, optional): minimum for f_classif. if < then we dont need this column. Defaults to 4000.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: df with data about what and by what was groupped with what function for each column at the last row of each column. \n",
    "    \"\"\"\n",
    "    # Копия для основной таблицы.\n",
    "    main_df = df.fillna(0).copy()\n",
    "    # Номер последней строки, куда потом будет происходить запись что с чем группируем и по какой функции.\n",
    "    num_for_new_row = main_df.shape[0]\n",
    "    \n",
    "    # Пустой итоговый дф.\n",
    "    func_dict = {}\n",
    "    annot_dict = {}\n",
    "    \n",
    "    # Цикл по тому, по чему группируем. С добавлением прогресс бара.\n",
    "    for i in tqdm(list_of_cat_cols, total=len(list_of_cat_cols)):\n",
    "        # Цикл по тому, что группируем. \n",
    "        for j in list_of_num_cols:\n",
    "            \n",
    "            # Если столбец есть и там, и там, то пропускаем такую группировку.\n",
    "            if j in i:\n",
    "                continue\n",
    "            \n",
    "            # Если тип столбца для группировки О (например там столбец с строками)\n",
    "            if np.issubdtype(main_df[j], 'object'):\n",
    "                # То применяем только count, т.к. mean не проканает, а sum - бессмысленен.\n",
    "                func_series = main_df.groupby(i)[j].transform('count') ############ здесь слабое место, я пока не придумал, как его обыграть.\n",
    "                # Присваиваем этому срезу новое название.\n",
    "                func_series.name = 'strong_column_' + str(len(func_dict))\n",
    "                # Далее происходит запись в словари. Записывается сам сериес и данные по сериесу, каждая запись в свой словарь.\n",
    "                func_dict[func_series.name] = func_series.to_dict()\n",
    "                annot_dict[func_series.name] = {f\"'cat_cols': {i}, 'num_cols': {j}, 'func': {k}\"}\n",
    "            \n",
    "            # Если же тип столбца для группировки числовой:\n",
    "            else:            \n",
    "                # Цикл по функциям.\n",
    "                for k in list_of_agg_func:\n",
    "                    # И дальше всё то же самое. \n",
    "                    # Сгруппировали, переименовали, записали в словари.\n",
    "                    func_series = main_df.groupby(i)[j].transform(k)\n",
    "                    func_series.name = 'strong_column_' + str(len(func_dict))\n",
    "                    \n",
    "                    func_dict[func_series.name] = func_series.to_dict()\n",
    "                    annot_dict[func_series.name] = {f\"'cat_cols': {i}, 'num_cols': {j}, 'func': {k}\"}\n",
    "    \n",
    "    # Формируем датафрейм по основному словарю.\n",
    "    func_df = pd.DataFrame(func_dict)\n",
    "    # Отсеиваем лишние столбцы, согласно условию, что число теста будет больше заданного нами, что определяет силу чтолбца.\n",
    "    func_df = func_df[func_df.columns[pd.Series(f_classif(func_df, main_df[targ_col].astype(int))[0]) > threshold_number]]\n",
    "    # Добавляем описание в последнюю строку из словаря annot_dict.\n",
    "    # Сначала формируем дф по нему, решейпим и конкатим.\n",
    "    func_df = pd.concat([func_df, pd.Series(annot_dict).to_frame().T], axis=0, join='inner').reset_index(drop=True)\n",
    "        \n",
    "    return func_df\n",
    "\n",
    "# Пример работы:\n",
    "#list_of_cat_cols = [['hotel_town', 'year_of_review', 'quarter_of_review', 'hotel_name'], ['hotel_town', 'year_of_review', 'hotel_name']]\n",
    "#list_of_num_cols = ['reviewer_score']\n",
    "#list_of_agg_func = ['mean']\n",
    "#targ_col = 'reviewer_score'\n",
    "#\n",
    "#strong_cols_auto_searcher(booking_df, list_of_cat_cols=list_of_cat_cols, list_of_num_cols=list_of_num_cols, list_of_agg_func=list_of_agg_func, targ_col=targ_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to contents](#contents)\n",
    "\n",
    "<h3 id='latlongdist'>Функция, которая вычисляет расстояние между двумя точками lattitude и longitude.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lat_long_dist(matrix, rad=6.371*1e+6, azimut=False):\n",
    "    \"\"\"function to calculate distance between 2 points lat and long\n",
    "       https://gis-lab.info/qa/great-circles.html\n",
    "\n",
    "    Args:\n",
    "        matrix (list, array, or df): any data sequence u like\n",
    "        rad (float or int, optional): radius of the earth. Defaults to 6.371*1e+6 in meters!!!.\n",
    "        azimut (bool, optional): the angle in decimal degrees. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        array: the array can be shape of (m,) or (m, n) depending on what we have submitted to the input\n",
    "    \"\"\"\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    if isinstance(matrix, list):\n",
    "        matrix = np.array(matrix)\n",
    "    elif isinstance(matrix, pd.DataFrame):\n",
    "        matrix = matrix.to_numpy()\n",
    "    \n",
    "    # the order of sequence can be changed here\n",
    "    # the order of start-end will only affect the azimuth\n",
    "    # cuz for the distance there is no matter what lat or long, its the distance between 2 points\n",
    "    if len(matrix.shape) == 2:\n",
    "        lat_start, long_start, lat_end, long_end = matrix[:, 0], matrix[:, 1], matrix[:, 2], matrix[:, 3]\n",
    "    else:\n",
    "        lat_start, long_start, lat_end, long_end = matrix[0], matrix[1], matrix[2], matrix[3]\n",
    "    \n",
    "    # radians of lat\n",
    "    f1 = lat_start * (np.pi / 180)\n",
    "    f2 = lat_end * (np.pi / 180)\n",
    "    \n",
    "    # delta radians\n",
    "    delta_lat = (lat_end - lat_start) * (np.pi / 180)\n",
    "    delta_long = (long_end - long_start) * (np.pi / 180)\n",
    "    \n",
    "    # Haversine formula \n",
    "    a = np.power(np.sin(delta_lat / 2), 2) + np.cos(f1) * np.cos(f2) * np.power(np.sin(delta_long / 2), 2)\n",
    "    haversine = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    \n",
    "    dist = rad * haversine\n",
    "    \n",
    "    if not azimut:\n",
    "        return dist\n",
    "    else:\n",
    "        # teta formula for azimut\n",
    "        teta = np.arctan2(np.sin(delta_long) * np.cos(f2), np.cos(f1) * np.sin(f2) - np.sin(f1) * np.cos(f2) * np.cos(delta_long))\n",
    "        \n",
    "        return np.array([dist, (teta * (180 / np.pi) + 360) % 360]).T # % 360 == module 360"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to contents](#contents)\n",
    "\n",
    "<h3 id='combinationsgenerator'>Функция для генерации комбинаций элементов входящего списка.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_combinations(incoming_list, critical_num=1):\n",
    "    \"\"\"generate_all_combinations \n",
    "\n",
    "    Функция принимает на вход список и возвращает список скомбинированных списков.\n",
    "    Например:\n",
    "    вход ['a', 'b', 'c'] -> выход [['a'], ['b'], ['c'], ['a', 'b'], ['a', 'c'], ['b', 'c'], ['a', 'b', 'c']]\n",
    "\n",
    "    Args:\n",
    "        incoming_list (list): incoming list of something.\n",
    "        critical_num (int, optional): thats if we dont need all combinations, just first half. Defaults to 1.\n",
    "        \n",
    "    Returns:\n",
    "        list: list with all combinations of items.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Внутренняя функция рекурсивная и делает она следующее:\n",
    "    # На каждый length, например пример с ['a', 'b', 'c'] на входе:\n",
    "    # При length=1 на выходе получаем [['a'], ['b'], ['c']]\n",
    "    # При length=2 - [['a', 'b'], ['a', 'c'], ['b', 'c']]\n",
    "    # При length=3 - [['a', 'b', 'c']]\n",
    "    def generate_combinations(incoming_list, length):\n",
    "        if length == 1:\n",
    "            return [[item] for item in incoming_list]\n",
    "        else:\n",
    "            combinations = []\n",
    "            for i, item in enumerate(incoming_list):\n",
    "                for next_combination in generate_combinations(incoming_list[i+1:], length - 1):\n",
    "                    combinations.append([item] + next_combination)\n",
    "            return combinations\n",
    "    \n",
    "    # И ниже мы это всё безобразие записываем в выходящий список.\n",
    "    # А критикал нум нам нужен для того, чтобы если вдруг нам не надо все все комбинации, а, например, половину...\n",
    "    all_combinations = []\n",
    "    for length in range(1, len(incoming_list) // critical_num + 1):\n",
    "        all_combinations.extend(generate_combinations(incoming_list, length))\n",
    "    return all_combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to contents](#contents)\n",
    "\n",
    "<h3 id='highcorreraser'>Функция для удаления сильно скоррелированных столбцов. (тестовая)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def high_corr_eraser(df, upper_bound=0.9, lower_bound=-0.9, thresh=1, method='pearson'):\n",
    "    \n",
    "    func_df = df.copy()\n",
    "    \n",
    "    corr_df = func_df.corr(numeric_only=True, method=method)\n",
    "    \n",
    "    list_of_low_corr_columns_name = corr_df.corr(numeric_only=True)[(corr_df.corr(numeric_only=True) <= upper_bound) | ((corr_df.corr(numeric_only=True) >= lower_bound) & (corr_df.corr(numeric_only=True) <= 0))].dropna(thresh=thresh, axis=1).columns\n",
    "    \n",
    "    return func_df[list_of_low_corr_columns_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to contents](#contents)\n",
    "\n",
    "<h3 id='fclassifchitester'>Функция для тестирования силы столбца, относительно целевого. f_classif + chi2</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_classif_chi_tester(series, targ_col):\n",
    "    \"\"\"f_classif_chi_tester _summary_\n",
    "\n",
    "    Функция.. проводит тестирование f_classifom и хи квадратом. И показывает число. Чем больше - тем лучше.\n",
    "    Хи квадрат какой-то левый тест. Многомилионные показатели никак не влияют на модель. Как будто там что-то ломается.\n",
    "    Но не суть. f_classif более правдивый.\n",
    "\n",
    "    Args:\n",
    "        series (pd.Series/pd.DataFrame): incoming series num only. Or if its df\n",
    "        targ_col (pd.Series): looks like your_df['your_targ_col']\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from sklearn.feature_selection import chi2 # хи-квадрат\n",
    "    from sklearn.feature_selection import f_classif # anovav\n",
    "    \n",
    "    func_series_or_df = series.copy()\n",
    "    \n",
    "    # if series\n",
    "    if isinstance(func_series_or_df, pd.Series):\n",
    "        chi = pd.Series(chi2(func_series_or_df.to_frame().abs(), targ_col.astype(int))[0], index=['chi'])\n",
    "        f = pd.Series(f_classif(func_series_or_df.to_frame(), targ_col.astype(int))[0], index=['f'])\n",
    "    \n",
    "    else: # if df\n",
    "        if targ_col.name in func_series_or_df.columns: # drop targ_col if it exists.\n",
    "            func_series_or_df.drop(columns=targ_col.name, inplace=True)\n",
    "        chi = pd.Series(chi2(func_series_or_df.abs(), targ_col.astype(int))[0], index=func_series_or_df.columns, name='chi')\n",
    "        f = pd.Series(f_classif(func_series_or_df, targ_col.astype(int))[0], index=func_series_or_df.columns, name='f_classif')\n",
    "        \n",
    "        return pd.DataFrame([f, chi]).T\n",
    "    return pd.concat([f, chi])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to contents](#contents)\n",
    "\n",
    "<h3 id='specialgrouper'>Группировщик с применением ко всему датафрейму</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def special_grouper(df, bywhat, what, list_of_agg_func=['sum', 'mean', 'count', 'nunique']):\n",
    "    \"\"\"special_grouper \n",
    "\n",
    "    Согласитесь, бесит, когда надо писать несколько строчек, чтобы группировка применилась ко всей таблице.\n",
    "    А метод transform принимает только 1 аргумент и ему плевать список там или нет. Хотя в документации там может быть список.\n",
    "    Этот момент я до конца не понял.\n",
    "    \n",
    "    Эта функция - исключительно для визуальной оценки, а потом мы делаем нужный нам сериес с выбранной агрегацией через transform.\n",
    "    Т.к. трансформ крутая штука на самом деле, туда можно пихнуть лямбду.\n",
    "    \n",
    "    Хотя и отсюда можно взять столбец, но через .loc/iloc, потому что функция возвращает тип датафрейм.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): any df.\n",
    "        bywhat (str or list): name or names of columns which we want to group.\n",
    "        what (str): name of col we wanna group bywhat (my logic works only for single col cuz .iloc[bla bla] (can be expanded))\n",
    "        list_of_agg_func (list, optional): list of agg func. Defaults to ['sum', 'mean', 'count', 'nunique'].\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: a column or columns with grouping applied to the entire df as a df.\n",
    "    \"\"\"\n",
    "    return pd.merge(df[bywhat], df.groupby(bywhat)[what].agg(list_of_agg_func).reset_index(), on=bywhat, how='left').iloc[:, -len(list_of_agg_func):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to contents](#contents)\n",
    "\n",
    "<h3 id='goodbadwordssearcher'>Функция для определения положительных и отрицательных слов в предложении</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Функция для определения тональности слова\n",
    "def extract_sentiment_words(sentence, positive_words, negative_words):\n",
    "    \n",
    "    def get_word_sentiment(word, positive_words, negative_words):\n",
    "        # Создание синтаксических представлений для положительных и отрицательных слов    \n",
    "        positive_synsets = [j for i in positive_words for j in wordnet.synsets(i, pos=wordnet.ADJ)]\n",
    "        negative_synsets = [j for i in negative_words for j in wordnet.synsets(i, pos=wordnet.ADJ)]\n",
    "        \n",
    "        # Определение синонимов слова в WordNet\n",
    "        synsets = wordnet.synsets(word, pos=wordnet.ADJ)\n",
    "        if synsets:\n",
    "            # Проверка синонимов на положительность или отрицательность\n",
    "            for synset in synsets:\n",
    "                if synset in positive_synsets:\n",
    "                    return 'positive'\n",
    "                elif synset in negative_synsets:\n",
    "                    return 'negative'\n",
    "        # Если не удалось определить тональность\n",
    "        return 'neutral'\n",
    "    \n",
    "    positive_list = []\n",
    "    negative_list = []\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    for word in words:\n",
    "        sentiment = get_word_sentiment(word, positive_words, negative_words)\n",
    "        if sentiment == 'positive':\n",
    "            positive_list.append(word)\n",
    "        elif sentiment == 'negative':\n",
    "            negative_list.append(word)\n",
    "    return positive_list, negative_list\n",
    "\n",
    "sentense = 'i ok feel very conflicted about this hotel unfortunately ultimately i wouldn t recommend it because with our experience the bad outweighed the good the fire alarm went off late on sunday night 11th july on the first night we stayed so naturally everyone evacuated and stood outside it was absolutely pouring so not the best start eventually a member of hotel staff came to the front desk and the alarm was silenced i would say the alarm had been going for about 10 minutes at this point though and there wasn t a staff member to tell us if it was a false alarm or real or anything she explained in a really aggressive manner that there was a fault with the alarm and heavy rain sometimes set it off she seemed irritated we had bothered to evacuate and said that it was not the first time it had happened no apology or further explanation of a fix or anything fair enough though it was close to 1am and we were all a bit moody we arrived back at the hotel on our first day and the girl at the front desk informed us the lift was broken we were on the top floor which was a pain but being able bodied it was hardly a big deal however it was not fixed for the full 4 days of our stay again no apology offered at any point they did however offer to help getting luggage downstairs which was a godsend no idea what they would have done if they had disabled guests all in all the staff were unpleasant to deal with when we returned to the hotel in the evening we arrived home late from disneyland on tuesday night and asked for some milk sachets for the room and we were told no milk is available by an exasperated girl on the front desk who was busy making a personal call we walked up the four flights of stairs and found some milk sachets on a cleaning trolley on the landing so i think she just had no interest in being helpful also just a word of advice our room 405 had the wc seperate then a shower in the middle of the bedroom'\n",
    "\n",
    "# Определение положительных и отрицательных слов\n",
    "positive_words = ['good', 'great', 'excellent', 'ok', 'awesome']  # Положительные слова\n",
    "negative_words = ['bad', 'poor', 'awful', 'conflict']  # Отрицательные слова\n",
    "\n",
    "print(extract_sentiment_words(sentence=sentense, positive_words=positive_words, negative_words=negative_words))\n",
    "print(wordnet.synsets('ok', pos=wordnet.ADJ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to contents](#contents)\n",
    "\n",
    "<h3 id='printmetrics'>Функция для подсчёта любых метрик на выбор, как для и просто тест выборки, так и трейн + тест выборок</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(y_test, y_test_pred, list_of_metrics, y_train=None, y_train_pred=None, names=False, metric_params_dict=False):\n",
    "    \"\"\"print_metrics \n",
    "\n",
    "    function for print any metric u like.\n",
    "    P.S. some metrics have hard names, so need to adjust names in 'block to adjust names' (see code)\n",
    "\n",
    "    Args:\n",
    "        y_test (pd.Series or numpy.ndarray): y_test\n",
    "        y_test_pred (list): y_test_pred (lgr.predict(X_test))\n",
    "        list_of_metrics (list): list of any metrics !!CAUTION!! Metrics should be imported like: from sklearn.metrics import accuracy_score\n",
    "        y_train (pd.Series or numpy.ndarray, optional): y_train. Defaults to None.\n",
    "        y_train_pred (list, optional): y_train_pred. Defaults to None.\n",
    "        names (list, optional): list of algo names (for ex: ['lgr_1', 'lgr_2']). Defaults to False.\n",
    "        metric_params_dict (dict, optional): dict look like {'metric_name': {'parameter': 'value'}}. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: with our metrics\n",
    "        \n",
    "    ex of using:\n",
    "    \n",
    "    list_of_metrics = ['accuracy_score', 'f1_score', 'precision_score', 'recall_score']\n",
    "    metric_params_dict = {'accuracy_score': {'normalize': False}}\n",
    "    \n",
    "    print_metrics(y_test=y_test, \n",
    "                  y_test_pred=[y_test_pred], \n",
    "                  list_of_metrics=list_of_metrics,\n",
    "                  names=['lgr_1'], metric_params_dict=metric_params_dict)\n",
    "    \"\"\"\n",
    "    \n",
    "    import pandas as pd\n",
    "    \n",
    "    # if we didnt set names it will be 0, 1 etc.\n",
    "    if not names:\n",
    "        names = [f'{i}' for i in range(len(y_test_pred))]\n",
    "    \n",
    "    ################### <----- block to adjust metrics params\n",
    "    # if we dont want to specify additional metric params\n",
    "    if not metric_params_dict:\n",
    "        metric_params_dict_func = {i:{} for i in list_of_metrics}\n",
    "    # else we make empty dict with each metric and update it with needed external dict\n",
    "    else:\n",
    "        metric_params_dict_func = {i:{} for i in list_of_metrics}\n",
    "        metric_params_dict_func.update(metric_params_dict)\n",
    "    \n",
    "    ################### <----- block to adjust names \n",
    "    # This for correct metrics names.\n",
    "    func_list_of_metrics = []\n",
    "    \n",
    "    for i in list_of_metrics:\n",
    "        # For ex: mean_absolute_error = MAE (if len(['mean', 'absolute', 'error']) > 2)\n",
    "        if len(i.split('_')) > 2:\n",
    "            func_list_of_metrics.append(''.join([j[0].capitalize() for j in i.split('_')]))\n",
    "        # else r2_score = r2\n",
    "        else:\n",
    "            func_list_of_metrics.append(i.split('_')[0])\n",
    "    ###################\n",
    "    \n",
    "    train_dict = {}\n",
    "    test_dict = {}\n",
    "    train_list = [[] for i in range(len(list_of_metrics))]\n",
    "    test_list = [[] for i in range(len(list_of_metrics))]\n",
    "    \n",
    "    ###################\n",
    "    # if we wanna see just test metrics:\n",
    "    if y_train is None:\n",
    "        for y_test_p in y_test_pred:\n",
    "            for i, j in enumerate(list_of_metrics):\n",
    "                # values of metrics\n",
    "                test_value = globals()[j](y_test, y_test_p, **metric_params_dict_func[j])\n",
    "                # adding to list\n",
    "                test_list[i].append(test_value)\n",
    "                \n",
    "                # refresh dict\n",
    "                test_dict[f\"{func_list_of_metrics[i]}\"] = test_list[i]\n",
    "        \n",
    "        func_df = pd.DataFrame(test_dict, index=names).T\n",
    "\n",
    "    \n",
    "    # if we wanna see train and test scores:    \n",
    "    else:\n",
    "        # for train_pred + test_pred\n",
    "        for y_train_p, y_test_p in zip(y_train_pred, y_test_pred):\n",
    "            for i, j in enumerate(list_of_metrics):\n",
    "                # values of metrics\n",
    "                train_value = globals()[j](y_train, y_train_p, **metric_params_dict_func[j])\n",
    "                test_value = globals()[j](y_test, y_test_p, **metric_params_dict_func[j])\n",
    "                # adding to lists\n",
    "                train_list[i].append(train_value)\n",
    "                test_list[i].append(test_value)\n",
    "                \n",
    "                # refresh dicts\n",
    "                train_dict[f\"{func_list_of_metrics[i]}\"] = train_list[i]\n",
    "                test_dict[f\"{func_list_of_metrics[i]}\"] = test_list[i]\n",
    "        \n",
    "        # making MultiIndex for columns\n",
    "        columns = pd.MultiIndex.from_product([['train', 'test'], names])\n",
    "        \n",
    "        # Combining the data into one array for further creation of a df\n",
    "        data = [train_list[i] + test_list[i] for i, metric in enumerate(func_list_of_metrics)]\n",
    "        \n",
    "        # Creating a df with a multi-index\n",
    "        func_df = pd.DataFrame(data, index=func_list_of_metrics, columns=columns)\n",
    "    \n",
    "    return func_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JIeMyp4uK",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
