{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Полезная штука, если данных много и в аутпуте всё не помещается.\n",
    "## В текущей директории создаётся тхт файл с полным содержимым.\n",
    "#\n",
    "#\n",
    "#series_for_output = pd.Series(booking_df['negative_review'].value_counts().reset_index()['negative_review'].sort_values().str.strip().str.lower().unique())\n",
    "#\n",
    "## Сюда нужно поместить то, что хотим полностью увидеть.\n",
    "#output_data = series_for_output[series_for_output.apply(lambda x: len(x)) < 14]\n",
    "\n",
    "## Если аутпут - сериес:\n",
    "#with open('output_neg_rev.txt', 'w') as f:\n",
    "#    output_data.to_csv('output_neg_rev.txt', sep='\\t', index=False)\n",
    "\n",
    "## Если аутпут - списко:\n",
    "#with open('output.txt', 'w') as f:\n",
    "#    for item in output_data:\n",
    "#        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id='contents'>Оглавляшка:</h2>\n",
    "\n",
    "1. [Функция-аналог .info() - my_info()](#myinfo)\n",
    "2. Графические функции:\n",
    "    * [Функция рисования 3-х графиков: гистограммы, коробки и Q-Q в одном флаконе.](#histboxqq)(для массива чисел)\n",
    "    * [Функция рисования графика корреляции.](#mycorr)\n",
    "    * [Функция декорирования оси Х столбчатой диаграммы.](#decoratexlabels)(крутая штука)\n",
    "3. [Функция, создающая словарь для .fillna() заполнения NaN.](#fillnadict)\n",
    "4. [Функция отлова выбросов по методу Тьюки.](#outlierstukie)\n",
    "5. [Функция проверки на нормальность Шапиро-Уилка.](#shapirowilkfunc)\n",
    "6. [Функция проверки равенства дисперсий от Левена.](#levenefunc)\n",
    "7. Функция для работы с [\"строками\", \"внутри\", \"списка тэгов\", \"1 Adult\", \"2 Adults\"]\n",
    "    * [ОСНОВНАЯ](#maintagsworkfunc)\n",
    "    * [Второстепенная](#retagsworkfunc)\n",
    "8. Функция для работы с текстом. Когда столбец состоит только из элементов str\n",
    "    * [Основная функция с алгоритмом Вейдера](#vadersentanalmain)\n",
    "    * [Второстепенная функция с алгоритмом Вейдера](#vadersentanalsec)\n",
    "    * [Функция с алгоритмом Роберты на основе отзывов в твиттере](#robertapolarity)\n",
    "9. [Функция для тренировки нескольких моделей, используя несколько алгоритмов ML за 1 заход](#multiplemodelstraining)\n",
    "10. [Функция для генерации комбинаций элементов входящего списка](#combinationsgenerator) (классный комбинатор элементов списка)\n",
    "11. [Функция для тестирования силы столбца, относительно целевого. f_classif + chi2](#fclassifchitester)\n",
    "12. [Группировщик с применением ко всему датафрейму](#specialgrouper)\n",
    "13. Тестовые функции:\n",
    "    * [Функция для удаления сильно скоррелированных столбцов. (тестовая)](#highcorreraser)\n",
    "    * [Функция для автоматического поиска сильных столбцов относительно целевого столбца (тестовая)](#strongcolssearcher)\n",
    "    * ТЕСТОВАЯ [Функция для определения положительных и отрицательных слов в предложении](#goodbadwordssearcher) (супер сырая)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример работы tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48e11616365f4927b2f9e0e47b2a5e2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cell progress: : 0%|          | 0/100 [Elapsed: 00:00 m/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "list_for_tqdm = [i for i in range(100)]\n",
    "\n",
    "total_progress = 100\n",
    "with tqdm(total=total_progress, desc=f\"Cell progress: \", bar_format='{desc}: {percentage:.0f}%|{bar}| {n:.0f}/{total_fmt} [Elapsed: {elapsed} m/s]') as pbar:\n",
    "    cumulative_progress = 0 # Чтобы прогресс бар работал, как надо и завершался всегда на 100.\n",
    "##########^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^##############\n",
    "# сюда нужно вставить свой цикл.\n",
    "    for i, word in enumerate(list_for_tqdm):\n",
    "        time.sleep(0.1)\n",
    "        \n",
    "##########vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv##############        \n",
    "        # Рассчитываем размер шага на основе оставшегося прогресса и оставшихся итераций.\n",
    "        remaining_iterations = len(list_for_tqdm) - i # вот тут пригодился индекс i\n",
    "        remaining_progress = total_progress - cumulative_progress\n",
    "        step_size = remaining_progress / remaining_iterations\n",
    "        \n",
    "        # Обновляем общий прогресс.\n",
    "        cumulative_progress += step_size\n",
    "        pbar.update(step_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id='myinfo'>Функция печатающая аналог <span style='color:orange'>.info()</span> в расширенном варианте.</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# My info.\n",
    "def my_info(df, catmaxnum=None):\n",
    "    \"\"\"Info function to show basic df indicators\n",
    "\n",
    "    Args:\n",
    "        df (pd.Dataframe): any df.\n",
    "        catmaxnum (int): the minimum threshold(rus: порог) after which a column will be considered a category. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        _type_: pd.Dataframe\n",
    "    \"\"\"\n",
    "    df_copy = df.copy() # it needs here.\n",
    "    \n",
    "    # Loop and condition for moment, when there are unhashable types in our column. (list, dict, set)\n",
    "    for i in df_copy.columns:\n",
    "        if any(isinstance(cell, (list, set, dict)) for cell in df_copy[i]):\n",
    "            df_copy[i] = df_copy[i].astype(str)\n",
    "                \n",
    "    func_df = pd.DataFrame({\n",
    "        \n",
    "        'column': df_copy.columns,\n",
    "        'num of unique vals': df_copy.nunique(),\n",
    "        'type': [str(df_copy.dtypes[i]) for i in df_copy.columns],\n",
    "        'mode': [df_copy[i].mode()[0] for i in df_copy.columns],\n",
    "        'number of entries': len(df_copy),\n",
    "        'NaN vals': df_copy.isna().sum(),\n",
    "        'number of dublics': df_copy.duplicated().sum(),\n",
    "        'describe': [{'min': df_copy[i].min(), 'max': df_copy[i].max(), 'mean': round(df_copy[i].mean()), 'std': round(df_copy[i].std())} if np.issubdtype(df_copy[i].dtype, np.number) else 'type obj' for i in df_copy.columns]\n",
    "        \n",
    "    }).sort_values(by='num of unique vals').reset_index(drop=True)\n",
    "    \n",
    "    if catmaxnum:\n",
    "        func_df.insert(3, 'classification', ['category' if i < catmaxnum else 'numeric' for i in func_df['num of unique vals']])\n",
    "        \n",
    "    return func_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to contents](#contents)\n",
    "\n",
    "<h2 id='outlierstukie'>Функция отлова выбросов по методу Тьюки</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Tukie outliers finder function.\n",
    "def outliers_iqr_mode_log(data, feature=None, log_scale=False, left=1.5, right=1.5):\n",
    "    \n",
    "    func_data = data.copy()\n",
    "    \n",
    "    if isinstance(func_data, pd.DataFrame):    \n",
    "        if log_scale: # if we want to logarithm a feature.\n",
    "            if any(func_data[feature] == 0):\n",
    "                x = np.log(func_data[feature] + 1)\n",
    "            else:\n",
    "                x = np.log(func_data[feature])\n",
    "        else:\n",
    "            x = func_data[feature]\n",
    "    else:\n",
    "        x = func_data # if data == array or series\n",
    "        \n",
    "    quantile_25, quantile_75 = x.quantile(0.25), x.quantile(0.75)\n",
    "    iqr = quantile_75 - quantile_25\n",
    "    \n",
    "    lower_bound = quantile_25 - iqr * left\n",
    "    upper_bound = quantile_75 + iqr * right\n",
    "    \n",
    "    outliers = func_data[(x < lower_bound) | (x > upper_bound)]\n",
    "    cleaned = func_data[(x >= lower_bound) & (x <= upper_bound)]\n",
    "    \n",
    "    return outliers, cleaned.reset_index(drop=True) # .copy() is needed here if we wanna use cleaned daataframe. Or system will swear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to contents](#contents)\n",
    "\n",
    "<h3 id='histboxqq'>Функция рисования 3-х графиков: гистограмма, коробка и Q-Q.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm # рисовалка qq-plot.\n",
    "import seaborn as sns\n",
    "\n",
    "def hist_box_qq(arg, full_iqr=False):\n",
    "    \"\"\"Function of drawing histogram, box and cuckoo in one bottle.\n",
    "\n",
    "    full_iqr... It's shorter.. I wrote a function for the lognormal distribution, i.e. there are no outliers on the left.\n",
    "    But then I thought about it and decided to add it, because then I still log the feature.\n",
    "    And in general, if the array is usually distributed.\n",
    "    In a word: full_iqr is Boolean. By default, False. I.e., only the right borders are drawn.\n",
    "    \n",
    "    The input is a Series or array.\n",
    "\n",
    "    Args:\n",
    "        arg (Series/np.array): any numeric series or array.\n",
    "        full_iqr (bool, optional): choose, u wanna show only right outliers borders or both. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        plt\n",
    "    \"\"\"\n",
    "    func_arg = arg.copy()\n",
    "    # replace inf.\n",
    "    func_arg = func_arg.replace([np.inf, -np.inf], np.nan)\n",
    "    # If there is a nan, I drop it. Because it does not draw if there is a nan.\n",
    "    func_arg = pd.Series(func_arg).dropna()\n",
    "    \n",
    "    fig, ax = plt.subplots(ncols=3, nrows=1, figsize=[18, 5])\n",
    "    \n",
    "    # The histogram. And the settings for the histogram lines. Legend = label in each line.\n",
    "    # Red is the median, green is the tukey, and blue is 3 sigma.\n",
    "    sns.histplot(func_arg, kde=True, bins=50, ax=ax[0])\n",
    "    \n",
    "    ax[0].get_lines()[0].set_color('black')\n",
    "    ax[0].axvline(func_arg.median(), color='red', linestyle='--', linewidth='1.8', label='median')\n",
    "    ax[0].axvline(func_arg.quantile(0.75) + ((func_arg.quantile(0.75) - func_arg.quantile(0.25)) * 1.5), color='g', ls='--', lw=2, label='1.5 IQR Tukie')\n",
    "    ax[0].axvline(func_arg.mean() + 3 * func_arg.std(), color='b', ls='--', lw=2, label='3 IQR z-score')\n",
    "    ax[0].legend()\n",
    "    ax[0].set_xlabel(f'Feauture {func_arg.name}')\n",
    "    \n",
    "    # Box. And the settings for it.\n",
    "    sns.boxplot(func_arg, ax=ax[1], orient='h', medianprops={'color': 'red', 'linestyle': '--'})\n",
    "\n",
    "    ax[1].axvline(func_arg.quantile(0.75) + ((func_arg.quantile(0.75) - func_arg.quantile(0.25)) * 1.5), color='g', ls='--', lw=2, label='1.5 IQR Tukie')\n",
    "    ax[1].axvline(func_arg.mean() + 3 * func_arg.std(), color='b', ls='--', lw=2, label='3 IQR z-score')\n",
    "    ax[1].legend()    \n",
    "    ax[1].set_xlabel(f'Feauture {func_arg.name}')\n",
    "    \n",
    "    if full_iqr: # Left emission catch lines are added.\n",
    "        ax[0].axvline(func_arg.quantile(0.25) - ((func_arg.quantile(0.75) - func_arg.quantile(0.25)) * 1.5), color='g', ls='--', lw=2)\n",
    "        ax[0].axvline(func_arg.mean() - 3 * func_arg.std(), color='b', ls='--', lw=2)\n",
    "        ax[1].axvline(func_arg.quantile(0.25) - ((func_arg.quantile(0.75) - func_arg.quantile(0.25)) * 1.5), color='g', ls='--', lw=2)\n",
    "        ax[1].axvline(func_arg.mean() - 3 * func_arg.std(), color='b', ls='--', lw=2)\n",
    "    \n",
    "    # This line builds a Q-Q graph.\n",
    "    qq = sm.ProbPlot(func_arg, fit=True).qqplot(marker='*', markerfacecolor='b', markeredgecolor='b', alpha=0.3, ax=ax[2])\n",
    "    # Line, also with the ability to influence color. fmt parameter.\n",
    "    sm.qqline(qq.axes[2], line='45', fmt='r', linestyle='--')\n",
    "    \n",
    "    # The general title.\n",
    "    plt.suptitle(f'Distribution of the feature: {func_arg.name}').set_fontsize(20)\n",
    "    \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to contents](#contents)\n",
    "\n",
    "<h3 id='mycorr'>Функция рисования тепловой карты корреляции.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def my_corr(df, method='spearman', corrneg=0, corrpos=0, figsize=(10, 5), xtickrot=90, show_high_corr_cols_names=False):\n",
    "    \"\"\"\n",
    "    The function of drawing a correlation graph, but not an ordinary one, but one in which the lower part is the entire one,\n",
    "    and the upper part contains only those correlation values that are greater and less than some number we have specified.\n",
    "\n",
    "    Args:\n",
    "        df (pd.Dataframe): any df.\n",
    "        method (str): method u like. default spearman.\n",
    "        corrneg (float): negative threshold for display. (for negative correlation value)\n",
    "        corrpos (float): positive threshold. (for a positive value) both default to 0. At 0, the full standard correlation matrix will be displayed.\n",
    "        figsize (tuple): u know what it is.)\n",
    "        xtickrot(int): rotate x axis labels. default 90\n",
    "        show_high_corr_cols_names(bool): if we wanna watch names of high corr cols. default False\n",
    "\n",
    "    Returns:\n",
    "        _type_: plt or df\n",
    "    \"\"\"\n",
    "    corr_df = df.corr(numeric_only=True, method=method).copy()\n",
    "    \n",
    "    # We find indices where the correlation value is greater and less than a certain number.\n",
    "    high_corr_pairs = ((corr_df >= corrpos) & (corr_df <= 1)) | (corr_df <= corrneg)\n",
    "    \n",
    "    # We receive only those pairs of features where the condition is met. The rest are NaN.\n",
    "    high_corr_features = corr_df[high_corr_pairs]\n",
    "    \n",
    "    # if we wanna watch high corr pairs in the form of a df. Without picture. I decided 2 split it up.\n",
    "    if show_high_corr_cols_names:\n",
    "        # stack hight corr feature.\n",
    "        shccn = high_corr_features.stack().to_frame().reset_index().rename(columns={0: 'corr'})\n",
    "        # sort'em.\n",
    "        shccn[['level_0', 'level_1']] = np.sort(shccn[['level_0', 'level_1']], axis=1)\n",
    "        # keep only 1 pair of high corr cols names.\n",
    "        shccn = shccn.groupby(['level_0', 'level_1']).first().reset_index()\n",
    "        # removing the diagonal.\n",
    "        shccn = shccn.query('level_0 != level_1')\n",
    "        # return with resetting index.\n",
    "        return shccn.sort_values(by='corr', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # if we wanna see the pic.\n",
    "    else:\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "        plt.suptitle(f'Correlation of incoming dataframe features.')\n",
    "        \n",
    "        # 2 masks, one triu (tri upper), the second tril (tri lower).\n",
    "        mask_for_logging_df = np.triu(np.ones_like(corr_df, dtype=bool))\n",
    "        mask_for_logging_df_2 = np.tril(np.ones_like(high_corr_features, dtype=bool))\n",
    "        \n",
    "        # 2 graphics. I didn’t figure out how to make such a picture using 1 chart. That's why cbar=False helps.\n",
    "        sns.heatmap(corr_df, annot=True, cmap='coolwarm', mask=mask_for_logging_df, vmax=1, vmin=-1, linewidths=0.1, fmt='.2f') # <--- lower part\n",
    "        \n",
    "        # if condition == 1 high_corr_features contains only nan, so we dont need upper part. \n",
    "        # or.. high_corr_features has no nan, i.e. corrpos and corrneg == 0, so we draw all corr matrix.\n",
    "        if (len(high_corr_features.isna().sum().value_counts()) != 1) | ((corrpos == 0) & (corrneg == 0)):\n",
    "            sns.heatmap(high_corr_features, annot=True, cmap='coolwarm', mask=mask_for_logging_df_2, cbar=False, vmax=1, vmin=-1, linewidths=0.1, fmt='.2f') # <--- upper part\n",
    "        \n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=xtickrot)\n",
    "    \n",
    "        plt.plot([0, len(df.columns)], [0, len(df.columns)], color='black', linewidth=2); # It's just a line. I don't know why. But i like it.\n",
    "        \n",
    "        return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to contents](#contents)\n",
    "\n",
    "<h3 id='fillnadict'>Функция, создающая словарь для заполнения метода .fillna</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillna_dict_for_custom_df(df, func):\n",
    "    \"\"\"Function for generating a dictionary for the fillna function argument.\n",
    "    \n",
    "    The function takes as input a dataframe with np.nan and the function that we want to aggregate.\n",
    "    That is, we have a feature in which there is a nan. We want to fill the nan, for example, with the median.\n",
    "    We run through the names of the columns that have nan and assign each nan a median value for the column.\n",
    "    Why the condition? When we ask for mode, we get a Series object that needs to be accessed by index to get the value.\n",
    "    And when we ask for the average or median, it immediately returns the value.\n",
    "\n",
    "    Args:\n",
    "        df (pd.Dataframe): any df\n",
    "        func (str): any func in str format. \n",
    "        \n",
    "        Ex: df.fillna(value=fillna_dict_for_custom_df(df, 'mean'))\n",
    "\n",
    "    Returns:\n",
    "        _type_: dict\n",
    "    \"\"\"\n",
    "    return {i: df[i].agg(func)[0] if func == 'mode' else df[i].agg(func) for i in df.loc[:, df.isna().mean() > 0].columns}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to contents](#contents)\n",
    "\n",
    "<h3 id='decoratexlabels'>Функция декорирования оси Х столбчатой диаграммы.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ОРИГИНАЛ!\n",
    "#from itertools import groupby # для рисования мультииндекс меток по оси х (и не только, но здесь - только).\n",
    "#\n",
    "## Чумовейшие три функции, которые рисуют мультииндекс разметку по оси Х!!!\n",
    "## Работа происходит по третьей функции. \n",
    "## На вход подаётся ax и датафрейм вида: \n",
    "## 1. главный индекс; \n",
    "## 2. второстепенный индекс или группа индексов, по иерархии от старшего к младшему; \n",
    "## 3. последним идёт индекс, который является оттенком (hue).\n",
    "## Вид группировки: .groupby(['1_st_order_idx', '2_nd_order_idx', ..., 'hue_idx'])['nums_column'].mean().unstack()\n",
    "## P.S. для работы нужен groupby из itertools\n",
    "#\n",
    "#def add_line(ax, xpos, ypos):\n",
    "#    line = plt.Line2D([xpos, xpos], [ypos + .1, ypos],\n",
    "#                      transform=ax.transAxes, color='gray')\n",
    "#    line.set_clip_on(False)\n",
    "#    ax.add_line(line)\n",
    "#\n",
    "#def label_len(my_index,level):\n",
    "#    labels = my_index.get_level_values(level)\n",
    "#    return [(k, sum(np.fromiter((1 for i in g), dtype=int))) for k, g in groupby(labels)]\n",
    "#\n",
    "#def label_group_bar_table(ax, df):\n",
    "#    ypos = -.1\n",
    "#    scale = 1./df.index.size\n",
    "#    for level in range(df.index.nlevels)[::-1]:\n",
    "#        pos = 0\n",
    "#        for label, rpos in label_len(df.index, level):\n",
    "#            lxpos = (pos + .5 * rpos)*scale\n",
    "#            ax.text(lxpos, ypos, label, ha='center', transform=ax.transAxes)\n",
    "#            add_line(ax, pos*scale, ypos)\n",
    "#            pos += rpos\n",
    "#        add_line(ax, pos*scale ,ypos)\n",
    "#        ypos -= .1\n",
    "        \n",
    "def label_group_bar_table(ax, df):\n",
    "    \"\"\"label_group_bar_table \n",
    "\n",
    "    Чумовейшие три функции, которые рисуют мультииндекс разметку по оси Х!!!\n",
    "    На вход подаётся ax и датафрейм вида: \n",
    "     1. главный индекс; \n",
    "     2. второстепенный индекс или группа индексов, по иерархии от старшего к младшему; \n",
    "     3. последним идёт индекс, который является оттенком (hue).\n",
    "     Вид группировки: .groupby(['1_st_order_idx', '2_nd_order_idx', ..., 'hue_idx'])['nums_column'].mean().unstack()\n",
    "     P.S. для работы нужен groupby из itertools\n",
    "     P.S.2 Сие ваяние не моё, потому хз, что там происходит и я не распишу каждую строчку, но результат меня устраивает.)\n",
    "\n",
    "    Args:\n",
    "        ax (_type_): ax with ur plot.\n",
    "        df (_type_): ur df which u wanna plot.\n",
    "    \"\"\"\n",
    "    from itertools import groupby # для рисования мультииндекс меток по оси х (и не только, но здесь - только).\n",
    "    \n",
    "    def add_line(ax, xpos, ypos):\n",
    "        line = plt.Line2D([xpos, xpos], [ypos + .1, ypos],\n",
    "                          transform=ax.transAxes, color='gray')\n",
    "        line.set_clip_on(False)\n",
    "        ax.add_line(line)\n",
    "    \n",
    "    def label_len(my_index,level):\n",
    "        labels = my_index.get_level_values(level)\n",
    "        return [(k, sum(np.fromiter((1 for i in g), dtype=int))) for k, g in groupby(labels)]\n",
    "    \n",
    "    \n",
    "    ypos = -.1\n",
    "    scale = 1./df.index.size\n",
    "    for level in range(df.index.nlevels)[::-1]:\n",
    "        pos = 0\n",
    "        for label, rpos in label_len(df.index, level):\n",
    "            lxpos = (pos + .5 * rpos)*scale\n",
    "            ax.text(lxpos, ypos, label, ha='center', transform=ax.transAxes)\n",
    "            add_line(ax, pos*scale, ypos)\n",
    "            pos += rpos\n",
    "        add_line(ax, pos*scale ,ypos)\n",
    "        ypos -= .1\n",
    "\n",
    "####################################################################################################################\n",
    "# Пример использования:\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(30, 8))\n",
    "\n",
    "plt.suptitle('Динамика соотношения среднего годового дохода в зависимости от формата занятости, года и опыта').set_fontsize(25)\n",
    "\n",
    "# Подготовка датафрейма.\n",
    "df_for_this_cell = df_for_this_cell.groupby([choose_job, 'remote_ratio', 'work_year', 'experience_level'])['salary_in_usd'].mean().unstack()\n",
    "\n",
    "# Сама строка.\n",
    "df_for_this_cell.plot(kind='bar', ax=ax, stacked=True, legend=True)\n",
    "\n",
    "ax.set_xticklabels('') # обязательно удаление названий по оси х\n",
    "ax.set_xlabel('')\n",
    "\n",
    "# И оборачивание подготовленного, сгруппированного датафрейма в функцию\n",
    "# На самом деле можно и без этого. Функция чисто декоративная.\n",
    "label_group_bar_table(ax, df_for_this_cell)\n",
    "####################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to contents](#contents)\n",
    "\n",
    "<h3 id='shapirowilkfunc'>Функция проверки на нормальность Шапиро-Уилка.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "alpha = 0.5\n",
    "\n",
    "# Функция проверки на нормальность Шапиро-Уилка.\n",
    "def shapirowilk(df_or_arr, alpha=0.05):\n",
    "    \"\"\"Продвинутая функция проверки нормальности Шапиро-Уилка.\n",
    "    \n",
    "    Тут всё просто. Если датафрейм, то формируем через цикл списки названий нормальных и не нормальных столбцов.\n",
    "    Иначе сразу печатаем реультат теста. Т.к. у нас Series или массив.\n",
    "    Далее смотрим на длины списков и формируем список выводов функции. И выводим его.\n",
    "    Почему продвинутая? Потому что я раньше считал количество норм и не норм столбцов через: dict(Counter([shapirowilk(df_for_this_cell[i]) for i in df_for_this_cell.columns]))\n",
    "    а сама функция выглядела, как функция Левена ниже.))\n",
    "\n",
    "    Args:\n",
    "        df_or_arr (pd.DataFrame): Dataframe or Series or array\n",
    "        alpha (float): alpha significance level\n",
    "\n",
    "    Returns:\n",
    "        df or str\n",
    "    \"\"\"\n",
    "    func_df = pd.DataFrame()\n",
    "\n",
    "    if isinstance(df_or_arr, pd.DataFrame):\n",
    "        for i, j in enumerate(df_or_arr.columns):\n",
    "            if stats.shapiro(df_or_arr[j])[1] <= alpha:\n",
    "                func_df.loc[i, 'Distribution'] = df_or_arr[j].name\n",
    "                func_df.loc[i, 'Status'] = 'не нормальное'\n",
    "            else:\n",
    "                func_df.loc[i, 'Distribution'] = df_or_arr[j].name\n",
    "                func_df.loc[i, 'Status'] = 'нормальное'\n",
    "    else:\n",
    "        return 'Распределение не нормальное. При alpha = 0.05' if stats.shapiro(df_or_arr)[1] <= alpha else 'Распределение нормальное. При alpha = 0.05'\n",
    "    \n",
    "    return func_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to contents](#contents)\n",
    "\n",
    "<h3 id='levenefunc'>Функция проверки равнества дисперсий от Левена.</h3>\n",
    "(недоработанная)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция проверки на равенство дисперсий от Левена.\n",
    "def levene(*args):\n",
    "    return 'Дисперсии не равны. Нужно использовать непараметрический тест. При alpha = 0.05' if stats.levene(*args)[1] <= alpha else 'Дисперсии равны. При alpha = 0.05'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to contents](#contents)\n",
    "\n",
    "<h3 id='maintagsworkfunc'>Функция для работы с [\"строками\", \"внутри\", \"списка тэгов\", \"1 Adult\", \"2 Adults\"]</h3>\n",
    "ОСНОВНАЯ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_specific_string(tags_column, words_to_transform, list_of_exceptions=[], hard_split_list=[], replacement_dict=False):\n",
    "    from tqdm.notebook import tqdm_notebook\n",
    "    \"\"\"\n",
    "    Функция преобразования str элементов списка.\n",
    "    Типа когда у нас есть список, в котором есть строки, которые нужно побить по какому-то слову или словосочетанию.\n",
    "    И вернуть обратно такой же список, но с побитыми строками. \n",
    "    P.S. Почему не сделать изначально весь столбец .lower() и потом использовать функцию?\n",
    "    Приятней же искать в выводе файла 'output_transformed.txt' следующие слова на обрезку, которые начинаются с заглавной буквы.\n",
    "    Ну, лично мне. Можно изначально сделать всё lower и оно всё равно будет работать.\n",
    "\n",
    "    Args:\n",
    "        tags_column (series): столбец со списками, которые с элементами str.\n",
    "        split_word (str): ключевое слово или словосочетание.\n",
    "        list_of_exceptions (list): список исключений, которые мы не хотим видеть в итоговом списке.\n",
    "        hard_split_list (list): список слов, по которым строка будет делиться, не склеивая остатки (with, and, etc).\n",
    "        replacement_dict (dict): словарь замены одинакового, написанного по-разному.\n",
    "\n",
    "    Returns:\n",
    "        list: возвращаем побитый список.))\n",
    "        \n",
    "    Пример использования: \n",
    "    нужно вставить код ниже в ячейку после функции и поменять your_df[tags_col] на свой дф с столбцом тэгов и везде заменить your_df (выделить your_df и на клаве: ctrl+D или cmd+D если мак). \n",
    "    ----------------------------------------------------------------------------------------------------------\n",
    "        words_to_transform = [# любые слова или словосочетания, по которым хотим побить каждую строку, порядок важен] # типа сначала сложносоставные слова \"1 2 Adult\", а потом \"2 Adult\". У себя я разбил на 2 тего: \"1 2 Adult\" - 1 адулт и 2 адулт. Чувак же указал.\n",
    "        list_of_exceptions = [# слова исключения, которым в списках не место (with, and)] # Тут порядок не важен.\n",
    "        hard_split_list = [# слова, по которым строка будет делиться, без склеивания остатков (with, and, etc)]\n",
    "        replacement_dict = {# словарь, если хотим заменить какие-то слова или словосочетания} # И тут тоже.\n",
    "        \n",
    "        # Помещаем преобразование в отдельный столбец.\n",
    "        your_df['transformed'] = transform_specific_string(your_df[tags_col], words_to_transform=words_to_transform, list_of_exceptions=list_of_exceptions, replacement_dict=replacement_dict)\n",
    "        \n",
    "        # vvvvvvvvvvvvvvvvvvv  Ниже работа с выводом  vvvvvvvvvvvvvvvvvvv\n",
    "        # Если хотим посмотреть, что у нас получилось используем следующее:\n",
    "        # Полезная штука, если данных много и в аутпуте всё не помещается.\n",
    "        # В текущей директории создаётся файл в котором находится список уникальных элементов столбца. Ну или не совсем уникальных, если не делали .lower() Нужно вставить после .explode()\n",
    "        output_data = list(set(your_df['transformed'].explode()))\n",
    "        \n",
    "        with open('output_transformed.txt', 'w') as f:\n",
    "            for item in output_data:\n",
    "                f.write(\"%s\\n\" % item)\n",
    "        \n",
    "        # Можно проверить, как оно бьётся по конкретному слову, словосочетанию:\n",
    "        print(your_df.loc[[' 2 adult '.lower() in str(i) for i in your_df['tags'].apply(lambda x: [i.lower() for i in x])]])\n",
    "        \n",
    "        # Можно вывести 2 столбца со списками и посмотреть, как идут дела.)\n",
    "        your_df.loc[:, ['tags', 'transformed']]\n",
    "        \n",
    "        # Если есть ещё одна функция, которая делает то же самое, то можно проверить столбцы на не равенство после работы двух функций. \n",
    "        # Всё должно быть False. И если есть True, убираем вальюкаунтс и спереди добавляем df.loc[]. И смотрим, что где плохо бьётся.\n",
    "        (df_main['transformed'].apply(lambda x: [i.lower() for i in x]) != df_main['transformed_2'].apply(lambda x: [i.lower() for i in x])).value_counts()\n",
    "    ----------------------------------------------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    # Внутренняя функция, которая лупит конкретную строку.\n",
    "    def transform(row, split_word, list_of_exceptions, hard_split_list, replacement_dict):\n",
    "        # Возвращаемый список.\n",
    "        new_row = []\n",
    "        # Цикл по элементам строки.\n",
    "        for item in row:\n",
    "            # Создаём временную переменную, которая является элементом списка + пробелы с двух сторон, потому что реплейс мы делаем по отдельно стоящему слову, т.е. слову, которое окружено с двух сторон пробелами.\n",
    "            temp_item = ' ' + item.lower() + ' '\n",
    "            # Условие на вхождение слова разделителя в элемент списка. Я сначала сделал через re.search, но re работает дольше, потому переписал через вот так.\n",
    "            if split_word.lower() in item.lower():\n",
    "                # Если это слова, по которым нужно именно разделить на отдельные части, то:\n",
    "                if split_word.lower() in hard_split_list:\n",
    "                    # Заменяем слово разделитель на |||||\n",
    "                    temp_item = temp_item.replace(f' {split_word.lower()} ', '|||||')\n",
    "                    # Если реплейс сработал то:\n",
    "                    if '|||||' in temp_item:\n",
    "                        # Делим по реплесу.\n",
    "                        split_items = temp_item.split('|||||')\n",
    "                        # Добавляем само слово разделитель и остатки после разделения - отдельно.\n",
    "                        new_row.extend([split_word.strip(), *[i.strip() for i in split_items]])\n",
    "                    else:\n",
    "                        # Иначе записываем просто итем.\n",
    "                        new_row.append(item.strip())\n",
    "                # Если же это другие слова или словосочетания, то:\n",
    "                else:\n",
    "                    # Заменяем разделитель на |||||.\n",
    "                    temp_item = temp_item.replace(f' {split_word.lower()} ', '|||||')\n",
    "                    # Тут как раз проверка на \"сработал реплейс или нет\". \n",
    "                    # У меня был момент: входящая строка: 'Two Connecting Superior Double Rooms', слово-разделитель: 'Superior Double Room'. \n",
    "                    # Входит? Входит. Реплейсит? - нет. Ничего не возвращает и я добавляю просто слово разделитель, упуская саму строку. \n",
    "                    # Потому сначала вот это условие.\n",
    "                    if '|||||' in temp_item:\n",
    "                        # Если да, то бьём и\n",
    "                        split_items = temp_item.split('|||||')\n",
    "                        # добавляем само слово разделитель, и склеиваем остатки по пробелу.\n",
    "                        new_row.extend([split_word.strip(), ' '.join([i.strip() for i in split_items])])\n",
    "                    else:\n",
    "                        # Иначе записываем просто итем.\n",
    "                        new_row.append(item.strip())\n",
    "            else:\n",
    "                # Как и здесь.\n",
    "                new_row.append(item.strip())\n",
    "                \n",
    "        # Чистим список.\n",
    "        new_row = [item for item in new_row if item != '' and item not in list_of_exceptions]\n",
    "        # Переименовываем элементы списка только тогда, когда это нужно и переименование зависит от слова-разделителя.\n",
    "        if replacement_dict and split_word in replacement_dict:\n",
    "            new_row = [replacement_dict[split_word] if split_word in item else item for item in new_row]\n",
    "\n",
    "        return new_row\n",
    "    \n",
    "    ##########vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv##############\n",
    "    # Общий прогресс прогресс бара от 0 до 100% и Инициализация прогресс бара с настройками отображения.\n",
    "    total_progress = 100\n",
    "    with tqdm_notebook(total=total_progress, desc=f\"Cell progress: \", bar_format='{desc}: {percentage:.0f}%|{bar}| {n:.0f}/{total_fmt} [Elapsed: {elapsed} m/s, Remaining: {remaining} m/s]') as pbar:\n",
    "        cumulative_progress = 0 # Чтобы прогресс бар работал, как надо и завершался всегда на 100.\n",
    "    ##########^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^##############\n",
    "        \n",
    "        # Здесь сам цикл, в котором мы перебираем каждое слово и передаём его в каждую строку, которая является списком.))\n",
    "        # И обрабатываем это слово с помощью функции transform для каждой строки.\n",
    "        for i, word in enumerate(words_to_transform):\n",
    "            tags_column = tags_column.apply(transform, split_word=word, list_of_exceptions=list_of_exceptions, hard_split_list=hard_split_list, replacement_dict=replacement_dict)\n",
    "            \n",
    "    ##########vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv##############        \n",
    "            # Рассчитываем размер шага на основе оставшегося прогресса и оставшихся итераций.\n",
    "            remaining_iterations = len(words_to_transform) - i # вот тут пригодился индекс i\n",
    "            remaining_progress = total_progress - cumulative_progress\n",
    "            step_size = remaining_progress / remaining_iterations\n",
    "            \n",
    "            # Обновляем внутренний и общий прогресс.\n",
    "            cumulative_progress += step_size\n",
    "            pbar.update(step_size)\n",
    "    ##########^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^##############\n",
    "\n",
    "    return tags_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to contents](#contents)\n",
    "\n",
    "<h3 id='retagsworkfunc'>Функция для работы с [\"строками\", \"внутри\", \"списка тэгов\", \"1 Adult\", \"2 Adults\"]</h3>\n",
    "Второстепенная с использованием модуля re."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_specific_string_2(tags_column, words_to_transform, list_of_exceptions=[], replacement_dict=False): # Рабочий вариант №2\n",
    "    from tqdm.notebook import tqdm_notebook\n",
    "    import re\n",
    "    \"\"\"\n",
    "    Функция преобразования str элементов списка.\n",
    "    Типа когда у нас есть список, в котором есть строки, которые нужно побить по какому-то слову или словосочетанию.\n",
    "    И вернуть обратно такой же список, но с побитыми строками. \n",
    "    P.S. Почему не сделать изначально весь столбец .lower() и потом использовать функцию, но.. \n",
    "    Приятней же искать в выводе файла 'output_transformed.txt' следующие слова на обрезку, которые начинаются с заглавной буквы.\n",
    "    Ну, лично мне. \n",
    "\n",
    "    Args:\n",
    "        tags_column (series): столбец со списками, которые с элементами str.\n",
    "        split_word (str): ключевое слово или словосочетание.\n",
    "        list_of_exceptions (list): список исключений, которые мы не хотим добавлять при сплите: with, and.\n",
    "        replacement_dict (dict): словарь замены одинакового, написанного по-разному.\n",
    "\n",
    "    Returns:\n",
    "        list: возвращаем побитый список.))\n",
    "        \n",
    "    Пример использования: \n",
    "    нужно вставить код ниже в ячейку после функции и поменять your_df[tags_col] на свой дф с столбцом тэгов и везде заменить your_df (выделить your_df и на клаве: ctrl+D или cmd+D если мак). \n",
    "    ----------------------------------------------------------------------------------------------------------\n",
    "        words_to_transform = [# любые слова или словосочетания, по которым хотим побить каждую строку, порядок важен] # типа сначала сложносоставные слова \"1 2 Adult\", а потом \"2 Adult\". У себя я разбил на 2 тего: \"1 2 Adult\" - 1 адулт и 2 адулт. Чувак же указал.\n",
    "        list_of_exceptions = [# слова исключения, которым в списках не место (with, and)] # Тут порядок не важен.\n",
    "        replacement_dict = {# словарь, если хотим заменить какие-то слова или словосочетания} # И тут тоже.\n",
    "        \n",
    "        # Помещаем преобразование в отдельный столбец.\n",
    "        your_df['transformed'] = transform_specific_string(your_df[tags_col], words_to_transform=words_to_transform, list_of_exceptions=list_of_exceptions, replacement_dict=replacement_dict)\n",
    "        \n",
    "        # vvvvvvvvvvvvvvvvvvv  Ниже работа с выводом  vvvvvvvvvvvvvvvvvvv\n",
    "        # Если хотим посмотреть, что у нас получилось используем следующее:\n",
    "        # Полезная штука, если данных много и в аутпуте всё не помещается.\n",
    "        # В текущей директории создаётся файл в котором находится список уникальных элементов столбца. Ну или не совсем уникальных, если не делали .lower() Нужно вставить после .explode()\n",
    "        output_data = list(set(your_df['transformed'].explode()))\n",
    "        \n",
    "        with open('output_transformed.txt', 'w') as f:\n",
    "            for item in output_data:\n",
    "                f.write(\"%s\\n\" % item)\n",
    "        \n",
    "        # Можно проверить, как оно бьётся по конкретному слову, словосочетанию:\n",
    "        print(your_df.loc[[' 2 adult '.lower() in str(i) for i in your_df['tags'].apply(lambda x: [i.lower() for i in x])]])\n",
    "        \n",
    "        # Можно вывести 2 столбца со списками и посмотреть, как идут дела.)\n",
    "        your_df.loc[:, ['tags', 'transformed']]\n",
    "        \n",
    "        # В эту версию я добавлю отладчик, т.к. основная версия другая, без модуля re.\n",
    "        new_row = []\n",
    "        item = 'Two Connecting Superior Double Rooms'\n",
    "        split_word_list = ['Superior Double Room']\n",
    "        \n",
    "        for split_word in split_word_list:\n",
    "            temp_item = ' ' + item.lower() + ' '\n",
    "            \n",
    "            if split_word.lower() in item.lower():\n",
    "                if split_word.lower() == 'with' or split_word.lower() == 'and':\n",
    "                    temp_item = temp_item.replace(f' {split_word.lower()} ', '|||||')\n",
    "                    split_items = temp_item.split('|||||')\n",
    "                    new_row.extend([split_word.strip(), *[i.strip() for i in split_items]])\n",
    "                else:\n",
    "                    \n",
    "                    temp_item = temp_item.replace(f' {split_word.lower()} ', '|||||')\n",
    "                    \n",
    "                    if '|||||' in temp_item:\n",
    "                        print(temp_item) # Принтами можно проверить каждый шаг.\n",
    "                        split_items = temp_item.split('|||||')\n",
    "                        new_row.extend([split_word.strip(), ' '.join([i.strip() for i in split_items if len(i) > 1])])\n",
    "                    else:\n",
    "                        new_row.append(item.strip())\n",
    "            else:\n",
    "                new_row.append('xyu')\n",
    "        new_row\n",
    "        \n",
    "        # Проверяем на равенство после работы двух функций.\n",
    "        (df_main['transformed'].apply(lambda x: [i.lower() for i in x]) != df_main['transformed_2'].apply(lambda x: [i.lower() for i in x])).value_counts()\n",
    "    ----------------------------------------------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    # Внутренняя функция, которая лупит конкретную строку.\n",
    "    def transform(row, split_word, list_of_exceptions, replacement_dict):\n",
    "        # Возвращаемый список.\n",
    "        new_row = []\n",
    "        # Цикл по элементам строки.\n",
    "        for item in row:\n",
    "            # Используется модуль re для поиска слова, которое окружено с двух сторон пробелами и проверяет его на вхождение в элемент списка.\n",
    "            if re.search(r'\\b' + split_word.lower() + r'\\b', item.lower()):\n",
    "                # Если это слова, по которым нужно именно разделить на отдельные части, то:\n",
    "                if split_word.lower() == 'with' or split_word.lower() == 'and':\n",
    "                    # бьём по нему элемент списка и\n",
    "                    split_items = item.lower().split(split_word.lower())\n",
    "                    # добавляем само слово разделитель и остальное, что осталось в разбитом элементе.\n",
    "                    new_row.extend([split_word.strip(), *[i.strip() for i in split_items]])\n",
    "                # Если же это другие слова или словосочетания, то:\n",
    "                else:\n",
    "                    # бьём,\n",
    "                    split_items = item.lower().split(split_word.lower())\n",
    "                    # добавляем само слово и склеиваем остатки.\n",
    "                    new_row.extend([split_word.strip(), ' '.join([i.strip() for i in split_items])])\n",
    "            # Если же слово не совпадает:\n",
    "            else:\n",
    "                # то просто добавляем элемент списка неизменным.\n",
    "                new_row.append(item.strip())\n",
    "        \n",
    "        # Чистим строку.\n",
    "        new_row = [item for item in new_row if len(item) > 2 and item not in list_of_exceptions]\n",
    "        # Переименовываем элементы списка только тогда, когда это нужно и переименование зависит от слова-разделителя.\n",
    "        if replacement_dict and split_word in replacement_dict:\n",
    "            new_row = [replacement_dict[split_word] if split_word in item else item for item in new_row]\n",
    "\n",
    "        return new_row\n",
    "    \n",
    "    # Общий прогресс можно убрать. Просто у меня файл достаточно большой и это занимает пару минут. Так чисто, декоративное.\n",
    "    ##########vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv##############\n",
    "    # Общий прогресс прогресс бара от 0 до 100% и Инициализация прогресс бара с настройками отображения.\n",
    "    total_progress = 100\n",
    "    with tqdm_notebook(total=total_progress, desc=f\"Cell progress: \", bar_format='{desc}: {percentage:.0f}%|{bar}| {n:.0f}/{total_fmt} [Elapsed: {elapsed} m/s, Remaining: {remaining} m/s]') as pbar:\n",
    "        cumulative_progress = 0 # Чтобы прогресс бар работал, как надо и завершался всегда на 100%.\n",
    "    ##########^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^##############\n",
    "        \n",
    "        # Здесь сам цикл, в котором мы перебираем каждое слово и передаём его в каждую строку, которая является списком.))\n",
    "        # И обрабатываем это слово с помощью функции transform для каждой строки.\n",
    "        for i, word in enumerate(words_to_transform):\n",
    "            tags_column = tags_column.apply(transform, split_word=word, list_of_exceptions=list_of_exceptions, replacement_dict=replacement_dict)\n",
    "            \n",
    "    ##########vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv##############        \n",
    "            # Рассчитываем размер шага на основе оставшегося прогресса и оставшихся итераций.\n",
    "            remaining_iterations = len(words_to_transform) - i # вот тут пригодился индекс i\n",
    "            remaining_progress = total_progress - cumulative_progress\n",
    "            step_size = remaining_progress / remaining_iterations\n",
    "            \n",
    "            # Обновляем общий прогресс.\n",
    "            cumulative_progress += step_size\n",
    "            pbar.update(step_size)\n",
    "    ##########^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^##############\n",
    "\n",
    "    return tags_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to contents](#contents)\n",
    "\n",
    "<h3 id='multiplemodelstraining'>Функция для тренировки нескольких моделей, используя несколько алгоритмов ML за 1 заход</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_models_training(df, target_feature, list_of_ml_algs, params_dict=False, random_state=42, test_size=0.25):\n",
    "    print(f'{Fore.RED}КОРОЛЬ АРТУР НА НАС НАПАЛИ!!!{Fore.RESET}') # https://www.youtube.com/watch?v=-XuyTXj_5F8&t=6s хз, чёт вспомнилось...\n",
    "    \"\"\" Freeware by Akialema.\n",
    "    Сие ваяние было вдохновлено... В телеграмме периодически запускают эвент типа мол: \"попробуйте себя в роли датасаентиста\",\n",
    "    который длится 3 дня. И на третий день предлагалось сделать вот это вот, что происходит в проекте-3 и что происходит в этой функции: \n",
    "    х-трэйны, у-трэйны, тэсты, предсказания, кто там на что делится.. тыща параметров и т.д. И финальным заданием было попробовать \n",
    "    перебрать несколько алгоритмов, чтобы получить наименьшее некое число. И вот в тот момент я понял, что для этого дела нужен цикл.\n",
    "    С графическим представлением. Потому что не даром их так много. А потом я посмотрел прекрасную работу Андрея Волкова и такой: \n",
    "    вот же она, эта картинка.) И чтоб не плодить простыню х-трэйнов-у-трэйнов, её можно просто обернуть в цикл и в функцию.\n",
    "    Я сначала делал график прям в функции, чтоб такой ждёшь, ждёшь выполнения ячейки и хочется какой-то интерактивности прогресса \n",
    "    с последующим финальным представлением графика сравнения МАРЕ на сладкое. Но потом понял, что лучше делать картиночку отдельной ячейкой.\n",
    "    Короче кайфанул и доволен. Да и я хз, может есть уже реализация подобного, я ж новичок. Потому быстро накалякал своё. Вообще не понимаю,\n",
    "    что там происходит и правильно ли. Но работает. Я просто хорошо задаю вопросы чату гпт.))) \n",
    "    Забирайте, если понравилось.)\n",
    "    \n",
    "    P.S. функция не идеальна. Я не знаю названия всех алгоритмов. Какие чат гпт сказал, что топ, те и тут.  \n",
    "    Явно я эту функцию ещё дополню. Или пойму, что это пустая трата времени. \n",
    "    P.S.2 ЕСЛИ ВЫ ЗНАЕТЕ, КАК СДЕЛАТЬ ПЛАВНЫЙ ПРОГРЕСС БАР ПО ЯЧЕЙКЕ - РАССКАЖИТЕ МНЕ. Пожалуйста. <^-------- Можно не читать.\n",
    "    \n",
    "    Функция проведения нескольких (или одной) тренировок моделей.\n",
    "    \n",
    "    ---> На вход подаём итоговый датафрейм, в котором только числа и нет нанок, \n",
    "         целевой признак,\n",
    "         список названий алгоритмов для тренировки,\n",
    "         и словарь с параметрами для алгоритмов.\n",
    "\n",
    "    Args:\n",
    "        df (pd.Dataframe): итоговый дф.\n",
    "        target_feature (str): целевой признак.\n",
    "        list_of_ml_algs (list): список Агрессоров, каждый из них в str формате.)) (агрессоров может быть как 1, так и сколько душе угодно, но важно, чтобы они соответствовали структуре \"инит класса -> тренировка -> предсказание\").\n",
    "        params_dict (dict, optional): словарь вида {alg: [{параметры алгоритма}, {параметры фита}]}, по которому создаётся датафрейм с параметрами. Первый внутренний словарь для параметров алгоритма, второй - для фита. Defaults to False.\n",
    "        random_state (int, optional): я хз, что это, я так до сих пор и не прочувствовал np.random.seed. Defaults to 42.\n",
    "        test_size (float, optional): размер тестовой выборки. Defaults to 0.25.\n",
    "\n",
    "    Returns:\n",
    "        pd.Dataframe: На выход получаем датафрейм с натренированными моделями, числом МАРЕ и чем-нибудь ещё.\n",
    "        \n",
    "    Пример работы. Нужно скопировать код ниже и вставить в ячейку после функции.\n",
    "    ------------------------------------------------------------------------------------------------------\n",
    "    # Здесь нужно выбрать, какими алгоритмами будем гонять числа.)\n",
    "    list_of_ml_algs = ['RandomForestRegressor', 'ExtraTreesRegressor', 'BaggingRegressor', 'GradientBoostingRegressor', 'CatBoostRegressor', 'AdaBoostRegressor', 'DecisionTreeRegressor']\n",
    "    \n",
    "    # Параметры для каждого алгоритма. Порядок не важен, как и указание параметров для всех алгоритмов. \n",
    "    # Можно указать параметры только для тех, для которых нужно, остальные отработают по своим дефолтным параметрам.\n",
    "    # Важна структура {alg: [{параметры алгоритма}, {параметры фита}]}, иначе можно хлебнуть горя.)) Индекс-столбец-столбец.\n",
    "    # А можно и не делать этот словарь вовсе. Алгоритмы отработают по своим настройкам по-умолчанию.\n",
    "    params_dict = {\n",
    "            'RandomForestRegressor': [{'n_estimators': 10, 'n_jobs': -1, 'random_state': 42}, {}],\n",
    "            'ExtraTreesRegressor': [{'n_estimators': 10, 'max_depth': 3}, {}],\n",
    "            'BaggingRegressor': [{'n_estimators': 10}, {}],\n",
    "            'GradientBoostingRegressor': [{'n_estimators': 10, 'learning_rate': .1}, {}],\n",
    "            'CatBoostRegressor': [{'iterations': 10000, 'learning_rate': .1, 'depth': 3, 'verbose': False}, {}],\n",
    "            'AdaBoostRegressor': [{'n_estimators': 10, 'learning_rate': 1.0}, {}],\n",
    "            'DecisionTreeRegressor': [{}, {}]\n",
    "    } # везде эстиматоры 10, чтоб оно отработало быстро и показало работу. \n",
    "    \n",
    "    # Здесь нужно вставить свой end_df, свой 'целевой_признак', решить, какими алгоритмами будем баловаться и с какими параметрами.)\n",
    "    any_var_name = multiple_models_training(end_df, 'reviewer_score', list_of_ml_algs=list_of_ml_algs, params_dict=params_dict)\n",
    "    \n",
    "    any_var_name\n",
    "    ------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    Нужно быть внимательным/ной, чтоб всё сработало, как надо. Я вроде всё расписал.\n",
    "    \"\"\"\n",
    "    \n",
    "    from sklearn import metrics\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from colorama import Fore, Back, Style # Чтоб аутпут был цветастый.\n",
    "    from tqdm.notebook import tqdm\n",
    "    import pandas as pd\n",
    "    import time\n",
    "    \n",
    "    # Если мы передаём параметры словарём извне.\n",
    "    if params_dict:\n",
    "        params_dict_func = {alg: [{}, {}] for alg in list_of_ml_algs} # Создаётся словарь следующей формы: {alg: [{}, {}]}, для каждого элемента списка с названиями алгоритмов.\n",
    "        params_dict_func.update(params_dict) # Обновление словаря словарём извне.\n",
    "        params_df = pd.DataFrame.from_dict(params_dict_func, orient='index', columns=['alg_params', 'fit_params']) # ДФ по словарю.\n",
    "    # Если мы не передаём параметры в функцию, то все параметры будут по умолчанию для каждого алгоритма.\n",
    "    else:\n",
    "        params_df = pd.DataFrame([[{},{}]], index=list_of_ml_algs, columns=['alg_params', 'fit_params'])\n",
    "    \n",
    "    X = df.drop(columns=[target_feature], axis=1) # дропаем целевой признак из икса.\n",
    "    y = df[target_feature] # и помещаем его в игрек.\n",
    "    \n",
    "    # Эти штуки.. Я надеюсь они в правильном порядке стоят. Вроде бы у всех так.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    list_of_algs_output = [] # список по которому будем формировать выходящий датафрейм.\n",
    "    \n",
    "    total_progress = 100  # Общий прогресс прогресс бара от 0 до 100%\n",
    "    \n",
    "    # Инициализация прогресс бара с настройками отображения.\n",
    "    with tqdm(total=total_progress, desc=f\"Cell progress: \", bar_format='{desc}: {percentage:.0f}%|{bar}| {n:.0f}/{total_fmt} [Elapsed: {elapsed} m/s]') as pbar:\n",
    "        \n",
    "        cumulative_progress = 0 # Чтобы прогресс бар работал, как надо и завершался всегда на 100.\n",
    "        \n",
    "        # Цикл по списку алгоритмов.\n",
    "        for i, alg in enumerate(list_of_ml_algs): # i пригодился в вычислении шага прогрессбара.\n",
    "            \n",
    "            start_alg_time = time.time() # Время начала работы обучения модели.\n",
    "            \n",
    "            # Инит алгоритма через globals, т.к. название алгоритма == str. (можно через eval(), но гпт чату не понравилась эта идея) \n",
    "            # Параметры == параметры столбца 'class_params' для строки с именем alg.\n",
    "            model = globals()[alg](**params_df.loc[alg, 'alg_params']) \n",
    "            \n",
    "            model.fit(X_train, y_train, **params_df.loc[alg, 'fit_params']) # Здесь сама тренировка. Параметры из столбца 'fit_params'.\n",
    "            \n",
    "            current_time = time.time() # Время окончания работы алгоритма.\n",
    "            \n",
    "            y_pred = model.predict(X_test) # Предсказание.\n",
    "            \n",
    "            # Делаем МАПЕ.\n",
    "            mape = metrics.mean_absolute_percentage_error(y_test, y_pred) * 100\n",
    "            # Я не знаю, что это и для чего это здесь. Просто добавил, чтобы убедиться, что в вывод можно ещё чего-нибудь прикрутить.\n",
    "            # Соответственно, нужно обновлять аппенд и формирование столбцов итогового дф.\n",
    "            accuracy = model.score(X_test, y_test) \n",
    "            \n",
    "            list_of_algs_output.append([alg, mape, accuracy, model]) # Добавляем всю эту петрушку в список, по которому будет формироваться таблица.\n",
    "            \n",
    "            ##########################################################################################################################\n",
    "            \n",
    "            # Вычисление времени для принта ниже. Типа вот, мол, модель обучилась, поздравляю.\n",
    "            execution_time_alg = current_time - start_alg_time\n",
    "            # Можно прикрутить, что-то другое для принта.\n",
    "            print(f\"Model training completed! With: {str(round(execution_time_alg, 2)) + ' sec.' if execution_time_alg < 60 else str(round(execution_time_alg/60, 2)) + ' min.'} Alg: {Fore.CYAN}{alg}{Fore.RESET}.\") # чтоб секунды были секундами, а минуты - минутами.\n",
    "            time.sleep(0.2) # Сон я добавил, потому что сначала прогресс бар отрабатывает, а потом принт. А надо наоборот.)\n",
    "            \n",
    "            # Рассчитываем размер шага на основе оставшегося прогресса и оставшихся итераций.\n",
    "            remaining_iterations = len(list_of_ml_algs) - i # вот тут пригодился индекс i\n",
    "            remaining_progress = total_progress - cumulative_progress\n",
    "            step_size = remaining_progress / remaining_iterations\n",
    "            \n",
    "            # Обновляем общий прогресс. Все округления происходят при инициализации прогрессбара.\n",
    "            cumulative_progress += step_size\n",
    "            pbar.update(step_size)\n",
    "        \n",
    "    func_df = pd.DataFrame(list_of_algs_output, columns=['alg_name', 'mape_percent', 'accuracy', 'model']).sort_values(by='mape_percent').reset_index(drop=True) # ДФ-чик \n",
    "    \n",
    "    print(f'{Fore.GREEN}Mission accomplished{Fore.RESET}.')\n",
    "    # Возвращаем дф, в котором мапе и сама модель. \n",
    "    return func_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to contents](#contents)\n",
    "\n",
    "<h3 id='vadersentanalsec'>Функция для формирования датафрейма с определением тональности текста по алгоритму Вейдера.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_sentiment_analyser(df, targ_col):\n",
    "    \"\"\"Функция для формирования датафрейма с определением тональности текста по алгоритму Вейдера.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): incoming df\n",
    "        targ_col (str): target column\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: df with neg, neu, pos, compound from Vader.\n",
    "        \n",
    "        looks like:\n",
    "        __|targ_col_neg|targ_col_neu|targ_col_pos|targ_col_comp|\n",
    "        0 |         0.1|         0.8|         0.2|         0.64|\n",
    "        etc.\n",
    "    \"\"\"\n",
    "    # Импорт необходимых библиотек. Библиотеки лучше выносить за рамки функции.\n",
    "    import pandas as pd\n",
    "    from tqdm.notebook import tqdm\n",
    "    import nltk\n",
    "    from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "    nltk.download('vader_lexicon')\n",
    "    \n",
    "    # Инит класса.\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    # Словарь для записи результатов.\n",
    "    res = {}\n",
    "    \n",
    "    total_progress = 100\n",
    "    with tqdm(total=total_progress, desc=f\"Cell progress: \", bar_format='{desc}: {percentage:.0f}%|{bar}| {n:.0f}/{total_fmt} [Elapsed: {elapsed} m/s, Remaining: {remaining} m/s]') as pbar:\n",
    "        cumulative_progress = 0 # Чтобы прогресс бар работал, как надо и завершался всегда на 100.\n",
    "    ##########^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^##############\n",
    "        \n",
    "        # Здесь сам цикл, в котором мы перебираем каждое слово и передаём его в каждую строку, которая является списком.))\n",
    "        # И обрабатываем это слово с помощью функции transform для каждой строки.\n",
    "        for i, text in enumerate(df[targ_col]):\n",
    "            res[i] = sia.polarity_scores(text)\n",
    "            \n",
    "    ##########vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv##############        \n",
    "            # Рассчитываем размер шага на основе оставшегося прогресса и оставшихся итераций.\n",
    "            remaining_iterations = len(df) - i # вот тут пригодился индекс i\n",
    "            remaining_progress = total_progress - cumulative_progress\n",
    "            step_size = remaining_progress / remaining_iterations\n",
    "            \n",
    "            # Обновляем внутренний и общий прогресс.\n",
    "            cumulative_progress += step_size\n",
    "            pbar.update(step_size)\n",
    "    ##########^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^##############\n",
    "    # Переименование столбцов.\n",
    "    new_columns = {col: f\"{targ_col}_{col}\" for col in res[0].keys()}\n",
    "    # Возврат дф-ки с решейпом и переименованием столбцов.\n",
    "    return pd.DataFrame(res).T.rename(columns=new_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to contents](#contents)\n",
    "\n",
    "<h3 id='vadersentanalmain'>Функция для формирования датафрейма с определением тональности текста по алгоритму Вейдера (основная).</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_sentiment_analysis(df, targ_col):\n",
    "    \"\"\"Функция для формирования датафрейма с определением тональности текста по алгоритму Вейдера.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): incoming df\n",
    "        targ_col (str): target column\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: df with neg, neu, pos, compound from Vader.\n",
    "        \n",
    "        looks like:\n",
    "        __|targ_col_neg|targ_col_neu|targ_col_pos|targ_col_comp|\n",
    "        0 |         0.1|         0.8|         0.2|         0.64|\n",
    "        etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Импорт необходимых библиотек. Библиотеки лучше выносить за рамки функции.\n",
    "    import pandas as pd\n",
    "    from tqdm.notebook import tqdm\n",
    "    import nltk\n",
    "    from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "    nltk.download('vader_lexicon')\n",
    "    \n",
    "    # Инит класса.\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    # Словарь для записи результатов.\n",
    "    res = {}\n",
    "    \n",
    "    # Цикл по строкам датафрейма и запись анализа настроения в словарь.\n",
    "    for i, text in enumerate(tqdm(df[targ_col], total=len(df))):\n",
    "        res[i] = sia.polarity_scores(text)\n",
    "    \n",
    "    # Переименование столбцов.\n",
    "    new_columns = {col: f\"{targ_col}_{col}\" for col in res[0].keys()}\n",
    "    # Возврат дф-ки с решейпом и переименованием столбцов.\n",
    "    return pd.DataFrame(res).T.rename(columns=new_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to contents](#contents)\n",
    "\n",
    "<h3 id='robertapolarity'>Функция с алгоритмом Роберты на основе отзывов в твиттере.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roberta_polarity_scores(df, targ_col):\n",
    "    \"\"\"Функция для формирования датафрейма с определением тональности текста по алгоритму Роберты.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): incoming df\n",
    "        targ_col (str): target column\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: df with neg, neu, pos, compound from Vader.\n",
    "        \n",
    "        looks like:\n",
    "        __|targ_col_neg|targ_col_neu|targ_col_pos|\n",
    "        0 |         0.1|         0.8|         0.2|\n",
    "        etc.\n",
    "    \"\"\"\n",
    "    # Подгрузка библиотек. Библиотеки лучше вынести за пределы функции.\n",
    "    from transformers import AutoTokenizer\n",
    "    from transformers import AutoModelForSequenceClassification\n",
    "    from scipy.special import softmax\n",
    "    \n",
    "    # Инит классов, токенизеров.\n",
    "    MODEL = f'cardiffnlp/twitter-roberta-base-sentiment'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "    \n",
    "    # Словарь, по которому формируется выходящий дф.\n",
    "    res = {}\n",
    "    # Цикл по строкам столбца.\n",
    "    for i, text in enumerate(tqdm(df[targ_col], total=len(df))):\n",
    "      # Трай - эксепт здесь для того, что некоторые строки слишком длинные и алгоритм роберты с ними не справляется и выдаёт ошибку.\n",
    "      try: \n",
    "        # Всё, что ниже можно написать в 1 строку, но го в 4.\n",
    "        encoded_text = tokenizer(text, return_tensors='pt') # Тут токенизация слов.\n",
    "        output = model(**encoded_text) # Тут я не помню, какой аутпут был, если интересно - ниже в одну строку и можно обрубить, и посмотреть на каждый этап.\n",
    "        scores = softmax(output[0][0].detach().numpy()) # Здесь происходит экспоненцирование (вот такое вот слово) чисел с помощью softmax.\n",
    "        res[i] = {'roberta_neg': scores[0], 'roberta_neu': scores[1], 'roberta_pos': scores[2]} # Ну и запись в словарь.\n",
    "      except RuntimeError:\n",
    "        print(f'Broke for id: {i}') # если ошибка, то выводим номер строки.\n",
    "\n",
    "    # Переименование столбцов.\n",
    "    new_columns = {col: f\"{targ_col}_{col}\" for col in res[0].keys()}\n",
    "    # Возврат дф-ки с решейпом и переименованием столбцов.\n",
    "    return pd.DataFrame(res).T.rename(columns=new_columns)\n",
    "\n",
    "# Эта строчка выведет настроение конкретной строки текста, т.е. вместо 'example' нужно вставить 'любой текст'.\n",
    "#print(softmax(AutoModelForSequenceClassification.from_pretrained(f'cardiffnlp/twitter-roberta-base-sentiment')(**AutoTokenizer.from_pretrained(f'cardiffnlp/twitter-roberta-base-sentiment')('example', return_tensors='pt'))[0][0].detach().numpy())) # вместо 'example' любой текст"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to contents](#contents)\n",
    "\n",
    "<h3 id='strongcolssearcher'>Функция для автоматического поиска сильных столбцов относительно целевого столбца (тестовая).</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strong_cols_auto_searcher(df, list_of_cat_cols, list_of_num_cols, list_of_agg_func, targ_col, threshold_number=4000):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.feature_selection import f_classif\n",
    "    from tqdm.notebook import tqdm\n",
    "    \n",
    "    \"\"\"strong_cols_auto_searcher:\n",
    "\n",
    "    Функция, которая ищет наиболее значимые столбцы, для предсказания целевого столбца.\n",
    "    т.е. по очереди происходит группировка всех названий столбцов из списка list_of_cat_cols с столбцами из списка list_of_num_cols\n",
    "    с агрегацией функций из списка list_of_agg_func и сразу происходит отсев по пороговому числу threshold_number.\n",
    "    Ну и запись в выходящий датафрейм.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): incoming df.\n",
    "        list_of_cat_cols (list): [['col1'], ['col2'], ['col3', 'col4']] list with lists inside with str in this lists.\n",
    "        list_of_num_cols (list): ['col1', 'col2', 'col3'] just list with str elements.\n",
    "        list_of_agg_func (list): ['mean', 'count', 'nunique', 'sum'] list with any func name.\n",
    "        targ_col (str): 'col' - any col u like.\n",
    "        threshold_number (int, optional): minimum for f_classif. if < then we dont need this column. Defaults to 4000.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: df with data about what and by what was groupped with what function for each column at the last row of each column. \n",
    "    \"\"\"\n",
    "    # Копия для основной таблицы.\n",
    "    main_df = df.fillna(0).copy()\n",
    "    # Номер последней строки, куда потом будет происходить запись что с чем группируем и по какой функции.\n",
    "    num_for_new_row = main_df.shape[0]\n",
    "    \n",
    "    # Пустой итоговый дф.\n",
    "    func_dict = {}\n",
    "    annot_dict = {}\n",
    "    \n",
    "    # Цикл по тому, по чему группируем. С добавлением прогресс бара.\n",
    "    for i in tqdm(list_of_cat_cols, total=len(list_of_cat_cols)):\n",
    "        # Цикл по тому, что группируем. \n",
    "        for j in list_of_num_cols:\n",
    "            \n",
    "            # Если столбец есть и там, и там, то пропускаем такую группировку.\n",
    "            if j in i:\n",
    "                continue\n",
    "            \n",
    "            # Если тип столбца для группировки О (например там столбец с строками)\n",
    "            if np.issubdtype(main_df[j], 'object'):\n",
    "                # То применяем только count, т.к. mean не проканает, а sum - бессмысленен.\n",
    "                func_series = main_df.groupby(i)[j].transform('count') ############ здесь слабое место, я пока не придумал, как его обыграть.\n",
    "                # Присваиваем этому срезу новое название.\n",
    "                func_series.name = 'strong_column_' + str(len(func_dict))\n",
    "                # Далее происходит запись в словари. Записывается сам сериес и данные по сериесу, каждая запись в свой словарь.\n",
    "                func_dict[func_series.name] = func_series.to_dict()\n",
    "                annot_dict[func_series.name] = {f\"'cat_cols': {i}, 'num_cols': {j}, 'func': {k}\"}\n",
    "            \n",
    "            # Если же тип столбца для группировки числовой:\n",
    "            else:            \n",
    "                # Цикл по функциям.\n",
    "                for k in list_of_agg_func:\n",
    "                    # И дальше всё то же самое. \n",
    "                    # Сгруппировали, переименовали, записали в словари.\n",
    "                    func_series = main_df.groupby(i)[j].transform(k)\n",
    "                    func_series.name = 'strong_column_' + str(len(func_dict))\n",
    "                    \n",
    "                    func_dict[func_series.name] = func_series.to_dict()\n",
    "                    annot_dict[func_series.name] = {f\"'cat_cols': {i}, 'num_cols': {j}, 'func': {k}\"}\n",
    "    \n",
    "    # Формируем датафрейм по основному словарю.\n",
    "    func_df = pd.DataFrame(func_dict)\n",
    "    # Отсеиваем лишние столбцы, согласно условию, что число теста будет больше заданного нами, что определяет силу чтолбца.\n",
    "    func_df = func_df[func_df.columns[pd.Series(f_classif(func_df, main_df[targ_col].astype(int))[0]) > threshold_number]]\n",
    "    # Добавляем описание в последнюю строку из словаря annot_dict.\n",
    "    # Сначала формируем дф по нему, решейпим и конкатим.\n",
    "    func_df = pd.concat([func_df, pd.Series(annot_dict).to_frame().T], axis=0, join='inner').reset_index(drop=True)\n",
    "        \n",
    "    return func_df\n",
    "\n",
    "# Пример работы:\n",
    "#list_of_cat_cols = [['hotel_town', 'year_of_review', 'quarter_of_review', 'hotel_name'], ['hotel_town', 'year_of_review', 'hotel_name']]\n",
    "#list_of_num_cols = ['reviewer_score']\n",
    "#list_of_agg_func = ['mean']\n",
    "#targ_col = 'reviewer_score'\n",
    "#\n",
    "#strong_cols_auto_searcher(booking_df, list_of_cat_cols=list_of_cat_cols, list_of_num_cols=list_of_num_cols, list_of_agg_func=list_of_agg_func, targ_col=targ_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to contents](#contents)\n",
    "\n",
    "<h3 id='combinationsgenerator'>Функция для генерации комбинаций элементов входящего списка.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_combinations(incoming_list, critical_num=1):\n",
    "    \"\"\"generate_all_combinations \n",
    "\n",
    "    Функция принимает на вход список и возвращает список скомбинированных списков.\n",
    "    Например:\n",
    "    вход ['a', 'b', 'c'] -> выход [['a'], ['b'], ['c'], ['a', 'b'], ['a', 'c'], ['b', 'c'], ['a', 'b', 'c']]\n",
    "\n",
    "    Args:\n",
    "        incoming_list (list): incoming list of something.\n",
    "        critical_num (int, optional): thats if we dont need all combinations, just first half. Defaults to 1.\n",
    "        \n",
    "    Returns:\n",
    "        list: list with all combinations of items.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Внутренняя функция рекурсивная и делает она следующее:\n",
    "    # На каждый length, например пример с ['a', 'b', 'c'] на входе:\n",
    "    # При length=1 на выходе получаем [['a'], ['b'], ['c']]\n",
    "    # При length=2 - [['a', 'b'], ['a', 'c'], ['b', 'c']]\n",
    "    # При length=3 - [['a', 'b', 'c']]\n",
    "    def generate_combinations(incoming_list, length):\n",
    "        if length == 1:\n",
    "            return [[item] for item in incoming_list]\n",
    "        else:\n",
    "            combinations = []\n",
    "            for i, item in enumerate(incoming_list):\n",
    "                for next_combination in generate_combinations(incoming_list[i+1:], length - 1):\n",
    "                    combinations.append([item] + next_combination)\n",
    "            return combinations\n",
    "    \n",
    "    # И ниже мы это всё безобразие записываем в выходящий список.\n",
    "    # А критикал нум нам нужен для того, чтобы если вдруг нам не надо все все комбинации, а, например, половину...\n",
    "    all_combinations = []\n",
    "    for length in range(1, len(incoming_list) // critical_num + 1):\n",
    "        all_combinations.extend(generate_combinations(incoming_list, length))\n",
    "    return all_combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to contents](#contents)\n",
    "\n",
    "<h3 id='highcorreraser'>Функция для удаления сильно скоррелированных столбцов. (тестовая)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def high_corr_eraser(df, upper_bound=0.9, lower_bound=-0.9, thresh=1, method='pearson'):\n",
    "    \n",
    "    func_df = df.copy()\n",
    "    \n",
    "    corr_df = func_df.corr(numeric_only=True, method=method)\n",
    "    \n",
    "    list_of_low_corr_columns_name = corr_df.corr(numeric_only=True)[(corr_df.corr(numeric_only=True) <= upper_bound) | ((corr_df.corr(numeric_only=True) >= lower_bound) & (corr_df.corr(numeric_only=True) <= 0))].dropna(thresh=thresh, axis=1).columns\n",
    "    \n",
    "    return func_df[list_of_low_corr_columns_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to contents](#contents)\n",
    "\n",
    "<h3 id='fclassifchitester'>Функция для тестирования силы столбца, относительно целевого. f_classif + chi2</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_classif_chi_tester(series, targ_col):\n",
    "    \"\"\"f_classif_chi_tester _summary_\n",
    "\n",
    "    Функция.. проводит тестирование f_classifom и хи квадратом. И показывает число. Чем больше - тем лучше.\n",
    "    Хи квадрат какой-то левый тест. Многомилионные показатели никак не влияют на модель. Как будто там что-то ломается.\n",
    "    Но не суть. f_classif более правдивый.\n",
    "\n",
    "    Args:\n",
    "        series (pd.Series/pd.DataFrame): incoming series num only. Or if its df\n",
    "        targ_col (pd.Series): looks like your_df['your_targ_col']\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from sklearn.feature_selection import chi2 # хи-квадрат\n",
    "    from sklearn.feature_selection import f_classif # anovav\n",
    "    \n",
    "    func_series_or_df = series.copy()\n",
    "    \n",
    "    # if series\n",
    "    if isinstance(func_series_or_df, pd.Series):\n",
    "        chi = pd.Series(chi2(func_series_or_df.to_frame().abs(), targ_col.astype(int))[0], index=['chi'])\n",
    "        f = pd.Series(f_classif(func_series_or_df.to_frame(), targ_col.astype(int))[0], index=['f'])\n",
    "    \n",
    "    else: # if df\n",
    "        if targ_col.name in func_series_or_df.columns: # drop targ_col if it exists.\n",
    "            func_series_or_df.drop(columns=targ_col.name, inplace=True)\n",
    "        chi = pd.Series(chi2(func_series_or_df.abs(), targ_col.astype(int))[0], index=func_series_or_df.columns, name='chi')\n",
    "        f = pd.Series(f_classif(func_series_or_df, targ_col.astype(int))[0], index=func_series_or_df.columns, name='f_classif')\n",
    "        \n",
    "        return pd.DataFrame([f, chi]).T\n",
    "    return pd.concat([f, chi])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to contents](#contents)\n",
    "\n",
    "<h3 id='specialgrouper'>Группировщик с применением ко всему датафрейму</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def special_grouper(df, bywhat, what, list_of_agg_func=['sum', 'mean', 'count', 'nunique']):\n",
    "    \"\"\"special_grouper \n",
    "\n",
    "    Согласитесь, бесит, когда надо писать несколько строчек, чтобы группировка применилась ко всей таблице.\n",
    "    А метод transform принимает только 1 аргумент и ему плевать список там или нет. Хотя в документации там может быть список.\n",
    "    Этот момент я до конца не понял.\n",
    "    \n",
    "    Эта функция - исключительно для визуальной оценки, а потом мы делаем нужный нам сериес с выбранной агрегацией через transform.\n",
    "    Т.к. трансформ крутая штука на самом деле, туда можно пихнуть лямбду.\n",
    "    \n",
    "    Хотя и отсюда можно взять столбец, но через .loc/iloc, потому что функция возвращает тип датафрейм.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): any df.\n",
    "        bywhat (str or list): name or names of columns which we want to group.\n",
    "        what (str): name of col we wanna group bywhat (my logic works only for single col cuz .iloc[bla bla] (can be expanded))\n",
    "        list_of_agg_func (list, optional): list of agg func. Defaults to ['sum', 'mean', 'count', 'nunique'].\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: a column or columns with grouping applied to the entire df as a df.\n",
    "    \"\"\"\n",
    "    return pd.merge(df[bywhat], df.groupby(bywhat)[what].agg(list_of_agg_func).reset_index(), on=bywhat, how='left').iloc[:, -len(list_of_agg_func):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to contents](#contents)\n",
    "\n",
    "<h3 id='goodbadwordssearcher'>Функция для определения положительных и отрицательных слов в предложении</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Функция для определения тональности слова\n",
    "def extract_sentiment_words(sentence, positive_words, negative_words):\n",
    "    \n",
    "    def get_word_sentiment(word, positive_words, negative_words):\n",
    "        # Создание синтаксических представлений для положительных и отрицательных слов    \n",
    "        positive_synsets = [j for i in positive_words for j in wordnet.synsets(i, pos=wordnet.ADJ)]\n",
    "        negative_synsets = [j for i in negative_words for j in wordnet.synsets(i, pos=wordnet.ADJ)]\n",
    "        \n",
    "        # Определение синонимов слова в WordNet\n",
    "        synsets = wordnet.synsets(word, pos=wordnet.ADJ)\n",
    "        if synsets:\n",
    "            # Проверка синонимов на положительность или отрицательность\n",
    "            for synset in synsets:\n",
    "                if synset in positive_synsets:\n",
    "                    return 'positive'\n",
    "                elif synset in negative_synsets:\n",
    "                    return 'negative'\n",
    "        # Если не удалось определить тональность\n",
    "        return 'neutral'\n",
    "    \n",
    "    positive_list = []\n",
    "    negative_list = []\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    for word in words:\n",
    "        sentiment = get_word_sentiment(word, positive_words, negative_words)\n",
    "        if sentiment == 'positive':\n",
    "            positive_list.append(word)\n",
    "        elif sentiment == 'negative':\n",
    "            negative_list.append(word)\n",
    "    return positive_list, negative_list\n",
    "\n",
    "sentense = 'i ok feel very conflicted about this hotel unfortunately ultimately i wouldn t recommend it because with our experience the bad outweighed the good the fire alarm went off late on sunday night 11th july on the first night we stayed so naturally everyone evacuated and stood outside it was absolutely pouring so not the best start eventually a member of hotel staff came to the front desk and the alarm was silenced i would say the alarm had been going for about 10 minutes at this point though and there wasn t a staff member to tell us if it was a false alarm or real or anything she explained in a really aggressive manner that there was a fault with the alarm and heavy rain sometimes set it off she seemed irritated we had bothered to evacuate and said that it was not the first time it had happened no apology or further explanation of a fix or anything fair enough though it was close to 1am and we were all a bit moody we arrived back at the hotel on our first day and the girl at the front desk informed us the lift was broken we were on the top floor which was a pain but being able bodied it was hardly a big deal however it was not fixed for the full 4 days of our stay again no apology offered at any point they did however offer to help getting luggage downstairs which was a godsend no idea what they would have done if they had disabled guests all in all the staff were unpleasant to deal with when we returned to the hotel in the evening we arrived home late from disneyland on tuesday night and asked for some milk sachets for the room and we were told no milk is available by an exasperated girl on the front desk who was busy making a personal call we walked up the four flights of stairs and found some milk sachets on a cleaning trolley on the landing so i think she just had no interest in being helpful also just a word of advice our room 405 had the wc seperate then a shower in the middle of the bedroom'\n",
    "\n",
    "# Определение положительных и отрицательных слов\n",
    "positive_words = ['good', 'great', 'excellent', 'ok', 'awesome']  # Положительные слова\n",
    "negative_words = ['bad', 'poor', 'awful', 'conflict']  # Отрицательные слова\n",
    "\n",
    "print(extract_sentiment_words(sentence=sentense, positive_words=positive_words, negative_words=negative_words))\n",
    "print(wordnet.synsets('ok', pos=wordnet.ADJ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JIeMyp4uK",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
